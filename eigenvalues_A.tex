\chapter{Iterative Methods to Approximate Eigenvalues}
\label{chapEigVal}

\section{Background in Linear Algebra}

Before developing methods to approximate eigenvalues of linear operators, we
need to review some basic concepts of Linear Algebra.  We also present some
theoretical results about the location of the eigenvalues.  In
Section~\ref{iter_LE_review} of Chapter~\ref{chaptSeqB}, we have already
given some properties of the eigenvalues of a linear operator.  We refer in
particular to Definition~\ref{iter_LE_eig2}, Theorem~\ref{spectral}
and Remarks~\ref{iter_LE_eig1} and \ref{iter_LE_eig3},

\subsection{Orthogonality}

To really understand the Gram-Schmidt orthogonalization process that
we give in Definition~\ref{C14L26}, we need to review some useful
concepts including projections on subspace of $\RR^n$.

\begin{defn}
Let $\ps{\cdot}{\cdot}$ be a scalar product on $\RR^n$.  A set of
non-null vectors $\{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ is
{\bfseries orthogonal}\index{Vectors!Orthogonal} if
$\ps{\VEC{v}_i}{\VEC{v}_j} = 0$ for $i\neq j$.
\end{defn}

\begin{prop}
Let $\ps{\cdot}{\cdot}$ be a scalar product on $\RR^n$ and
$S = \{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ be a set of
orthogonal vectors.  Then $S$ is a set of linearly independent
vectors.
\end{prop}

\begin{proof}
Suppose that $\displaystyle \VEC{0} = \sum_{j=1}^k a_j \VEC{v}_j$.
We have
\[
0 = \ps{\VEC{v}_i}{\VEC{0}}
= \ps{\VEC{v}_i}{\sum_{j=1}^k a_j \VEC{v}_j}
= \sum_{j=1}^k a_j
\underbrace{\ps{\VEC{v}_i}{\VEC{v}_j}}_{=0\text{ for } j\neq i}
= a_i \ps{\VEC{v}_i}{\VEC{v}_i}
\]
for $i=1$, $2$, \ldots, $k$.  Since $\ps{\VEC{v}_i}{\VEC{v}_i} \neq 0$
because $\VEC{v}_i \neq \VEC{0}$, we get $a_i=0$.
\end{proof}

\begin{defn}
Let $S = \{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ be a set of
vectors in $\RR^n$.  The {\bfseries span}\index{Vectors!Span} of $S$
is the subspace, denoted $\Span(S)$, defined by
\[
  \Span(S) = \left\{ \sum_{j=1}^k a_j \VEC{v}_j : a_j \in \RR \right\} \quad .
\]
\end{defn}

\begin{defn}
Let $\ps{\cdot}{\cdot}$ be a scalar product on $\RR^n$.  Let
$S = \{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ be an orthogonal
set of $\RR^n$ and $V = \Span(S)$.
The
{\bfseries orthogonal projection}\index{Linear Mappings!Orthogonal Projection}
$P$ on $V$ is the mapping
defined by $P(\VEC{x}) = \displaystyle \sum_{j=1}^k a_j \VEC{v}_j$,
where
$\displaystyle a_j = \frac{\ps{\VEC{v}_j}{\VEC{x}}}
{\ps{\VEC{v}_j}{\VEC{v}_j}}$.
\label{C14L1}
\end{defn}

\begin{prop}
The orthogonal project $P$ defined in Definition~\ref{C14L1} is
a linear mapping such that $\VEC{x} - P(\VEC{x}) \Bot V$ for all
$\VEC{x} \in \RR^n$; namely, $\ps{\VEC{v}}{\VEC{x} - P(\VEC{x})} = 0$
for all $\VEC{v} \in V$.
\end{prop}

\begin{proof}
That $P$ is a linear mapping is a consequence of the linearity in the
second component of the scalar product.  We leave it to the reader to
verify that $P(a\VEC{x} + b\VEC{y}) = a P(\VEC{x}) + b P(\VEC{y})$ for
all $\VEC{x},\VEC{y} \in \RR^n$ and $a,b \in \RR$.

Choose $\VEC{x} \in \RR^n$.  To prove that $\VEC{x} - P(\VEC{x}) \Bot V$,
it suffices to prove that $\ps{\VEC{v}_i}{\VEC{x} - P(\VEC{x})} = 0$ for
$1 \leq i \leq k$.

Let $\displaystyle P(\VEC{x}) = \sum_{j=1}^k a_j \VEC{v}_j$ with
$\displaystyle a_j = \frac{\ps{\VEC{v}_j}{\VEC{x}}}
{\ps{\VEC{v}_j}{\VEC{v}_j}}$.  We have
\begin{align*}
\ps{\VEC{v}_i}{\VEC{x} - P(\VEC{x})} &=
\ps{\VEC{v}_i}{\VEC{x} - \sum_{j=1}^k a_j \VEC{v}_j}
= \ps{\VEC{v}_i}{\VEC{x}} - \sum_{j=1}^k a_j \ps{\VEC{v}_i}{\VEC{v}_j}  \\
& = \ps{\VEC{v}_i}{\VEC{x}} - a_i \ps{\VEC{v}_i}{\VEC{v}_i} = 0
\end{align*}
for $1 \leq i \leq k$ by definition of $a_i$.
\end{proof}

We illustrate in Figure~\ref{sketchproj} an orthogonal projection $P$ on a
subspace $V$ of $\RR^3$ generated by two orthogonal vectors
$\VEC{v}_1$ and $\VEC{v}_2$.

\pdfF{eigenvalues_A/projection}{Sketh of a projection onto a
subspace of $\RR^3$}{Sketch of the image of a vector $\VEC{x}$ by the
orthogonal projection $P$ on a subspace $V$ of $\RR^3$ generated by
two orthogonal vectors $\VEC{v}_1$ and $\VEC{v}_2$.}{sketchproj}

\begin{prop}
The orthogonal project $P$ defined in Definition~\ref{C14L1} has
the following property.
\[
  \|\VEC{x} - P(\VEC{x})\| < \|\VEC{x} - \VEC{w}\|
\]
for all $\VEC{w} \in V$ such that $\VEC{w} \neq P(\VEC{x})$.  The norm
of a vector $\VEC{y}\in \RR^n$ is obviously defined by
$\|\VEC{y}\| = \sqrt{\ps{\VEC{y}}{\VEC{y}}}$.
\end{prop}

\begin{proof}
The conclusion of the proposition is illustrated in
Figure~\ref{sketchproj}.

Suppose that $\VEC{x}, \VEC{w} \in \RR^n$.
Since $P(\VEC{x})- \VEC{w} \in V$, we have from the previous
proposition that $\ps{P(\VEC{x})- \VEC{w}}{\VEC{x} - P(\VEC{x})} = 0$.
Hence
\begin{align*}
\|\VEC{x} - \VEC{w}\|^2 &= \ps{\VEC{x} - \VEC{w}}{\VEC{x} - \VEC{w}} \\
& = \ps{(\VEC{x} - P(\VEC{x})) + (P(\VEC{x})- \VEC{w})}
{(\VEC{x} - P(\VEC{x})) + (P(\VEC{x})- \VEC{w})} \\
& = \ps{\VEC{x} - P(\VEC{x})}{\VEC{x} - P(\VEC{x})}
+ \ps{P(\VEC{x})- \VEC{w}}{\VEC{x} - P(\VEC{x})}
+ \ps{\VEC{x} - P(\VEC{x})}{P(\VEC{x})- \VEC{w}} \\
&\qquad + \ps{P(\VEC{x})- \VEC{w}}{P(\VEC{x})- \VEC{w}} \\
& = \ps{\VEC{x} - P(\VEC{x})}{\VEC{x} - P(\VEC{x})}
+ \ps{P(\VEC{x})- \VEC{w}}{P(\VEC{x})- \VEC{w}} \\
& = \|\VEC{x} - P(\VEC{x})\|^2 + \|P(\VEC{x})- \VEC{w}\|^2
> \|\VEC{x} - P(\VEC{x})\|^2
\end{align*}
unless $\|P(\VEC{x}) - \VEC{w}\| = 0$; namely, unless
$\VEC{w} = P(\VEC{x})$.
\end{proof}

\begin{rmk}
The previous proposition shows that the orthogonal projection $P$
defined in Definition~\ref{C14L1} is independent of the
orthogonal set $S$ generating the subspace $V$.
\end{rmk}

\begin{defn}
Let $\ps{\cdot}{\cdot}$ be a scalar product on $\RR^n$ and $V$ be a
subspace of $\RR^n$.  A set of non-null vector
$\{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ is
{\bfseries orthogonal basis}\index{Vectors!Orthogonal Basis} of $V$ if
it is a basis of $V$ and it is orthogonal.  It is an {\bfseries
  orthonormal basis}\index{Vectors!Orthonormal Basis} of $V$ if it 
an orthogonal basis of $V$ such that $\|\VEC{v}_j\| = 1$ for
$1 \leq j \leq k$.
\end{defn}

\begin{prop}
Let $\ps{\cdot}{\cdot}$ be a scalar product on $\RR^n$.  If
$S = \{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ is an orthogonal
basis of a subspace $V$ of $\RR^n$, then
$\displaystyle \VEC{v} = \sum_{j=1}^k a_j \VEC{v}_j$ with
$\displaystyle a_j = \frac{\ps{\VEC{v}_j}{\VEC{x}}}
{\ps{\VEC{v}_j}{\VEC{v}_j}}$ for all $\VEC{v} \in V$.
\label{C14L2}
\end{prop}

\begin{proof}
Given $\VEC{v} \in V$, since $S$ is a basis of $V$, we have
$\displaystyle \VEC{v} = \sum_{j=1}^k a_j \VEC{v}_j$ for some
$a_j \in \RR$.  Hence
\[
\ps{\VEC{v}_i}{\VEC{v}} =
\ps{\VEC{v}_i}{\sum_{j=1}^k a_j \VEC{v}_j}
= \sum_{j=1}^k a_j \ps{\VEC{v}_i}{\VEC{v}_j}
= a_i \ps{\VEC{v}_i}{\VEC{v}_i}
\]
for $1 \leq i \leq k$.  Thus,
$\displaystyle a_i = \frac{\ps{\VEC{v}_i}{\VEC{x}}}
{\ps{\VEC{v}_i}{\VEC{v}_i}}$ for $1 \leq i \leq k$.
\end{proof}

\subsection{Self-adjoint and Unitary Operators}

Let $V$ be a vector space over the complex numbers.  A linear
functional\index{Linear Functional} $L$ on $V$ is a linear mapping
from $V$ to $\CC$.  The vector space of all linear functional on $V$
is denoted $V^\ast$. It is called the {\bfseries dual}\index{Dual} of
$V$.

\begin{theorem}[Reisz]
Let $V$ be a vector space over the complex numbers and
$\ps{\cdot}{\cdot}:V \times V \rightarrow \CC$ be an hermitian product.
Given a linear functional $L$ on $V$, there exist a unique $\VEC{w}\in V$
such that $L(\VEC{v}) = \ps{\VEC{v}}{\VEC{w}}$ for all $\VEC{v} \in V$.  The
mapping $\VEC{w} \mapsto \ps{\cdot}{\VEC{w}}$ is a
{\bfseries conjugate-linear isomorphism}\index{Conjugate-Linear Isomorphism}
from $V$ to $V^\ast$ (because
$\lambda \VEC{w} \mapsto \overline{\lambda} \ps{\cdot}{\VEC{w}}$\,).
\end{theorem}

\begin{cor}
Let $V$ be a vector space over the complex numbers and
$\ps{\cdot}{\cdot}:V \times V \rightarrow \CC$ be an hermitian product.
Suppose that $A$ is a linear mapping from $V$ into itself.  There exists a
unique linear mapping $B$ from $V$ into itself such that
$\ps{A\VEC{v}}{\VEC{w}} = \ps{\VEC{v}}{B\VEC{w}}$ for all $\VEC{v}$ and
$\VEC{w}$ in $V$.
\label{C14L3}
\end{cor}

\begin{defn}
The linear mapping $B$ in Corollary~\ref{C14L3} is called the
{\bfseries adjoint}\index{Linear Mappings!Adjoint} of $A$ and is
denoted $A^\ast$.  If $V=\CC^n$ and
$\ps{\cdot}{\cdot}:\CC^n \times \CC^n \to \CC$ is the standard
hermitian product, then this definition of adjoint
corresponds to the definition of adjoint for a matrix.
Namely, $\displaystyle A^\ast = \overline{A}^\top$.

We say that $A$ is
{\bfseries hermitian}\index{Linear Mappings!Hermitian} or
{\bfseries self-adjoint}\index{Linear Mappings!Self-Adjoint} if
$A = A^\ast$.
\end{defn}

\begin{defn}
Let $V$ be a vector space over the complex numbers and
$\ps{\cdot}{\cdot}:V \times V \rightarrow \CC$ be an hermitian product.  A
linear mapping $A:V \rightarrow V$ is
{\bfseries (complex) unitary}\index{Linear Mappings!Unitary} if
$\ps{A\VEC{v}}{A\VEC{w}} = \ps{\VEC{v}}{\VEC{w}}$ for all $\VEC{v}$ and
$\VEC{w}$ in $V$.
\end{defn}

\begin{theorem}
Let $V$ be a vector space over the complex numbers and
$\ps{\cdot}{\cdot}:V \times V \rightarrow \CC$ be an hermitian product.  Let
$\|\cdot\|:V \rightarrow [0,\infty[$ be the norm induced by the scalar
product on $V$; namely, $\|\VEC{v}\| = \sqrt{\ps{\VEC{v}}{\VEC{v}}}$
for all $\VEC{v} \in V$.  Let $A:V \rightarrow V$ be a linear mapping.
The following statement are equivalent:
\begin{enumerate}
\item $A$ is complex unitary.
\item $\| A\VEC{v} \| =  \|\VEC{v}\|$ for all $\VEC{v} \in V$.
\item $A^\ast\,A = A \, A^{\ast} = \Id$.
\end{enumerate}
\end{theorem}

\subsection{Symmetric and Orthogonal Operators}

The notion of Hermitian and unitary Operators can be restricted to vector
spaces over the real numbers.  To do this, we need the following theorem.

\begin{theorem}
Let $V$ be a vector space over the real numbers and
$\ps{\cdot}{\cdot} :V \times V \rightarrow \RR$ be a scalar product.  Suppose
that $A: V \rightarrow V$ is a linear mapping.  Then there exists a
unique linear mapping $B:V \rightarrow V$ such that
$\ps{A\VEC{v}}{\VEC{w}} = \ps{\VEC{v}}{B\VEC{w}}$ for
all $\VEC{v}$ and $\VEC{w}$ in $V$.
\label{C14L4}
\end{theorem}

\begin{defn}
The linear mapping $B$ in Corollary~\ref{C14L4} is called the
{\bfseries transpose}\index{Linear Mappings!Transpose} of $A$ and is
denoted $A^\top$.  If $V=\RR^n$ and
$\ps{\cdot}{\cdot} : \RR^n \times \RR^n \to \RR$
is the standard scalar product, then this definition of transpose
corresponds to the definition of transpose for a matrix.  We say that
$A$ is {\bfseries symmetric}\index{Linear Mappings!Symmetric} if
$A = A^\top$. 
\end{defn}

\begin{defn}
Let $V$ be a vector space over the real numbers and
$\ps{\cdot}{\cdot}:V \times V \rightarrow \RR$ be a scalar product.
A linear mapping $A: V \rightarrow V$ is
{\bfseries real unitary}\index{Linear Mappings!Real Unitary} or
{\bfseries orthogonal}\index{Linear Mappings!Orthogonal} if
$\ps{A\VEC{v}}{A\VEC{w}} = \ps{\VEC{v}}{\VEC{w}}$ for all $\VEC{v}$ and
$\VEC{w}$ in $V$.
\end{defn}

\begin{theorem}
Let $V$ be a vector space over the real numbers and
$\ps{\cdot}{\cdot}:V \times V \rightarrow \RR$ be a scalar product.
Let $\|\cdot\|:V \rightarrow [0,\infty[$ be the norm induced by the scalar
product on $V$; namely, $\|\VEC{v}\| = \sqrt{\ps{\VEC{v}}{\VEC{v}}}$.  Let
$A:V \rightarrow V$ be a linear mapping.  The following statement are
equivalent:
\begin{enumerate}
\item $A$ is orthogonal.
\item $\| A\VEC{v} \| =  \|\VEC{v}\|$ for all $\VEC{v} \in V$.
\item $A^\top\,A = A\,A^\top = \Id$.
\end{enumerate}
\end{theorem}

\subsection{Triangular and Diagonal Matrices}

\begin{defn}
Two \nm{n}{n} matrices $A$ and $B$ are
{\bfseries similar}\index{Matrices!Similar} if there exists an
invertible \nm{n}{n} matrix $N$ such that $B = N^{-1} A N$.
\end{defn}

\begin{theorem}
Let $A$ and $B$ be two similar \nm{n}{n} matrices as defined in the
previous definition.  Then $A$ and $B$ have the
same characteristic polynomial.  In particular, $\VEC{x}$ is an eigenvector
of $A$ associated to the eigenvalue $\lambda$ if and only if $N^{-1}\VEC{x}$
is an eigenvector of $B$ associated to the eigenvalue $\lambda$.
\end{theorem}

\begin{theorem}[Schur Form and decomposition]
Let $A$ be an \nm{n}{n} matrix with entries in $\CC$.  Then there exists an
unitary matrix $U$ such that $U^\ast A U$ is upper-triangular.  We say that
$A$ is {\bfseries unitary similar}\index{Matrices!Unitary Similar} to
an upper-triangular matrix.
\end{theorem}

\begin{theorem}
Let $V$ be a vector space over the real numbers and
$\ps{\cdot}{\cdot}:V \times V \rightarrow \RR$ be a scalar product.  Suppose
that $A:V \rightarrow V$ is a symmetric linear mapping.  Then there exists an
orthogonal basis of $V$ consisting of eigenvectors of $A$.  In particular,
all the eigenvalues of $A$ are real.
\label{C14L5}
\end{theorem}

\begin{cor}
Let $A$ be a \nm{n}{n} symmetric matrix with entries in $\RR$ and
$\ps{\cdot}{\cdot}:\RR^n \times \RR^n \to \RR$ be the standard scalar
product.  Then there exists a real unitary matrix $U$ such that
$U^{\top}\,A\,U$ is diagonal.  If $\EE$ is the canonical basis in
$\RR^n$ and $\BB$ is the orthogonal basis of eigenvectors of $A$ given
in Theorem~\ref{C14L5},  then $U = Q_\EE^\BB(\Id_n)$, where
$Q_\EE^\BB(\Id_n)$ is the matrix of change of basis from $\BB$ to
$\EE$.
\end{cor}

\subsection{Definite Positive Matrices}

\begin{defn}
A {\bfseries quadratic form}\index{Quadratic Form} on $\RR^n$
(resp. $\CC^n$) is a real-valued function $Q$ of the form
$Q(\VEC{x}) = \VEC{x}^\ast \,A\,\VEC{x}$ for
$\VEC{x}$ in $\RR^n$ (resp.\ $\CC^n$), where $A$ is a \nm{n}{n} symmetric
(resp.\ hermitian) matrix.
\end{defn}

\begin{defn}
\begin{enumerate}
\item A quadratic form $Q(\VEC{x})$ is
{\bfseries positive definite}\index{Quadratic Form!Positive Definite}
if $Q(\VEC{x}) \geq 0$ for all $\VEC{x}$. 
\item A quadratic form $Q(\VEC{x})$ is
{\bfseries strictly positive definite}\index{Quadratic Form!Strictly
Positive Definite} if $Q(\VEC{x}) > 0$ for all $\VEC{x} \neq \VEC{0}$.
\item A quadratic form $Q(\VEC{x})$ is
{\bfseries indefinite}\index{Quadratic Form!Indefinite} if there
exists $\VEC{x}_1$ and $\VEC{x}_2$ such that
$Q(\VEC{x}_1) < 0 < Q(\VEC{x}_2)$. 
\item A \nm{n}{n} matrix $A$ is
{\bfseries positive definite}\index{Matrices!Positive Definite}
(resp.\ {\bfseries strictly positive definite}\index{Matrices!Strictly
Positive Definite}) if $Q(\VEC{x}) = \VEC{x}\,^\ast A \VEC{x}$ is
positive definite (resp.\ strictly positive definite).
\end{enumerate}
\end{defn}

\begin{theorem}
Let $A$ be a \nm{n}{n} symmetric matrix.  $A$ is positive definite if and
only if all the eigenvalues of $A$ are greater than 0.
\end{theorem}

\begin{proof}
\subQ{i} Suppose that $A$ is positive definite.  Let $\lambda$ be an
eigenvalue of $A$ and $\VEC{v}$ be an eigenvector associated to $\lambda$. We
have
\[
0 < \VEC{v}^\top A \VEC{v} = \VEC{v}^\top \left( \lambda \VEC{v}\right) =
\lambda \VEC{v}^\top \VEC{v} = \lambda \|\VEC{v}\|^2 \  .
\]
Thus $\lambda >0$.

\subQ{ii} Suppose that all eigenvalues of $A$ are positive.
Let $\displaystyle \{\VEC{v}_i\}_{i=1}^n$ be an orthogonal basis of
eigenvectors of $A$ given by Theorem~\ref{C14L5}.  Let $\lambda_j$ be
the eigenvalue associated to the eigenvector $\VEC{v}_j$ for $j=1$, $2$,
\ldots, $n$.  Given $\VEC{x} \in \RR^n$, we may write
$\displaystyle \VEC{x} = \sum_{j=1}^n \beta_j \VEC{v}_j$ for some unique
$\beta_j \in \RR$.  Hence,
\begin{align*}
\VEC{x}^\top A \VEC{x} &=
\VEC{x}^\top \left( \sum_{j=1}^n \beta_j A \VEC{v}_j \right)
= \VEC{x}^\top \left( \sum_{j=1}^n \lambda_j \beta_j \VEC{v}_j \right)
= \sum_{i=1}^n\left( \sum_{j=1}^n \lambda_j \beta_j \beta_i \VEC{v}_i^\top
\VEC{v}_j \right) \\
&= \sum_{i=1}^n\left(\sum_{j=1}^n \lambda_j \beta_j \beta_i \delta_{i,j}\right)
= \sum_{j=1}^n \lambda_j \beta_j^2 > 0
\end{align*}
if $\VEC{x} \neq \VEC{0}$.
\end{proof}

\begin{theorem}
Let $A$ be a \nm{n}{n} symmetric (or hermitian) matrix.
\begin{enumerate}
\item $A$ is positive definite if and only if $\det(A_k) > 0$ for all
{\bfseries principal submatrices}\index{Matrices!Principal Submatrices}
\[
A_k = \begin{pmatrix}
a_{1,1} & \ldots & a_{1,k} \\
\vdots & \ddots & \vdots \\
a_{k,1} & \ldots & a_{k,k}
\end{pmatrix}
\]
of $A$.
\item $A$ is positive definite if and only if all the pivots used in the
reduction process of $A$ to a row-echelon form, without interchanging
rows, are positive.
\end{enumerate}
\end{theorem}

\subsection{Gerschgorin's Theorem}

\begin{theorem}[Gerschgorin's Circles]
Let $A$ be an \nm{n}{n} matrix and $\lambda$ be an eigenvalue of $A$.  Then
there exists an index $i$ with $1\leq i \leq n$, such that
\[
|\lambda - a_{i,i}| \leq \sum_{\substack{j=1\\ j\neq i}}^n\,|a_{i,j}| \ .
\]
More precisely, each component (i.e.\ a connected set which is not properly
contained in a larger connected set) of the union
$\displaystyle \bigcup_{i=1}^n\,U_i$ of the
{\bfseries Gerschgorin's circles}\index{Gerschgorin's Circles}
\[
U_i = \left\{ \lambda : |\lambda - a_{i,i}| \leq
\sum_{\substack{j=1\\ j\neq i}}^n\,|a_{i,j}| \right\}
\]
contains exactly as many eigenvalues of $A$ (counted with algebraic
multiplicity) as circles $U_i$ forming the component.
\end{theorem}

\begin{proof}
\stage{i} Suppose that $\lambda$ is an eigenvalue of $A$ and that $\VEC{v}$ is
an eigenvector associated to $\lambda$.  Let $k$ be an integer such that
$\displaystyle |v_k| = \|\VEC{v}\|_\infty = \max_{1\leq j\leq n} |v_j|$.
Note that $v_k \neq 0$ because $\VEC{v} \neq \VEC{0}$.  From
$A\VEC{v} = \lambda \VEC{v}$, we get
\[
\sum_{j=1}^n a_{k,j} v_j = \lambda v_k \ .
\]
Thus
\[
\left| (a_{k.k} - \lambda ) v_k \right|
= \left| - \sum_{\substack{j=1\\ j\neq k}} a_{k,j} v_j \right| \  .
\]
After dividing both sides of this equality by $|v_k|$, we get
\[
\left| a_{k.k} - \lambda \right|
\leq \sum_{\substack{j=1\\ j\neq k}} \left|a_{k,j} \right|
\frac{|v_j|}{|v_k|} \leq \sum_{\substack{j=1\\ j\neq k}} \left|a_{k,j} \right| \ .
\]
This implies that $\lambda \in U_k$.

\stage{ii} To prove the second statement of the theorem, suppose that $U$ is a
component of the form
\[
U = \bigcup_{i\in I} U_i \  ,
\]
where $I$ is a subset of $\{1,2,\ldots,n\}$.  Let
$A(t) = tA + (1-t)D$. where $D$ is the diagonal matrix defined by
\[
D =
\begin{pmatrix}
a_{1,1} & 0 & 0 & \ldots & 0 \\
0 & a_{2,2} & 0 & \ldots & 0 \\
0 & 0 & a_{3,3} & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & a_{n,n} 
\end{pmatrix} \ ,
\]
Let $\displaystyle R(t) = \bigcup_{i\in I} R_i(t)$ with
\[
R_i(t) = \left\{ z : |z-a_{i,i}| \leq t \sum_{\substack{j=1\\ j\neq i}}
| a_{i,j} | \right\} \quad .
\]
At $t=0$, there are obviously $|I|$ eigenvalues of $A(0) = D$ in
$\displaystyle R(0) = \left\{ a_{j,j} : j \in I \right\}$
(counted with algebraic multiplicity); these eigenvalues are $a_{j,j}$ for
$j\in I$.

The eigenvalues of $A(t)$ are in $\displaystyle \bigcup_{i=1}^n R_i(t)$
because of (i). Moreover, $R(t)$ is a closed set such that
$R(t) \cap R_i(t) = \emptyset$ for all $i\not\in I$ and all
$0\leq t \leq 1$ because $ R_i(t) \subset R_i(1) = U_i$ for all
$0\leq t \leq 1$ and all $i$, and $U \cap U_i = \emptyset$ for all
$i\not\in I$ (Figure~\ref{eig_ggo_FIG}).
Since the eigenvalues of $A(t)$ are continuous functions of $t$, because the
roots of the characteristic polynomial
$p_t(\lambda) = \det(A(t) - \lambda \Id)$
are continuous functions of its coefficients which are continuous functions
of $t$, the number of eigenvalues of $A(t)$ in $R(t)$ (counted with
algebraic multiplicity) is constant.  No eigenvalue of $A(t)$ can jump from
$R(t)$ to one of the $R_i(t)$ with $i\not\in I$ by continuity.

Therefore, $R(1)=U$ contains $|I|$ eigenvalues (counted with algebraic
multiplicity) as $R(0)$.
\end{proof}

\pdfF{eigenvalues_A/ggo}{Gerschgorin's Circles}{Example of the Gerschgorin's
circles in the case of a \nm{5}{5} matrix $A$.  $U = U_1\cup U_2\cup U_3$ is a
component containing three eigenvalues (counted with multiplicity) of $A$.}
{eig_ggo_FIG}

\section{Power Method}

The first method that we present can be used to approximate the largest
eigenvalue in absolute value of an \nm{n}{n} matrix $A$.

Suppose that the \nn matrix $A$ has $m$ distinct eigenvalues such that
\begin{equation} \label{C14L6}
|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq \ldots \geq |\lambda_m| \ .
\end{equation}
We assume that there is a basis of eigenvectors of $A$ for $\RR^n$.
So, every vector in $\VEC{x} \in \RR^n$ can be expressed uniquely as a
sum $\displaystyle \VEC{x} = \sum_{j=1}^m \VEC{v}_j$, where
$\VEC{v}_j$ is an eigenvector associated to $\lambda_j$ or the null
vector.

Given $\VEC{y} \neq \VEC{0}$, we may write $\VEC{y}$ as
$\displaystyle \VEC{y} = \sum_{j=1}^m \VEC{v}_j$ where
$\VEC{v}_j$ is an eigenvector associated to $\lambda_j$ for each $j$.
It is easy to prove by induction that
\[
A^j \VEC{y} = \sum_{i=1}^m  \lambda_i^j \VEC{v}_i
= \lambda_i^j \left(\VEC{v}_1 + \sum_{i=2}^m
\left(\frac{\lambda_i}{\lambda_1}\right)^j \VEC{v}_i \right)
\]
for $j\geq 0$.  If
\[
\psi_j = \sum_{i=2}^m \left(\frac{\lambda_i}{\lambda_1}\right)^j \VEC{v}_i \ ,
\]
we have that $\psi_j \rightarrow 0$ as $j\rightarrow 0$ because of 
(\ref{C14L6}), and so
$\displaystyle \lambda_1^{-j}\, A^j\VEC{y} = \VEC{v}_1 + \psi_j \to \VEC{v}_1$
as $j \rightarrow \infty$.

Let $\phi:\RR^n \rightarrow \RR$ be a linear functional
such that $\phi(\VEC{v}_1) \neq 0$.  We have that
\[
\mu_j = \frac{\phi(A^j\VEC{y})}{\phi(A^{j-1}\VEC{y})}
= \lambda_1 \frac{\phi\left( \VEC{v}_1 + \psi_j \right)}
{\phi\left( \VEC{v}_1 + \psi_{j-1}\right)} \rightarrow
\lambda_1 \frac{\phi\left( \VEC{v}_1\right)}
{\phi\left( \VEC{v}_1\right)} = \lambda_1
\quad \text{as} \quad j\rightarrow \infty \ .
\]
There are infinitely many possible choices for the linear functional
$\phi$.  A linear functional that is often used is defined by
$\phi(\VEC{x}) = x_k$, the $k^{th}$ component of the vector $\VEC{x}$,
for $k$ constant.

Generally, the sequence $\displaystyle \left\{ \mu_j \right\}_{j=1}^\infty$
converge linearly to $\lambda_1$.  We have
\begin{align*}
\left| \frac{\mu_{j+1}-\lambda_1}{\mu_j-\lambda_1} \right|
&=\left|\left(\lambda_1 \frac{\phi\left(\VEC{v}_1+\psi_{j+1}\right)}
{\phi\left(\VEC{v}_1+\psi_j\right)} - \lambda_1 \right)
\left(\lambda_1\frac{\phi\left(\VEC{v}_1+\psi_{j}\right)}
{\phi\left(\VEC{v}_1+\psi_{j-1}\right)} - \lambda_1 \right)^{-1}\right| \\
&=\left| \left(\frac{\phi\left(\VEC{v}_1+\psi_{j+1}\right)
-\phi\left(\VEC{v}_1+\psi_j\right)}
{\phi\left(\VEC{v}_1+\psi_{j}\right) -
\phi\left(\VEC{v}_1+\psi_{j-1}\right)}\right)
\left( \frac{\phi\left(\VEC{v}_1+\psi_{j-1}\right)}
{\phi\left(\VEC{v}_1+\psi_{j}\right)} \right)\right|\\
&=\left| \left(\frac{\phi\left(\psi_{j+1}\right)-\phi\left(\psi_j\right)}
{\phi\left(\psi_{j}\right)-\phi\left(\psi_{j-1}\right)}\right)
\left( \frac{\phi\left(\VEC{v}_1\right)+\phi\left(\psi_{j-1}\right)}
{\phi\left(\VEC{v}_1\right)+\phi\left(\psi_{j}\right)} \right)\right| \ ,
\end{align*}
where the linearity of $\phi$ has been used to get the last equality.
If we assume that $|\lambda_3|< |\lambda_2|$, then
\begin{align*}
&\frac{\phi\left(\psi_{j+1}\right)-\phi\left(\psi_j\right)}
{\phi\left(\psi_{j}\right)-\phi\left(\psi_{j-1}\right)}
= \frac{\displaystyle \sum_{i=2}^m
\left(\frac{\lambda_i}{\lambda_1}\right)^{j+1} \phi(\VEC{v}_i)
-\sum_{i=2}^m \left(\frac{\lambda_i}{\lambda_1}\right)^j \phi(\VEC{v}_i)}
{\displaystyle \sum_{i=2}^m
\left(\frac{\lambda_i}{\lambda_1}\right)^j \phi(\VEC{v}_i)
-\sum_{i=2}^m \left(\frac{\lambda_i}{\lambda_1}\right)^{j-1} \phi(\VEC{v}_i)} \\
&\qquad\quad =\left(\frac{\lambda_2}{\lambda_1}\right)
\frac{\displaystyle \left(\frac{\lambda_2}{\lambda_1} - 1\right)
\phi(\VEC{v}_2) +\sum_{i=3}^m \left(\frac{\lambda_i}{\lambda_1} - 1\right)
\left(\frac{\lambda_i}{\lambda_2}\right)^j \phi(\VEC{v}_i)}
{\displaystyle \left(\frac{\lambda_2}{\lambda_1}-1\right)
\phi(\VEC{v}_2) +\sum_{i=3}^m \left(\frac{\lambda_i}{\lambda_1} - 1\right)
\left(\frac{\lambda_i}{\lambda_2}\right)^{j-1} \phi(\VEC{v}_i)}
\rightarrow \left(\frac{\lambda_2}{\lambda_1}\right) \neq 0
\quad \text{as} \quad j\rightarrow \infty
\end{align*}
because $|\lambda_i|/|\lambda_2| <1 $ for
$3\leq i \leq n$.  Moreover
\[
\frac{\phi\left(\VEC{v}_1\right)+\phi\left(\psi_{j-1}\right)}
{\phi\left(\VEC{v}_1\right)+\phi\left(\psi_{j}\right)}
\rightarrow \frac{\phi\left(\VEC{v}_1\right)}
{\phi\left(\VEC{v}_1\right)}=1 \quad \text{as} \quad
j \rightarrow \infty \ .
\]
Thus
\[
\lim_{j\rightarrow \infty}
\left| \frac{\mu_{j+1}-\lambda_1}{\mu_j-\lambda_1} \right|
= \left| \frac{\lambda_2}{\lambda_1} \right| \neq 0
\]
if $|\lambda_3| < |\lambda_2|$.  The method
may converge less than linearly if $|\lambda_3| = |\lambda_2|$.
Even when the convergence is linear, it could still be very slow if
$|\lambda_1| \approx |\lambda_2|$.  There is also the danger of
divisions by very small numbers if $A^j\VEC{y}$ approaches the origin when
$j \to \infty$ \footnote{To avoid divisions by very small numbers, one
may generate the $\VEC{y}_j$ as it follows:
$\VEC{y}_0 = \|\VEC{y}\|^{-1}\VEC{y}$ and
$\VEC{y}_j = \| A\VEC{y}_{j-1}\|^{-1} A\VEC{y}_{j-1}$ for $j> 0$.
Then $\displaystyle \mu_j = \frac{\phi(\VEC{y}_j)}{\phi(\VEC{y}_{j-1})}$
for $j>0$.}.
So, the power method is not that powerful.  It will need to be
improved.  One may use Aitken's $\Delta^2$ procedure to accelerate the
convergence toward the eigenvalue $\lambda_1$ but even that is not a
huge improvement.

If $\lambda_1$ is real and positive, the sequence
$\displaystyle \left\{\VEC{w}\right\}_{j=0}^\infty$ defined by
$\displaystyle \VEC{w}_j = \frac{1}{\|A^j\VEC{y}\|}\,A^j\VEC{y}$ converges
to $\displaystyle \frac{1}{\|\VEC{v}_1\|}\, \VEC{v}_1$, an
eigenvector of norm one associated to the eigenvalue $\lambda_1$,
because
\[
\VEC{w}_j = \frac{1}{\|A^j\VEC{y}\|}\, A^j\VEC{y}
= \displaystyle \left\| \sum_{i=1}^m \lambda_i^j \VEC{v}_i\right\|^{-1}
\left( \sum_{i=1}^m \lambda_i^j \VEC{v}_i \right)
= \frac{1}{\displaystyle \left\| \VEC{v}_1 + \psi_j \right\|}
\left( \VEC{v}_1 + \psi_j \right)
\to \frac{1}{\displaystyle \| \VEC{v}_1 \|} \, \VEC{v}_1
\quad \text{as} \quad  j \to \infty \ .
\]
In general, the vector $\VEC{w}_j$ is getting ``more parallel'' to
the direction of the eigenvector $\VEC{v}_1$ as $j \to \infty$.

\section{Rayleigh Quotient for Symmetric Matrices}

We consider a \nn symmetric matrix $A$.  As for the iterative power method
of the previous section, we assume that $A$ has $m$ distinct
eigenvalues which are real according to Theorem~\ref{C14L5}.
\[
|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq \ldots \geq |\lambda_m| \ .
\]
Moreover, there exists an orthonormal basis of eigenvectors of $A$.
As we mentioned in the previous section in such case, 
every vector in $\VEC{x} \in \RR^n$ can be expressed uniquely as a
sum $\displaystyle \VEC{x} = \sum_{j=1}^m \VEC{v}_j$, where
$\VEC{v}_j$ is an eigenvector associated to $\lambda_j$ or the null
vector.

\begin{defn}
The {\bfseries Rayleigh Quotient}\index{Rayleigh Quotient} of the
symmetric matrix $A$ is the function
\[
\rho_A(\VEC{x}) = \frac{\ps{\VEC{x}}{A\VEC{x}}}{\ps{\VEC{x}}{\VEC{x}}}
\]
for $\VEC{x} \neq \VEC{0}$.
\end{defn}

Given $\VEC{y} \neq \VEC{0}$, we write
$\displaystyle \VEC{y} = \sum_{j=1}^m \VEC{v}_j$, where $\VEC{v}_j$ is
an eigenvector associated to $\lambda_j$.  The sequence
$\displaystyle \{\mu_j\}_{j=1}^\infty$ defined by
$\displaystyle \mu_j = \rho_A(A^j\VEC{y})$
converges to $\lambda_1$.  To prove it, let
\[
\psi_k = \sum_{i=2}^m
\left(\frac{\lambda_i}{\lambda_1}\right)^k \VEC{v}_i^\top \VEC{v}_i \ .
\]
We then have
\begin{align*}
\mu_j &= \frac{(A^j\VEC{y})^\top A (A^j \VEC{y})}
{(A^j\VEC{y})^\top (A^j \VEC{y})}
= \frac{\VEC{y}^\top A^{2j+1} \VEC{y}}
{\VEC{y}^\top A^{2j} \VEC{y}}
= \frac{\displaystyle \sum_{i=1}^m \lambda_i^{2j+1} \VEC{v}_i^\top\VEC{v}_i}
{\displaystyle \sum_{i=1}^m \lambda_i^{2j} \VEC{v}_i^\top \VEC{v}_i}
= \lambda_1 \left(
\frac{\displaystyle \VEC{v}_1^\top \VEC{v}_1 + \psi_{2j+1}}
{\displaystyle \VEC{v}_1^\top \VEC{v}_1 + \psi_{2j}} \right)
\rightarrow \lambda_1
\quad \text{as} \quad j\rightarrow \infty
\end{align*}
because $|\lambda_i/\lambda_1|<1$ for all $i>1$.

The sequence $\displaystyle \left\{ \mu_j \right\}_{j=1}^\infty$
converges much faster to the eigenvalue $\lambda_1$ than the simple power
method of the previous section.

As for the power method of the previous section, if $\lambda_1 > 0$,
the sequence $\displaystyle \left\{\VEC{w}\right\}_{j=0}^\infty$
defined by $\displaystyle \VEC{w}_j = \frac{1}{\|A^j\VEC{y}\|}A^j\VEC{y}$
converges to $\displaystyle \frac{1}{\|\VEC{v}_1\|}\, \VEC{v}_1$, an
eigenvector of norm one associated to the eigenvalue $\lambda_1$.
If $\lambda_1 < 0$, the sequence
$\displaystyle \left\{\VEC{w}\right\}_{j=0}^\infty$
defined by $\displaystyle \VEC{w}_j = \frac{1}{\|A^{2j}\VEC{y}\|}A^{2j}\VEC{y}$
converges to $\displaystyle \frac{1}{\|\VEC{v}_1\|}\, \VEC{v}_1$.

\section{Inverse Power Method}

Until now, we have presented methods to approximate the largest
eigenvalue in absolute value of a \nm{n}{n} matrix $A$.  How can we
find the other eigenvalues?  We present in this section one possible
method to answer this question.  Suppose that
$\displaystyle \left\{ \lambda_i \right\}_{i=1}^n$
are the eigenvalues of $A$ counted with their algebraic multiplicity.

Choose $q \neq \lambda_i$ for all $i$.  The matrix $A-q\Id$ is invertible and
the eignevalues of $(A-q\Id)^{-1}$ are of the form
$\displaystyle 1/(\lambda_i + q)$, where $\lambda_i$ is an eigenvalue
of $A$.  In fact, $\VEC{v}_i$ is an eigenvector of $A$ associated to the
eigenvalue $\lambda_i$ if and only if
\[
(\lambda_i-q) \VEC{v}_i = \left(A-q\Id\right) \VEC{v}_i \ .
\]
This is if and only if
\[
\left(A-q\Id\right)^{-1} \VEC{v}_i = \frac{1}{\lambda_i-q} \VEC{v}_i \ .
\]
This last equation says that $\VEC{v}_i$ is an eigenvector of $A-q\Id$
associated to the eigenvalue $\displaystyle 1/(\lambda_i - q)$.

Suppose that $k$ is an index such that
$\displaystyle 1/|\lambda_k - q| > 1/|\lambda_i -q|$
for $i \neq k$.  We may then use the iterative power method with
$(A-q\Id)^{-1}$ instead of $A$, to approximate the eigenvalue
$1/(\lambda_k - q)$ and an eigenvector associated to this eigenvalue.
This gives us an approximation of the eigenvalue $\lambda_k$ of $A$.

If $A$ is symmetric, then $(A-q\Id)^{-1}$ is also a symmetric matrix.  Hence,
we may use the Rayleigh quotient to approximate the eigenvalue
$\displaystyle 1/(\lambda_k - q)$ of $(A-q\Id)^{-1}$.

\section{Householder's Matrices and Hessemberg Forms}

The {\bfseries (principal) subdiagonal}\index{Matrices!Principal Subdiagonal}
of an \nn matrix $B$ is
the set formed by the components $b_{i+1,i}$ for $i=1$, $2$, \ldots $n-1$.
Given an \nn matrix $A$, the goal of this section is to find a matrix $B$
conjugate to $A$ such that the elements below the principal subdiagonal are
zero  (i.e.\ $b_{i,j}=0$ for $i>j+1$).  If $A$ is symmetric, then $B$ is also
symmetric.  Thus $B$ satisfies $b_{i,j} = 0$ for $|i-j|\geq 2$.  Such
matrices are called
{\bfseries tridiagonal matrices}\index{Matrices!Tridiagonal Matrices}.

In the next section, we will present a method to find eignevalues of
symmetric tridiagonal matrices like $B$ above.  But first, we have to
review some concepts in linear algebra.

\begin{defn}
Let $\VEC{w} \in \RR^n$ be a non-null vector.  the
\nn {\bfseries Householder matrix}\index{Matrices!Householder}
$H_{\VEC{w}}$ is defined by
\[
H_{\VEC{w}} = \Id_n - \left(\frac{2}{\VEC{w}^\top \VEC{w}}\right)
\VEC{w} \VEC{w}^\top \ .
\]
\end{defn}

We present a geometric interpretation of the Householder matrix.  Let
$\Pi$ be the $n-1$ dimensional subspace of $\RR^n$ defined by 
$\Pi = \{ \VEC{v} \in \RR^n : \VEC{v} \Bot \VEC{w}\}$ and
$L:\RR^n \to \RR^n$ be the reflection through $\Pi$.

\pdfbox{eigenvalues_A/householder}

$L$ is a linear mapping.   If $\VEC{y} = L(\VEC{x})$, we have that
$\VEC{y} = \VEC{x} + \alpha \VEC{w}$ for some $\alpha \in \RR$.
Thus $\displaystyle \VEC{z} = \VEC{x} + \frac{\alpha}{2} \VEC{w} \in \Pi$
and $\VEC{z} \Bot \VEC{w}$.

From
\[
0 = \VEC{w}^\top \VEC{z}
= \VEC{w}^\top \left( \VEC{x} +  \frac{\alpha}{2} \VEC{w} \right)
= \VEC{w}^\top \VEC{x} +  \frac{\alpha}{2} \VEC{w}^\top \VEC{w} \ ,
\]
we get
\[
  \alpha = - \frac{2\, \VEC{w}^\top \VEC{x}}
    {\VEC{w}^\top\VEC{w}} \ .
\]
Hence,
\[
L(\VEC{x}) = \VEC{x} + \alpha \VEC{w}
= \VEC{x} - \left( \frac{2\, \VEC{w}^\top \VEC{x}}
{\VEC{w}^\top\VEC{w}} \right) \VEC{w}
= \VEC{x} - \left(\frac{2}{\VEC{w}^\top\VEC{w}}\right)
\VEC{w} \VEC{w}^\top \VEC{x}
= H_{\VEC{w}}(\VEC{x}) \ .
\]
Thus $H_{\VEC{w}}$ is the reflection through the subspace orthogonal
to $\VEC{w}$.

\begin{theorem}
Let $H_{\VEC{w}}$ be an \nn Householder matrix.  Then
\begin{enumerate}
\item $H_{\VEC{w}}$ is symmetric and orthogonal.
\item $H_{\VEC{w}}(\VEC{x})$ is the reflection of the vector $\VEC{x}$
through the subspace orthogonal to $\VEC{w}$.
\item $\det(H_{\VEC{w}}) = -1$.
\item \label{C14L7}
For any $\VEC{x}$ and $\VEC{y}$ with $\VEC{x} \neq \VEC{y}$, there
exists $\VEC{w} \in \RR^n$ such that $H_{\VEC{w}}(\VEC{x})$ is a scalar 
multiple of $\VEC{y}$.  In fact,
\begin{description}
\item[(i)] if $\VEC{x} \neq \lambda \VEC{y}$ with $\lambda > 0$, then we can
take $\displaystyle \VEC{w} = \VEC{x} -
\frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\,\VEC{y}$.  We get
$\displaystyle H_{\VEC{w}}(\VEC{x}) =
\frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y}$.
\item[(ii)] if $\VEC{x} \neq \lambda \VEC{y}$ with $\lambda < 0$, then we can
take $\displaystyle
\VEC{w} = \VEC{x} + \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\,\VEC{y}$.
We get $\displaystyle H_{\VEC{w}}(\VEC{x}) =
-\frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\,\VEC{y}$.
\end{description}
\end{enumerate}
\label{C14L8}
\end{theorem}

\begin{proof}
\stage{1} We have
\begin{align*}
  H_{\VEC{w}}^\top &=
\left(\Id_n - \left(\frac{2}{\VEC{w}^\top \VEC{w}}\right)
  \VEC{w} \VEC{w}^\top \right)^\top
= \Id_n^\top - \left(\frac{2}{\VEC{w}^\top \VEC{w}}\right)
  \left( \VEC{w} \VEC{w}^\top \right)^\top \\
&=\Id_n - \left(\frac{2}{\VEC{w}^\top \VEC{w}}\right)
\VEC{w} \VEC{w}^\top = H_{\VEC{w}} \ .
\end{align*}
Thus, $H_{\VEC{w}}$ is symmetric.  Moreover,
\begin{align*}
H_{\VEC{w}}^2 &= \left(\Id_n - \left(\frac{2}{\VEC{w}^\top \VEC{w}}\right)
\VEC{w} \VEC{w}^\top \right)
\left(\Id_n - \left(\frac{2}{\VEC{w}^\top \VEC{w}}\right)
  \VEC{w} \VEC{w}^\top \right) \\                
&= \Id_n - \left(\frac{4}{\VEC{w}^\top \VEC{w}}\right) \VEC{w} \VEC{w}^\top
+ \left(\frac{4}{(\VEC{w}^\top \VEC{w})^2}\right) \VEC{w} \VEC{w}^\top
\VEC{w} \VEC{w}^\top \\
&= \Id_n - \left(\frac{4}{\VEC{w}^\top \VEC{w}}\right) \VEC{w} \VEC{w}^\top
+ \left(\frac{4}{(\VEC{w}^\top \VEC{w})^2}\right)
\VEC{w} \left( \VEC{w}^\top \VEC{w} \right) \VEC{w}^\top = \Id_n \ .
\end{align*}
Thus $H_{\VEC{w}}^{-1} = H_{\VEC{w}} = H_{\VEC{w}}^\top$ implies that
$H_{\VEC{w}}$ is orthogonal.

\stage{2}  This has been proved before the statement of the theorem.

\stage{3} Let $\{\VEC{v}_1, \VEC{v}_2, \ldots, \VEC{v}_{n-1}\}$ be an
orthogonal basis of
$\Pi = \{ \VEC{v} \in \RR^n : \VEC{v} \Bot \VEC{w}\}$.  Then \\
$\{\VEC{w}, \VEC{v}_1, \VEC{v}_2, \ldots, \VEC{v}_{n-1}\}$ is an
orthogonal basis of $\RR^n$.  Since $H_{\VEC{w}}(\VEC{w}) = -\VEC{w}$
and $H_{\VEC{w}}(\VEC{v}_i) = \VEC{v}_i$ for all $i$, we have that
$-1$ is an eigenvalue of algebraic and geometric multiplicity one
while $1$ is an eigenvalue of algebraic and geometric multiplicity
$n-1$.  Hence, since $\det(H_{\VEC{w}})$ is equal to the product of the
eigenvalues, we have that $\det(H_{\VEC{w}}) = -1$.

Another way to show that $\det(H_{\VEC{w}}) = -1$ is to consider the
\nn matrix\\
$A = \begin{pmatrix} \VEC{w} & \VEC{v}_1 & \VEC{v}_2 &
\ldots & \VEC{v}_{n-1} \end{pmatrix}$.   We have that
$H_{\VEC{w}} A = \begin{pmatrix} -\VEC{w} & \VEC{v}_1 & \VEC{v}_2 &
\ldots & \VEC{v}_{n-1} \end{pmatrix}$.   Since $H_{\VEC{w}} A$ is
obtained from $A$ by multiplying the first column of $A$ by $-1$,
we have that
\[
\det(H_{\VEC{w}}) \det(A) = \det(H_{\VEC{w}} A) =  -\det(A)  
\]
Since $\det(A) \neq 0$, we get $\det(H_{\VEC{w}}) = -1$.

\stage{4} For (i). it is enough to prove that
$\displaystyle \frac{2\,\VEC{w}^\top \VEC{x}}{\VEC{w}^\top\VEC{w}} = 1$
for
$\displaystyle \VEC{w} = \VEC{x} - \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}
\, \VEC{y}$ because this will implies that
\[
H_{\VEC{w}}(\VEC{x}) = \VEC{x} -\left(\frac{2}{\VEC{w}^\top\VEC{w}}\right)
\VEC{w}\VEC{w}^\top \VEC{x}
= \VEC{x} - \left(\frac{2\,\VEC{w}^\top \VEC{x}}{\VEC{w}^\top\VEC{w}}\right)
\VEC{w}
= \VEC{x} - \VEC{w} = \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\, \VEC{y} \ .
\]
\pdfbox{eigenvalues_A/householder_1}

Since $\VEC{x} \neq \lambda \VEC{y}$ with $\lambda > 0$, we have that
$\VEC{w} \neq \VEC{0}$.   Hence,
\[
\VEC{w}^\top\VEC{x} =
\left( \VEC{x} - \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y} \right)^\top
\VEC{x}
= \VEC{x}^\top \VEC{x}
- \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y}^\top \VEC{x}
= \|\VEC{x}\|_2^2- \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y}^\top \VEC{x}
\]
and
\begin{align*}
\VEC{w}^\top\VEC{w} &= 
\left(\VEC{x}^\top - \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\, \VEC{y}^\top \right)
\left(\VEC{x} - \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y} \right)
= \VEC{x}^\top \VEC{x} - \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}
\underbrace{\left( \VEC{y}^\top\VEC{x} + \VEC{x}^\top\VEC{y} \right)
}_{= 2\, \VEC{y}^\top\VEC{x}}
+ \frac{\|\VEC{x}\|_2^2}{\|\VEC{y}\|_2^2} \, \VEC{y}^\top \VEC{y} \\
&= 2 \left( \|\VEC{x}\|_2^2 -
\frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\, \VEC{y}^\top\VEC{x}\right) \ .
\end{align*}
Thus $\displaystyle \frac{2\,\VEC{w}^\top \VEC{x}}{\VEC{w}^\top\VEC{w}} = 1$.

For (ii). it is also enough to prove that
$\displaystyle \frac{2\,\VEC{w}^\top \VEC{x}}{\VEC{w}^\top\VEC{w}} = 1$
for
$\displaystyle \VEC{w} = \VEC{x} + \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}
\, \VEC{y}$ because this will implies that
\[
H_{\VEC{w}}(\VEC{x}) = \VEC{x} -\left(\frac{2}{\VEC{w}^\top\VEC{w}}\right)
\VEC{w}\VEC{w}^\top \VEC{x}
= \VEC{x} - \left(\frac{2\,\VEC{w}^\top \VEC{x}}{\VEC{w}^\top\VEC{w}}\right)
\VEC{w}
= \VEC{x} - \VEC{w} = - \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\, \VEC{y} \ .
\]
\pdfbox{eigenvalues_A/householder_2}

Since $\VEC{x} \neq \lambda \VEC{y}$ with $\lambda < 0$, we have that
$\VEC{w} \neq \VEC{0}$.   Hence,
\[
\VEC{w}^\top\VEC{x} =
\left( \VEC{x} + \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y} \right)^\top
\VEC{x}
= \VEC{x}^\top \VEC{x}
+ \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y}^\top \VEC{x}
= \|\VEC{x}\|_2^2+ \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y}^\top \VEC{x}
\]
and
\begin{align*}
\VEC{w}^\top\VEC{w} &= 
\left(\VEC{x}^\top + \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\, \VEC{y}^\top \right)
\left(\VEC{x} + \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2} \, \VEC{y} \right)
= \VEC{x}^\top \VEC{x} + \frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}
\underbrace{\left( \VEC{y}^\top\VEC{x} + \VEC{x}^\top\VEC{y} \right)
}_{= 2\, \VEC{y}^\top\VEC{x}}
+ \frac{\|\VEC{x}\|_2^2}{\|\VEC{y}\|_2^2} \, \VEC{y}^\top \VEC{y} \\
&= 2 \left( \|\VEC{x}\|_2^2 +
\frac{\|\VEC{x}\|_2}{\|\VEC{y}\|_2}\, \VEC{y}^\top\VEC{x}\right) \ .
\end{align*}
Thus $\displaystyle \frac{2\,\VEC{w}^\top \VEC{x}}{\VEC{w}^\top\VEC{w}} = 1$.
\end{proof}

\begin{algo}[QR Decomposition with Householder Matrices]
Let $A$ be a \nm{n}{m} matrix with entries in $\RR$.
\begin{enumerate}
\item  Use item~\ref{C14L7} of Theorem~\ref{C14L8} to find
$\VEC{w} \in \RR^n$ such that $H_{\VEC{w}}$ maps the first column of $A$ to a
non-negative multiple of $\VEC{e}_1$ in $\RR^n$.  If the first column of $A$
is already a multiple of $\VEC{e}_1 \in \RR^n$, take $\VEC{w}=\VEC{0}$.
\item Let $Q_1 = H_{\VEC{w}}$ (when the first column of $A$ is already a
multiple of $\VEC{e}_1 \in \RR^n$, then $Q_1 = \Id_n$) and let
$A_1 = Q_1 A$. 
The matrix $Q_1$ is an orthogonal matrix.  $A_1$ is of the form
\[
A_1 = \begin{pmatrix}
R_1 & B_1 \\
0 & C_1
\end{pmatrix} \ ,
\]
where $R_1 \in \RR$.
\item Suppose that $A_i$ is of the form
\[
A_i = \begin{pmatrix}
R_i & B_i \\
0 & C_i
\end{pmatrix} \ ,
\]
where $R_i$ is an \nm{i}{i} upper-triangular matrix.  Use item~\ref{C14L7}
of Theorem~\ref{C14L8} to find $\VEC{w} \in \RR^{n-i}$ such that
$H_{\VEC{w}}$ maps the first column of $C_i$ to a non-negative multiple of
$\VEC{e}_1$ in $\RR^{n-i}$.  If the first column of $C_i$ is already a
non-negative multiple of $\VEC{e}_1 \in \RR^{n-i}$, take $\VEC{w} = \VEC{0}$.
\item Let
\[
Q_{i+1} = \begin{pmatrix}
\Id_i & 0 \\
0 & H_{\VEC{w}}
\end{pmatrix}
\]
(when the first column of $C_i$ is already a non-negative multiple of
$\VEC{e}_1 \in \RR^{n-i}$, then $Q_{i+1} = \Id_n$) and let
$A_{i+1} = Q_{i+1} A_i$.  The matrix $Q_{i+1}$ is an orthogonal matrix.
$A_{i+1}$ is of the form
\[
A_{i+1} = \begin{pmatrix}
R_{i+1} & B_{i+1} \\
0 & C_{i+1}
\end{pmatrix} \ ,
\]
where $R_i$ is an \nm{(i+1)}{(i+1)} upper-triangular matrix.
\item Repeat (3) and (4) with $i$ replace by $i+1$ until $i=n-1$.
\end{enumerate}
Then $Q_n Q_{n-1} \cdots Q_1 A = R$ is an upper-triangular matrix with
non-negative entries on the main diagonal.  If
$Q = Q_1 Q_2 \cdots Q_n$, then $Q$ is an orthogonal matrix such that
$A = Q R$.
\label{C14L9}
\end{algo}

The QR decomposition with Householder matrices gives a method to solve linear
systems of equations of the form $A\VEC{x} = \VEC{b}$, where $A$ is an 
\nm{n}{m} matrix and $\VEC{b}\in \RR^n$ .  Suppose that
$A=Q R$ is the QR decomposition of $A$.  Since $Q^{-1}=Q^\top$, $\VEC{x}$
is the solution of $A\VEC{x} = \VEC{b}$ if and only if $\VEC{x}$ is the
solution of $R\VEC{x} = Q^\top \VEC{b}$.  The solution $\VEC{x}$ of
$R\VEC{x} = Q^\top \VEC{b}$ is found using backward substitution.

\begin{defn}
We say that an \nn matrix $M$ is in
{\bfseries Hessemberg form}\index{Matrices!Hessemberg Form} if
$m_{i,j} = 0$ for $j + 1 < i \leq n$ and $1\leq j \leq n-2$.
\end{defn}

\begin{algo}[Hessemberg form]
Let $A$ be a \nn-matrix with entries in $\RR$.
\begin{enumerate}
\item  Suppose that
\[
A = \begin{pmatrix}
T & \VEC{r}^\top \\
\VEC{s} & C
\end{pmatrix} \ ,
\]
where $T \in \RR$.  Use item~\ref{C14L7} of Theorem~\ref{C14L8} to
find $\VEC{w}_1 \in \RR^{n-1}$ such that $H_{\VEC{w}_1}$ maps $\VEC{s}$ to a
multiple of $\VEC{e}_1$ in $\RR^{n-1}$.  If $\VEC{s}$ is already a
non-negative multiple of $\VEC{e}_1 \in \RR^{n-1}$, take
$\VEC{w}_1=\VEC{0}$.
\item  Let
\[
G_1 = \begin{pmatrix}
1 & 0 \\
0 & H_{\VEC{w}_1}
\end{pmatrix}
\]
(when $\VEC{s}$ is already a non-negative multiple of
$\VEC{e}_1 \in \RR^{n-1}$, then $G_1 = \Id_n$) and let $A_1 = G_1 A G_1$.
The matrix $G_1$ is an orthogonal matrix.  $A_1$ is of the form
\[
A_1 = \begin{pmatrix}
T_1 & B_1 \\
0 & C_1
\end{pmatrix} \ ,
\]
where $T_1$ is an \nm{2}{1} matrix.
\item Suppose that $A_i$ is of the form
\[
A_i =
\begin{pmatrix}
T_i & B_i \\
0 & C_i
\end{pmatrix} \ ,
\]
where $M=T_i$ is an \nm{(i+1)}{i} matrix satisfying $M_{j,k} = 0$
for $j > k+1$.  Use item~\ref{C14L7} of Theorem~\ref{C14L8} to find
$\VEC{w}_{i+1} \in \RR^{n-i-1}$ such that $H_{\VEC{w}_{i+1}}$ maps the first
column of $C_i$ to a multiple of $\VEC{e}_1$ in $\RR^{n-i-1}$.  If the first
column of $C_i$ is already a non-negative multiple of
$\VEC{e}_1 \in \RR^{n-i-1}$, take $\VEC{w}_{i+1} = \VEC{0}$.
\item Let
\[
G_{i+1} = \begin{pmatrix}
\Id_{i+1} & 0 \\
0 & H_{\VEC{w}_{i+1}}
\end{pmatrix}
\]
(when the first column of $C_i$ is already a non-negative
multiple of $\VEC{e}_1 \in \RR^{n-i-1}$, then $G_{i+1} = \Id_n$) and let
$A_{i+1} = G_{i+1} A_i G_{i+1}$.  The matrix $G_{i+1}$ is an orthogonal
matrix.  $A_{i+1}$ is of the form
\[
A_{i+1} =
\begin{pmatrix}
T_{i+1} & B_{i+1} \\
0 & C_{i+1}
\end{pmatrix} \ ,
\]
where $M=T_{i+1}$ is an \nm{(i+2)}{(i+1)} matrix satisfying $M_{j,k} = 0$
for $j > k+1$.
\item Repeat (3) and (4) with $i$ replace by $i+1$ until $i=n-3$.
\end{enumerate}
Then $T=G_{n-2} G_{n-3} \ldots G_1 A G_1 G_2 \cdots G_{n-2}$ is a matrix 
in the Hessemberg form.  If $G = G_1 G_2 \cdots G_{n-2}$, then the matrix $G$
is an orthogonal matrix such that $A = G^\top T G$.
\label{C14L10}
\end{algo}

Our goal is now to implement efficiently the previous theorem to get, for any
given matrix $A$, an Hessemberg form conjugate to $A$.  The implementation
presented is based on \cite{BT}.

\subsection{Finding the vector $\VEC{w}_i$}

The first task is to find an efficient way to find the vector $\VEC{w}_i$.
Without lost of generality, we will assume that $\|\VEC{w}_i\|_2 = 1$.
This rule out the possibility of using item~\ref{C14L7} of
Theorem~\ref{C14L8} to find $\VEC{w}_i$.  Though this complicates
the procedure to find $\VEC{w}_i$, it is a small price to pay to get a
more efficient procedure to compute $G_i A_{i-1} G_i$ later.

We have
\[
G_i =
\begin{pmatrix}
\Id_{i} & 0 \\
0 & H_{\VEC{w}_i}
\end{pmatrix}
\  ,
\]
where $M=H_{\VEC{w}_i}$ is an \nm{(n-i)}{(n-i)} matrix with the components
\[
m_{j,k} =
\begin{cases}
1-2w_{i,j}^2 & \quad \text{if} \quad j=k \\
- 2 w_{i,j}w_{i,k} & \quad \text{if} \quad j\neq k
\end{cases}
\]
for $1\leq j,k\leq n-i$, and $w_{i,j}$ is the $j^{th}$ coordinates of
the vector $\VEC{w}_i$.  We can write $A_{i-1}$ as
\[
A_{i-1} = 
\begin{pmatrix}
B & C \\
D & E
\end{pmatrix} \ ,
\]
where $B$ is an \nm{i}{i} matrix in Hessemberg form, $C$ is an
\nm{i}{(n-i)} matrix, $D$ is an \nm{(n-i)}{i} matrix of the form
\[
D =
\begin{pmatrix}
0 & 0 & \ldots & 0 & d_{1,i} \\
0 & 0 & \ldots & 0 & d_{2,i} \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & 0 & d_{n-i,i} \\
\end{pmatrix}
\]
and $E$ is an \nm{(n-i)}{(n-i)} matrix.  We then have
\[
G_i A_{i-1} G_i =
\begin{pmatrix}
\Id_i & 0 \\
0 & H_{\VEC{w}_i}
\end{pmatrix}
\begin{pmatrix}
B & C \\
D & E
\end{pmatrix}
\begin{pmatrix}
\Id_i & 0 \\
0 & H_{\VEC{w}_i}
\end{pmatrix}
=
\begin{pmatrix}
B & C H_{\VEC{w}_i} \\
H_{\VEC{w}_i} D & H_{\VEC{w}_i} E H_{\VEC{w}_i}
\end{pmatrix} \ ,
\]
where
\begin{align*}
H_{\VEC{w}_i} D &=
H_{\VEC{w}_i}
\begin{pmatrix}
0 & 0 & \ldots & 0 & d_{1,i} \\
0 & 0 & \ldots & 0 & d_{2,i} \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & 0 & d_{n-i,i}
\end{pmatrix}
=
\begin{pmatrix}
0 & 0 & \ldots & 0 & \displaystyle
d_{1,i} - 2 \sum_{k=1}^{n-i} w_{i,1}w_{i,k}d_{k,i} \\
0 & 0 & \ldots & 0 & \displaystyle
d_{2,i} - 2 \sum_{k=1}^{n-i} w_{i,2}w_{i,k}d_{k,i} \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & 0 & \displaystyle
d_{n-i,i} - 2 \sum_{k=1}^{n-i} w_{i,n-i}w_{i,k}d_{k,i}
\end{pmatrix} \ .
\end{align*}
We need to have
\begin{equation}\label{C14L11}
d_{m,i} - 2 w_{i,m} \sum_{k=1}^{n-i} w_{i,k}d_{k,i} = 0
\end{equation}
for $m=2$, $3$, \ldots, $n-i$.  Let
$\displaystyle \sigma_i = \sum_{k=1}^{n-i} w_{i,k}d_{k,i}$.
We have
\begin{equation} \label{C14L12}
\sum_{m=1}^{n-i} \left( d_{m,i} -2 w_{i,m} \sigma_i \right)^2 =
\sum_{m=1}^{n-i} d_{m,i}^2 - 4\sigma_i \sum_{m=1}^{n-i} w_{i,m} d_{m,i}
+4 \sigma_i^2 \sum_{m=1}^{n-i} w_{i,m}^2
= \sum_{m=1}^{n-i} d_{m,i}^2 \ ,
\end{equation}
where we have used $\|\VEC{w}_i\|_2=1$ and the definition of
$\sigma_i$ to get the last equality.  From (\ref{C14L11}), we have
$d_{m,i} -2 w_{i,m} \sigma_i = 0$ for $m=2$, $3$, \ldots, $n-i$.
Hence, we get from (\ref{C14L12}) that
\begin{equation} \label{C14L13}
d_{1,i} -2 w_{i,1} \sigma_i = \epsilon \sqrt{\sum_{m=1}^{n-i} d_{m,i}^2} \ ,
\end{equation}
where $\epsilon =1$ or $-1$.  Let
\begin{equation}\label{C14L14}
s_i = \sqrt{\sum_{m=1}^{n-i} d_{m,i}^2} \  .
\end{equation}
We may assume that $s_i \neq 0$.  If $s_i=0$, then we may take
$\VEC{w}_i=\VEC{0}$ because $d_{m,i}=0$ for $m=1$, $2$, \ldots, $n-i$.
It follows from the definition of $\sigma_i$, (\ref{C14L11}),
(\ref{C14L13}) and $\|\VEC{w}_i\|_2=1$ that
\begin{align*}
\sigma_i &= w_{i,1}d_{1,i} + \sum_{k=2}^{n-i} w_{i,k}d_{k,i} =
\left( 2 w_{i,1} \sigma_i + \epsilon s_i\right) w_{i,1}
+ \sum_{k=2}^{n-i} w_{i,k} \left( 2 w_{i,k} \sigma_i \right) \\
&= \epsilon s_i w_{i,1} + 2 \sigma_i \sum_{k=1}^{n-i} w_{i,k}^2
= \epsilon s_i w_{i,1} + 2 \sigma_i \ .
\end{align*}
Thus, $\sigma_i = -\epsilon s_i w_{i,1}$.  If we substitute this
expression in (\ref{C14L13}), we get
$d_{1,i} + 2 \epsilon w_{i,1}^2 s_i = \epsilon s_i$.  Hence,
\begin{equation} \label{C14L15}
w_{i,1}^2 = \frac{s_i - \epsilon d_{1,i}}{2s_i} \ .
\end{equation}
To avoid the possibility of a subtraction of two numbers almost equal, we
take $\epsilon = - \sgn(d_{1,i})$.  The formulae to compute $w_{i,k}$
for $k>1$ will involve a division by $w_{i,1}$.  It is therefore
important to compute $w_{i,1}$ as accurately as we can.  The formula
to compute $w_{i,1}$ is
\begin{equation} \label{C14L16}
w_{i,1}^2 = \frac{s_i + \left| d_{1,i} \right|}{2s_i} \ .
\end{equation}
For the other components of $\VEC{w}_i$, we use (\ref{C14L11}) to get
\[
0 = d_{m,i} - 2 w_{i,m} \sigma_i = d_{m,i} + 2 w_{i,m}
\left( \epsilon s_i w_{i,1} \right)
\]
for  $m=2$, $3$, \ldots, $n-i$.  Thus
\begin{equation} \label{C14L17}
w_{i,m} = \frac{-d_{m,i}}{2 \epsilon w_{i,1} s_i}
= \sgn(d_{1,i}) \frac{d_{m,i}}{2 w_{i,1} s_i}
\end{equation}
for $m=2$, $3$, \ldots, $n-i$.

Note that
\begin{align}
\VEC{w}_i^\top\VEC{w}_i &= \sum_{k=1}^{n-i} w_{i,k}^2
= w_{i,1}^2 + \sum_{k=2}^{n-i} w_{i,k}^2
= \frac{s_i + \epsilon d_{1,i}}{2s_i} + \sum_{k=2}^{n-i}
\frac{d_{k,i}^2}{4 w_{i,1}^2 s_i^2} \nonumber \\
&= \frac{s_i + \epsilon d_{1,i}}{2s_i} +
\frac{1}{4 w_{i,1}^2 s_i^2}\left(\sum_{k=1}^{n-i} d_{k,i}^2
- d_{1,i}^2\right)
= \frac{s_i + \epsilon d_{1,i}}{2s_i} +
\frac{1}{4 w_{i,1}^2 s_i^2}\left(s_i^2- d_{1,i}^2\right) \ .  \label{C14L18}
\end{align}
Since
\[
4 w_{i,1}^2 s_i^2 = 4 \left(\frac{s_i + \epsilon d_{1,i}}{2s_i}\right)
s_i^2 = 2\left( s_i + \epsilon d_{1,i}\right)s_i \ ,
\]
we get from (\ref{C14L18}) that
\begin{align*}
\VEC{w}_i^\top\VEC{w}_i &= \frac{s_i + \epsilon d_{1,i}}{2s_i}
+ \frac{1}{2\left( s_i + \epsilon d_{1,i}\right)s_i}
\left(s_i^2- d_{1,i}^2\right) \\
&= \frac{\left(s_i + \epsilon d_{1,i}\right)
\left( s_i + \epsilon d_{1,i}\right) + \left(s_i^2- d_{1,i}^2\right)}
{2\left( s_i + \epsilon d_{1,i}\right)s_i}
= \frac{2 s_i^2 + 2 \epsilon d_{1,i} s_i}
{2\left( s_i + \epsilon d_{1,i}\right)s_i} = 1
\end{align*}
as expected.

(\ref{C14L16}) and (\ref{C14L17}) are the formulae used to find
the vectors $\VEC{w}_i \in \RR^{n-i}$ for $i=1$, $2$, \ldots, $n-2$.

\subsection{Computing $\mathbf{G_i A_{i-1} G_i}$}

We now give an efficient way to compute $G_i A_{i-1} G_i$ for each $i$.  Let
\begin{equation} \label{C14L19}
\VEC{v}_i =
\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0 \\ \sgn(d_{1,i}) s_i + d_{1,i} \\
d_{2,i} \\ \vdots \\ d_{n-i,i}
\end{pmatrix} \ .
\end{equation}
Recall that the vector $\VEC{w}_i$ is represented algebraically by a
\nm{(n-i)}{1} column matrix.  We get from (\ref{C14L15}) and
(\ref{C14L17}) that
\[
\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0 \\ \VEC{w}_i
\end{pmatrix} = \frac{\sgn(d_{1,i})}{2 w_{i,1} s_i} \VEC{v}_i \ .
\]
Thus
\begin{align*}
G_i &=
\begin{pmatrix}
\Id_{i} & 0 \\
0 & H_{\VEC{w}_i}
\end{pmatrix}
= \Id_n - 2 \begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0 \\ \VEC{w}_i
\end{pmatrix} \begin{pmatrix}
0 & 0 & \cdots & 0 & \VEC{w}_i^\top
\end{pmatrix} \\
&= \Id_n - 2 \left(\frac{\sgn(d_{1,i})}{2 w_{i,1} s_i}\right)^2 \VEC{v}_i
\VEC{v}_i^\top
= \Id_n - \frac{1}{2 w_{i,1}^2 s_i^2} \VEC{v}_i \VEC{v}_i^\top \  .
\end{align*}
From (\ref{C14L16}), we get
\[
2 w_{i,1}^2 s_i^2 = 2 \left(\frac{s_i + \left|d_{1,i}\right|}{2s_i}\right)
s_i^2 = \left( s_i + \left| d_{1,i}\right| \right)s_i \ .
\]
Hence, if we define
\begin{equation} \label{C14L20}
\alpha_i = \frac{1}{\left( s_i + \left| d_{1,i}\right| \right)s_i} \  ,
\end{equation}
then $\displaystyle G_i = \Id_n - \alpha_i \VEC{v}_i \VEC{v}_i^\top$.
Let
\begin{equation}\label{C14L21}
\begin{split}
\VEC{x}_i &= \alpha_i A_{i-1} \VEC{v}_i \quad  , \quad
\VEC{y}_i = \alpha_i A_{i-1}^\top \VEC{v}_i \quad , \quad
\mu_i = \frac{1}{2} \alpha_i \VEC{v}_i^\top \VEC{x}_i \  , \\
\VEC{p}_i &= \VEC{y}_i - \mu_i \VEC{v}_i \quad \text{and} \quad
\VEC{q}_i = \VEC{x}_i - \mu_i \VEC{v}_i \  .
\end{split}
\end{equation}
We have
\begin{align}
G_i A_{i-1}G_i &= \left( \Id_n - \alpha_i \VEC{v}_i \VEC{v}_i^\top\right)
A_{i-1} \left( \Id_n - \alpha_i \VEC{v}_i \VEC{v}_i^\top \right) \nonumber \\
&= A_{i-1} - \alpha_i \VEC{v}_i \VEC{v}_i^\top A_{i-1} -
\alpha_i A_{i-1} \VEC{v}_i \VEC{v}_i^\top
+ \alpha_i^2 \VEC{v}_i \VEC{v}_i^\top A_{i-1} \VEC{v}_i \VEC{v}_i^\top
\nonumber \\
&= A_{i-1} - \VEC{v}_i \left( \alpha_i \VEC{v}_i^\top A_{i-1}
- \frac{1}{2} \alpha_i^2 \VEC{v}_i^\top A_{i-1} \VEC{v}_i \VEC{v}_i^\top
\right) \nonumber \\
&\qquad
- \left( \alpha_i A_{i-1} \VEC{v}_i
- \frac{1}{2} \alpha_i^2 \VEC{v}_i \VEC{v}_i^\top A_{i-1} \VEC{v}_i \right)
\VEC{v}_i^\top \nonumber \\
&= A_{i-1} - \VEC{v}_i \left( \VEC{y}_i^\top - \mu_i \VEC{v}_i^\top \right)
- \left( \VEC{x}_i - \mu_i \VEC{v}_i \right) \VEC{v}_i^\top \nonumber \\
&= A_{i-1} - \VEC{v}_i \VEC{p}_i^\top - \VEC{q}_i \VEC{v}_i^\top \  .
\label{C14L22}
\end{align}

We can combine (\ref{C14L14}), (\ref{C14L19}), (\ref{C14L20}),
(\ref{C14L21}) and (\ref{C14L22}) to get the following code.

\begin{code}[Householder Reduction Algorithm]
To produce a matrix $B$ in the Hessemberg form which is conjugate to the
given matrix $A$.\\
\subI{Input} The matrix $A$.\\
\subI{Output} The matrix $B$.
\small
\begin{verbatim}
% function B = householder(A)

function B = householder(A)
  dim = size(A,1);

  for i = 1:dim-2
    v = zeros(dim,1);
    z = v;

    % s_i
    s = norm(A(i+1:dim,i));

    if ( s != 0 )
      % alpha_i
      alpha = 1/( (s + abs(A(i+1,i)))*s );

      % v_i
      v(i+1,1) = sign(A(i+1,i))*s + A(i+1,i);    
      z(i+1,1) = alpha*v(i+1,1);
      v(i+2:dim,1) = A(i+2:dim,i);
      z(i+2:dim,1) = alpha*v(i+2:dim,1);

      % x_i  and  y_i
      x = A*z;
      y = A'*z;

      % mu_i
      mu = (alpha * (v' *x))/2;

      % p_i  and  q_i
      z = mu*v;
      p = y - z;
      q = x - z;

      %  A_i
      A = A - v * p' - q*v';
    end
  end
  B = A;
end
\end{verbatim}
\end{code}

\begin{rmkList}
\begin{enumerate}
\item The previous algorithm requires $O(4n^2)$ multiplications to compute
$A_i$ from $A_{i-1}$.  The direct product $G_i A_{i-1} G_i$ requires
$O(2n^3)$ multiplications.
\item If $A$ is symmetric, it is easy to prove by induction that the matrices
$A_i$ are symmetric for all $i$ because the $G_i$ are symmetric.  The
resulting matrix $B$ is a symmetric tridiagonal matrix.  The previous
algorithm may also be improved because $\VEC{x}_i = \VEC{y}_i$ and
$\VEC{p}_i = \VEC{q}_i$.
\end{enumerate}
\end{rmkList}

Since the resulting matrix $T$ given by the Householder Reduction Algorithm
is conjugate to the given matrix $A$, the matrices $A$ and $T$ have the same
eigenvalues.  In particular, if $A$ is symmetric, then $T$ is a symmetric
tridiagonal matrix.  The next section will present a method to compute the
eigenvalues of a symmetric tridiagonal matrix $T$, hence the eigenvalues of
$A$.

We now present a theoretical method to find the eigenvalues of a symmetric
tridiagonal \nn matrix $T = (t_{i,j})$.  We say theoretical because it
is not the best method to compute eigenvalues of a matrix.  Let
\[
M_i =
\begin{pmatrix}
t_{1,1}-\lambda & t_{1,2} & 0 & 0& \ldots & 0 & 0 & 0 \\
t_{2,1} & t_{2,2}-\lambda & t_{2,3} & 0 & \ldots & 0 & 0 & 0 \\
0 & t_{3,2} & t_{3,3} -\lambda & t_{3,4} &\ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & t_{i-1,i-2} & t_{i-1,i-1} - \lambda & t_{i-1,i} \\
0 & 0 & 0 & 0 & \ldots & 0 & t_{i,i-1} & t_{i,i} - \lambda
\end{pmatrix}
\]
and $p_i(\lambda) = \det M_i$ for $i=1$, $2$, \ldots, $n$.  We have $T=M_n$.
Let $p_0(\lambda) = 1$.  Developing the determinant of $M_i$ along the last
row and using the symmetry of $T$ (in particular, $t_{i,i-1} = t_{i-1,i}$),
we get
\begin{equation}\label{C14L23}
p_i(\lambda) = (t_{i,i}-\lambda) p_{i-1}(\lambda)
- t_{i,i-1}^2 p_{i-2}(\lambda) \quad , \quad i=2,4,\ldots, n \  .
\end{equation}
Only $3n-6$ multiplications are needed to compute the determinant of $T$; a
lot less than the $n!$ multiplication needed for a full ordinary \nn matrix.

\begin{theorem}
Consider a symmetric tridiagonal matrix $T$.  If $t_{i,i-1} \neq 0$ for
$2\leq i \leq n$, then the roots of $p_i$ are distinct and between
any two consecutive roots of $p_i$ there is a root of $p_{i-1}$.
\end{theorem}

\begin{proof}
\stage{1} We first show that $p_i$ and $p_{i-1}$ cannot have a common
root.  Suppose that $c$ is a common root of $p_i$ and $p_{i-1}$ for
some $i$.  If $i\geq 2$, we get from (\ref{C14L23}) that $c$ is
also a root of $p_{i-2}$ because $t_{i,i-1} \neq 0$.  Thus,
inductively, $c$ is a root of $p_0$ which is impossible because
$p_0(x) = 1$ for all $x$.

\stage{2} We prove by induction that the roots of $p_i$ are distinct
and between any two roots of $p_i$ there is a root of $p_{i-1}$

We have $p_1(\lambda) = t_{1,1} - \lambda$ and
$p_2(\lambda) = (t_{2,2}-\lambda)(t_{1,1}-\lambda) - t_{2,1}^2$.
The only root of $p_1$ is $t_{1,1}$.  Since
$p_2(t_{1,1}) = - t_{2,1}^2 < 0$ and $p_2$ is a polynomial of degree
$2$ of the form $p_2(\lambda) = \lambda^2 + l.o.t.$ ({\it l.o.t.}\
stands for lower order terms in $\lambda$), it has two distinct roots;
one smaller than $t_{1,1}$ and one bigger that $t_{1,1}$.   Thus, the
hypothesis of induction is true for $i=2$.

Let's assume that the hypothesis of induction is true for $i$.  We
have that
\[
  p_{i+1}(\lambda) = (t_{i+1,i+1}-\lambda) p_i(\lambda)
  - t_{i+1,i}^2 p_{i-1}(\lambda) \quad .
\]
Let $\alpha$ be the largest root of $p_i$.  We assume first that $i$
is even.  We have
\[
  p_{i-1}(\lambda) = (-1)^{i-1}\lambda^{i-1} + l.o.t.
  = - \lambda^{i-1} + l.o.t.
\]
Since $p_{i-1}$ does not have roots bigger than $\alpha$ because the
roots of $p_{i-1}$ are between the roots of $p_i$ by the induction
hypothesis, we have that $p_{i-1}(\alpha)< 0$.
Since
\[
  p_{i+1}(\alpha) = (t_{i+1,i+1}-\alpha) p_i(\alpha)
  - t_{i+1,i}^2 p_{i-1}(\alpha) = - t_{i+1,i}^2 p_{i-1}(\alpha) \quad ,
\]
we have that $p_{i+1}(\alpha)> 0$.  But we also have that
\[
  p_{i+1}(\lambda) = (-1)^{i+1}\lambda^{i+1} + l.o.t.
  = - \lambda^{i+1} + l.o.t.
\]
Thus, there must be a root of $p_{i+1}$ greater than $\alpha$.

Similarly, if $i$ is odd, we have
\[
  p_{i-1}(\lambda) = (-1)^{i-1}\lambda^{i-1} + l.o.t.
  = \lambda^{i-1} + l.o.t.
\]
Since $p_{i-1}$ does not have roots bigger than $\alpha$ because the
roots of $p_{i-1}$ are between the roots of $p_i$ by the induction
hypothesis, we have that $p_{i-1}(\alpha)> 0$.
Since
\[
  p_{i+1}(\alpha) = (t_{i+1,i+1}-\alpha) p_i(\alpha)
  - t_{i+1,i}^2 p_{i-1}(\alpha) = - t_{i+1,i}^2 p_{i-1}(\alpha) \quad ,
\]
we have that $p_{i+1}(\alpha)< 0$.  But we also have that
\[
  p_{i+1}(\lambda) = (-1)^{i+1}\lambda^{i+1} + l.o.t.
  = \lambda^{i+1} + l.o.t.
\]
Thus, again, there must be a root of $p_{i+1}$ greater than $\alpha$.

Proceeding as we did for the largest root of $p_i$, we can show that
$p_{i+1}$ has a root smaller than the smallest root of $p_i$.

Let's $\alpha < \beta$ be two consecutive roots of $p_i$.  Since there
is one (and only one) root of $p_{i-1}$ between two consecutive roots
of $p_i$ by the hypothesis of induction, $p_{i-1}(\alpha)$ and
$p_{i-1}(\beta)$ must be of opposite sign.  However, since
\begin{align*}
  p_{i+1}(\alpha) &= (t_{i+1,i+1}-\alpha) p_i(\alpha)
  - t_{i+1,i}^2 p_{i-1}(\alpha) = - t_{i+1,i}^2 p_{i-1}(\alpha)
\intertext{and}
  p_{i+1}(\beta) &= (t_{i+1,i+1}-\beta) p_i(\beta)
  - t_{i+1,i}^2 p_{i-1}(\beta) = - t_{i+1,i}^2 p_{i-1}(\beta) \quad ,
\end{align*}
we have that $p_{i+1}(\alpha)$ and $p_{i+1}(\beta)$ must
also be of opposite sign.  So there is a root of $p_{i+1}$ between the
two consecutive roots, $\alpha$ and $\beta$, of $p_i$.

The distinct roots $\beta_j$ for $1\leq j \leq i$ of $p_i$ divide
the real line into $n+1$ subintervals $]-\infty,\beta_1[$,
$]\beta_j, \beta_{j+1}[$ for $1 \leq j < i$, and
$]\beta_i,\infty[$.  We have shown that there is a root of $p_{i+1}$
in each of these subintervals.   Since $p_{i+1}$ is of degree $i+1$,
those are all the roots of $p_{i+1}$ and they separate the roots of
$p_i$.  This complete the proof by induction.
\end{proof}

The number $N_i(\beta)$ of
{\bfseries sign agreements}\index{Sign Agreements} at
$\lambda = \beta \in \RR$ is the number of times that
$\sgn(p_j(\beta)) = \sgn(p_{j-1}(\beta))$
for $j=1$, $2$, \ldots, $i$.  By convention, we assume that there is a
sign agreement when $p_j(\beta)=0$.  For instance, there are three
sign agreements in the sequence $+,+,+,-,-$.  There are also three
sign agreements in the sequence $+,+,0,-,-$.

The following result follows from the previous theorem.

\begin{prop}
Consider a symmetric tridiagonal matrix $T$.  $N_i(\beta)$ is equal to the
number of roots of $p_i$ which are greater or equal to $\beta \in \RR$.
\end{prop}

\begin{proof}
As in the previous theorem, we will first assume that
$t_{i,i-1} \neq 0$ for $2\leq i \leq n$.  So, we have that the roots
$p_i$ are distinct and between any two consecutive roots of $p_i$
there is a root of $p_{i-1}$.

The proof is by induction on $i$, the degree of the polynomial $p_i$.

Since $p_0(\lambda) = 1$ and $p_1(\lambda) = t_{1,1}-\lambda$, we have
that
\[
N_1(\beta) = \begin{cases}
1 & \quad \text{if} \quad \beta \leq t_{1,1} \\
0 & \quad \text{if} \quad \beta > t_{1,1}    
\end{cases}
\]
\pdfbox{eigenvalues_A/sign1}
Effectively, $p_1$ has one root greater or equal to $\beta$ if
$\beta \leq t_{1,1}$ and no root greater or equal to $\beta$ if
$\beta > t_{1,1}$.  Thus, the induction hypothesis is true for $i=1$.

Let's assume that the result is true for $i$; namely, $p_i$ has
$N_i(\beta)$ roots greater or equal to $\beta$.

Let $\alpha_{i,1} < \alpha_{i,2} < \ldots < \alpha_{i,i}$ and
$\alpha_{i+1,1} < \alpha_{i+1,2} \ldots < \alpha_{i+1,i+1}$ be the
roots of $p_i$ and $p_{i+1}$ respectively.  From the previous theorem,
we know that
\[
  \alpha_{i+1,1} < \alpha_{i,1} < \alpha_{i+1,2} < \alpha_{i,2} < \ldots
  < \alpha_{i,i} < \alpha_{i+1,i+1}  \quad .
\]
Suppose that $N_i(\beta) = i-j$.  By induction, this means that
$\beta \leq \alpha_{i,1}$ if $j=0$,
$\alpha_{i,j} < \beta \leq \alpha_{i,j+1}$ if $0 < j < i$,
or $\beta > \alpha_{i,i}$ if $j=i$.

When $i$ is even, we have $p_i(\lambda) = \lambda^i + l.o.t.$ and
$p_{i+1}(\lambda) = -\lambda^{i+1} + l.o.t.$.  We have sketched
the case $i=2$ in the following figure.
\pdfbox{eigenvalues_A/sign2}
When $i$ is odd, we have $p_i(\lambda) = -\lambda^i + l.o.t.$ and
$p_{i+1}(\lambda) = \lambda^{i+1} + l.o.t.$.  We have sketched
the case $i=3$ in the following figure.
\pdfbox{eigenvalues_A/sign3}

We consider first the case for $0 < j < i$.  We have that
$\alpha_{i,j} < \alpha_{i+1,j+1} \leq \alpha_{i,j+1}$.
Either $\alpha_{i,j} < \beta \leq \alpha_{i+1,j+1}$
or $\alpha_{i+1,j+1} < \beta \leq \alpha_{i,j+1}$

\begin{enumerate}
\item When $\alpha_{i,j} < \beta \leq \alpha_{i+1,j+1}$, we have that
$\sgn(p_i(\beta))=\sgn(p_{i+1}(\beta))$ or $p_{i+1}(\beta) = 0$.
Thus, we have an additional sign agreement and
$N_{i+1}(\beta) = 1 + N_i(\beta) = 1 + (i-j)$.
We effectively have $i+1-j$ roots of $p_{i+1}$ greater or equal to
$\beta$; namely, $\alpha_{i+1,k}$ for $k>j$.
\item When $\alpha_{i+1,j+1} < \beta \leq \alpha_{i,j+1}$, we have that
$\sgn(p_i(\beta)) \neq \sgn(p_{i+1}(\beta))$.
Thus, we have no additional sign agreement and
$N_{i+1}(\beta) =  N_i(\beta) = i-j$.  We effectively have
$(i+1)-(j+1)$ roots of $p_{i+1}$ greater or equal to $\beta$; namely,
$\alpha_{i+1,k}$ for $k>j+1$.
\end{enumerate}

A similar argument can be used for $j=0$ and $j=i$ to prove that
$p_{i+1}$ has $N_{i+1}(\beta)$ roots greater or equal to $\beta$.

To show that the assumption that
$t_{i,i-1} \neq 0$ for $2\leq i \leq n$ is not needed, we note that
any symmetric tridiagonal matrix with some null elements on its
subdiagonal is the limit of symmetric  tridiagonal matrices with no
null element on its subdiagonal.  We leave the details to the reader.
\end{proof}

We can use this theorem and the bisection method to find all the roots of
$p_n$.

\begin{algo}{\alg}
Suppose that $[a,b]$ is an interval containing all the roots of
$p_n$.  Such an interval can be found with the help of Gerschgorin theorem.
We obviously have that $N(a)=n$ and $N(b) = 0$.
To find all the roots of $p_n$, one may proceed as follows for $i=1$, $2$,
\ldots, $n$.
\begin{enumerate}
\item Let $\alpha =a$ and $\beta = b$.
\item Let $m = (\alpha+\beta)/2$.  $m$ is the midpoint of the interval
$[\alpha,\beta]$. 
\item If $N_n(m) = i$, then a single root of $p_n$ exists in
$[m,\beta]$.  The bisection method may be used to approximate the
root of $p_n$ in the interval $[m,\beta]$.\\
If $N_n(m)>i$, set $\alpha = m$ and go back to (2).\\
If $N_n(m)<i$, set $\beta = m$ and go back to (2).
\item Let $\beta=m$ and go back to (1) until all $n$ roots of
$p_n$ have been found.
\end{enumerate}
\end{algo}

This method to find all the roots of $p_n$ may not work if the distance
between two roots of $p_n$ is smaller than the accuracy of the computer used.
If $t_{i,i-1} \neq 0$ for $2\leq i \leq n$ is not satisfied, there may be
roots of algebraic multiplicity greater than one.  The method will not
provide the algebraic multiplicity of the roots.  At the third step
of the algorithm above, if $N_n(m)>i$ for all computed midpoints.  One may
temporary say that $b$ is a root of algebraic multiplicity
$N_n(m) - i + 1$ and proceed to step (4) with $i= N_n(m)+1$.  The roots of
$p_n$ very closed to $b$ have to be determined using another approach.

\section{QR Algorithm}

The mean goal of this section is to present a method to compute the
eigenvalues of a symmetric tridiagonal matrix $T$.

Let $A$ be an \nn matrix.  Starting with $A_0 = A$, we produce
recursively a sequence of \nn matrices
$\displaystyle \{ A_i\}_{i=0}^\infty$ which are all conjugated to $A$
as follows.   Given the \nn matrix $A_i$, we write $A_i$ as
$A_i = Q_i R_i$, where $Q_i$ is an orthogonal matrix and $R_i$ is an
upper-triangular matrix.  The next matrix is defined by
$A_{i+1} = R_i Q_i$.

We have that
\[
A_{i+1} = R_i Q_i = Q_i^\top A_i Q_i \  .
\]
By induction,
\begin{equation} \label{C14L24}
A_{i+1} =  Q_i^\top Q_{i-1}^\top \ldots Q_0^\top A_0 Q_0 \ldots Q_{i-1} Q_i
= \left( Q_0 \ldots Q_{i-1} Q_i\right)^\top A_0 \left( Q_0 \ldots Q_{i-1} Q_i
\right) \  .
\end{equation}
Thus $A_i$ is orthogonally conjugate to $A$.  In particular, $A_i$ and $A$
have the same eigenvalues with the same algebraic multiplicity.

In Section~\ref{C14L31}, we explain how to express a \nn matrix $A$
as the product $A = QR$, where $Q$ is an orthogonal matrix and $R$ is
an upper-triangular matrix.  The tool that we will use to do this
is the Gram-Schmidt orthogonalization process that we cover in the next
section.

\subsection{Gram-Schmidt Orthogonalization Process} \label{C14L25}

\begin{defn}[Gram-Schmidt Orthogonalization Process]
Let $\ps{\cdot}{\cdot}$ be a scalar product on $\RR^n$ and let
$\{\VEC{u}_1, \VEC{u}_2, \ldots , \VEC{u}_k\}$ be a subset of $\RR^n$.
We define the set $\{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ as
follows:
\begin{enumerate}
\item $\VEC{v}_1 = \VEC{u}_1$
\item For $2 \leq i \leq k$,
$\displaystyle \VEC{v}_i = \VEC{u}_i - \sum_{j=1}^{i-1}r_{j,i}\VEC{v}_j$,
where
\[
r_{j,i} = \begin{cases}
\displaystyle \frac{\ps{\VEC{v}_j}{\VEC{u}_i}}{\ps{\VEC{v}_j}{\VEC{v}_j}}
& \quad \text{if} \quad \VEC{v}_j \neq \VEC{0} \\
0 & \quad \text{if} \quad \VEC{v}_j = \VEC{0}
\end{cases}
\]
for $1 \leq j < i$.
\end{enumerate}
\label{C14L26}
\end{defn}

The set $\{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ given by the
Gram-Schmidt Orthogonalization Process has the following properties.

\begin{prop}
Let $\VEC{u}_j$ and $\VEC{v}_j$ for $1\leq j\leq k$ be the vectors
defined in Definition~\ref{C14L26}.  Let 
$S_i = \{\VEC{u}_1, \VEC{u}_2, \ldots , \VEC{u}_i\}$
and $V_i = \Span(S_i)$ for $1\leq i \leq k$.
\begin{enumerate}
\item $V_i = \Span(T_i)$ where
$T_i = \{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_i\}$.
\item $\ps{\VEC{v}_p}{\VEC{v}_q} = 0$ for $1 \leq p < q \leq i$.
\label{C14L27}
\item $\VEC{v}_i = 0$ if and only if $\VEC{u}_i \in V_{i-1}$.
\item If $S_k$ is a basis of $V_k$, then $T_k$ is an orthogonal basis of
$V$. \label{C14L28}
\end{enumerate}
\end{prop}

\begin{rmk}
If $P_j$ is the orthogonal projection of $\RR^n$ onto $V_j$ for
$1 \leq j \leq k$, we have that
$\VEC{v}_i = \VEC{u}_i - P_{i-1}(\VEC{u}_i)$.
Items~\ref{C14L27} and \ref{C14L28} of the previous proposition
imply that all finite dimensional vector spaces have an orthogonal
basis.
\end{rmk}

\begin{proof}
\stage{1} The proof is by induction on $i$.

Since $\VEC{v}_1 = \VEC{u}_1$, we have that $V_1 = \Span(T_1)$.  So,
the result is true for $i=1$.

Suppose that the result is true for $i$; namely, $V_i = \Span(T_i)$.
To show that $V_{i+1} = \Span(T_{i+1})$, it suffices to show that
$\VEC{u}_{i+1} \in \Span(T_{i+1})$ because
$\VEC{u}_j \in \Span(T_i) \subset \Span(T_{i+1})$ for $1 \leq j \leq i$
by the hypothesis of induction.  From
\[
\VEC{v}_{i+1} = \VEC{u}_{i+1} - \sum_{j=1}^i r_{j,i+1} \VEC{v}_j \ ,
\]
we get
\[
\VEC{u}_{i+1} = \VEC{v}_{i+1} + \sum_{j=1}^i r_{j,i+1} \VEC{v}_j \ .
\]
Thus $\VEC{u}_{i+1} \in \Span(T_{i+1})$.

\stage{2} The proof is again by induction on $i$.

Since $\VEC{v}_2 = \VEC{u}_2 - r_{1,2} \VEC{v}_1$ with
\[
r_{1,2} = \begin{cases}
\displaystyle \frac{\ps{\VEC{v}_1}{\VEC{u}_2}}{\ps{\VEC{v}_1}{\VEC{v}_1}}
& \quad \text{if} \quad \VEC{v}_1 \neq \VEC{0} \\
0 & \quad \text{if} \quad \VEC{v}_1 = \VEC{0}
\end{cases}
\]
we get
\[
\ps{\VEC{v}_1}{\VEC{v}_2}
= \ps{\VEC{v}_1}{\VEC{u}_2 - r_{1,2} \VEC{v}_1}
= \ps{\VEC{v}_1}{\VEC{u}_2} - r_{1,2} \ps{\VEC{v}_1}{\VEC{v}_1}
= 0 \ .
\]
So the result is true for $i=2$.

Suppose that the result is true for $i$; namely,
$\ps{\VEC{v}_p}{\VEC{v}_q} = 0$ for $1 \leq p < q \leq i$.  To prove
that $\ps{\VEC{v}_p}{\VEC{v}_q} = 0$ for $1 \leq p < q \leq i+1$, it
suffices to prove that
$\ps{\VEC{v}_m}{\VEC{v}_{i+1}} = 0$ for $1 \leq m \leq i$.
Since
$\displaystyle \VEC{v}_{i+1} = \VEC{u}_{i+1} - \sum_{j=1}^ir_{j,i+1}\VEC{v}_j$,
where
\[
r_{j,i+1} = \begin{cases}
\displaystyle \frac{\ps{\VEC{v}_j}{\VEC{u}_{i+1}}}{\ps{\VEC{v}_j}{\VEC{v}_j}}
& \quad \text{if} \quad \VEC{v}_j \neq \VEC{0} \\
0 & \quad \text{if} \quad \VEC{v}_j = \VEC{0}
\end{cases}
\]
we get
\begin{align*}
\ps{\VEC{v}_m}{\VEC{v}_{i+1}}
&= \ps{\VEC{v}_m}{\VEC{u}_{i+1} - \sum_{j=1}^ir_{j,i+1}\VEC{v}_j}
= \ps{\VEC{v}_m}{\VEC{u}_{i+1}} -
\sum_{j=1}^ir_{j,i+1}\ps{\VEC{v}_m}{\VEC{v}_j} \\
&= \ps{\VEC{v}_m}{\VEC{u}_{i+1}} - r_{m,i+1}\ps{\VEC{v}_m}{\VEC{v}_m}
= 0
\end{align*}
for $1 \leq m \leq i$.   So the result is true for $i+1$.

\stage{3}  If $\VEC{v}_i = \VEC{0}$, then
$\displaystyle \VEC{u}_i - \sum_{j=1}^{i-1}r_{j,i}\VEC{v}_j = \VEC{0}$.
This gives
$\displaystyle \VEC{u}_i = \sum_{j=1}^{i-1}r_{j,i}\VEC{v}_j$.  Thus
$\VEC{u}_i \in V_{i-1}$ because $V_{i-1} = \Span(T_{i-1})$ by (1).
Conversely, if $\VEC{u}_i \in V_{i-1} = \Span(T_{i-1})$, we get from
Proposition~\ref{C14L2} that
$\displaystyle \VEC{u}_i = \sum_{j=1}^i a_j \VEC{v}_j$ with
$a_j = r_{j,i}$.  Thus,
$\displaystyle \VEC{v}_{i+1} = \VEC{u}_i - \sum_{j=1}^{i-1}r_{j,i}\VEC{v}_j
= \VEC{0}$.

\stage{4} If we use (2), this follows from (1) with $i=k$.
\end{proof}

\begin{prop}
Let $\ps{\cdot}{\cdot}$ be the standard scalar product on $\RR^n$ and
let $\{\VEC{v}_1, \VEC{v}_2, \ldots , \VEC{v}_k\}$ be an orthonormal
basis of a subspace $V$ of $\RR^n$.  Let
$Q = \begin{pmatrix} \VEC{v}_1 & \VEC{v}_2 & \ldots & \VEC{v}_k 
\end{pmatrix}$.  Then

\begin{enumerate}
\item $Q^\top Q = \Id_k$.  \label{C14L29}
\item The orthogonal projection $P$ on $V$ is given by $P = QQ^\top$.
\item $P$ is symmetric (i.e.\ $P^\top = P$).
\item $P^2 = P$.
\item $P(\Id_n - P) = (\Id_n - P) P = 0$.
\item $(\Id_n - P )Q = 0$.
\end{enumerate}
\label{C14L30}
\end{prop}

\begin{proof}
\stage{1}  The component on the $i^{th}$ row and $j^{th}$ column of
$Q^\top Q$ is
\[
\ps{\VEC{v}_i}{\VEC{v}_j} = \delta_{i,j}
= \begin{cases}
1 & \quad \text{if} \quad i = j \\
0 & \quad \text{if} \quad i \neq j
\end{cases}
\]

\stage{2} Given $\VEC{v} \in \RR^n$,
\[
Q Q^\top \VEC{v} = Q
\begin{pmatrix}
\VEC{v}_1^\top \VEC{x} \\
\VEC{v}_2^\top \VEC{x} \\
\vdots \\
\VEC{v}_k^\top \VEC{x}
\end{pmatrix}
= \sum_{j=1}^k \ps{\VEC{v}_j}{\VEC{x}} \VEC{v}_j
= \sum_{j=1}^k \frac{\ps{\VEC{v}_j}{\VEC{x}}}{\ps{\VEC{v}_j}{\VEC{v}_j}}
\VEC{v}_j = P\VEC{x}
\]
because $\ps{\VEC{v}_j}{\VEC{x}}= \VEC{v}_1^\top \VEC{x}$
and $\ps{\VEC{v}_j}{\VEC{v}_j}=1$ for $1\leq j \leq k$.

\stage{3} We have
\[
  P^\top = (Q Q^\top)^\top = (Q^\top)^\top Q^\top = Q Q^\top = P \quad .
\]

\stage{4} We have
\[
  P^2 = (Q Q^\top)(Q Q^\top) = Q (Q^\top Q) Q^\top
  = Q \Id_n Q^\top = Q Q^\top = P \quad .
\]

\stage{5} We have
\[
 P(\Id_n-P) = P - P^2 = P - P = 0 \qquad \text{and} \qquad
 (\Id_n - P)P = P - P^2 = P - P = 0 \quad .
\]

\stage{6} We have
\[
  (\Id_n - P)Q = Q - (Q Q^\top) Q = Q - Q ( Q^\top Q)
  = Q - Q \Id_k = Q - Q = 0 \quad .  \qedhere
\]
\end{proof}

\subsection{Normalized QR Decomposition}\label{C14L31}

The Gram-Schmidt orthogonalization process given in
Definition~\ref{C14L26} can be summarize as follows.
Consider the \nm{n}{k} matrices
$A = \begin{pmatrix} \VEC{u}_1 & \VEC{u}_2 & \ldots & \VEC{u}_k\end{pmatrix}$
and
$Q_0 = \begin{pmatrix} \VEC{v}_1 & \VEC{v}_2 & \ldots & \VEC{v}_k\end{pmatrix}$.
Let $R_0$ be the \nm{k}{k} upper-triangular matrix
\[
\begin{pmatrix}
r_{1,1} & r_{1,2} & r_{1,3} & \ldots & r_{1,k} \\
r_{2,1} & r_{2,2} & r_{2,3} & \ldots & r_{2,k} \\
r_{3,1} & r_{3,2} & r_{3,3} & \ldots & r_{3,k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
r_{k,1} & r_{k,2} & r_{k,3} & \ldots & r_{k,k}
\end{pmatrix} \quad ,
\]
where
\[
r_{j,i} = \begin{cases}
0 & \quad \text{if} \quad j > i \\    
1 & \quad \text{if} \quad j = i \\    
\displaystyle \frac{\ps{\VEC{v}_j}{\VEC{u}_i}}{\ps{\VEC{v}_j}{\VEC{v}_j}}
& \quad \text{if} \quad j < i \quad \text{and} \quad
\VEC{v}_j \neq \VEC{0} \\
0 & \quad \text{if} \quad j < i \quad \text{and} \quad
\VEC{v}_j = \VEC{0}
\end{cases} \qquad .
\]  
Then $A = Q_0 R_0$.  This is called the
{\bfseries unnormalized QR decomposition}\index{Unnormalized QR
Decomposition} of the matrix $A$. 

If we eliminate the null columns of $Q_0$, we get a \nm{n}{p} matrix
$Q_1 = \begin{pmatrix} \VEC{v}_{j_1} & \VEC{v}_{j_2} & \ldots &
  \VEC{v}_{j_p} \end{pmatrix}$ for some $j_1 < j_2, < \ldots < j_p$ in
$\{1,2,\ldots, k\}$ with $p \leq k$.   If we eliminate the rows other
than the rows $j_1$, $j_2$, \ldots, $j_p$ from $R_0$, we get the
\nm{p}{k} upper-triangular matrix
\[
R_1 =
\begin{pmatrix}
r_{j_1,1} & r_{j_1,2} & r_{j_1,3} & \ldots & r_{j_1,k} \\
r_{j_2,1} & r_{j_2,2} & r_{j_2,3} & \ldots & r_{j_2,k} \\
r_{j_3,1} & r_{j_3,2} & r_{j_3,3} & \ldots & r_{j_3,k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
r_{j_p,1} & r_{j_p,2} & r_{j_p,3} & \ldots & r_{j_p,k}
\end{pmatrix} \quad ,
\]
We have $A = Q_1 R_1$.  Finally, if we define the \nm{n}{p} matrix
\[
Q = Q_1 \begin{pmatrix}
\displaystyle \frac{1}{\|\VEC{v}_{j_1}\|} & 0 & \ldots & 0 \\
0 & \displaystyle \frac{1}{\|\VEC{v}_{j_2}\|} &\ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \displaystyle \frac{1}{\|\VEC{v}_{j_p}\|}
\end{pmatrix}
\]
and the \nm{p}{k} matrix
\[
R = \begin{pmatrix}
\|\VEC{v}_{j_1}\| & 0 & \ldots & 0 \\
0 & \|\VEC{v}_{j_2}\| &\ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \|\VEC{v}_{j_p}\|
\end{pmatrix} R_1 \quad ,
\]
then $A = QR$.  This is the
{\bfseries (normalized) QR decomposition}\index{Normalized QR Decomposition}
of $A$.  We have that $Q^\top Q = \Id_k$ and $R$ is upper-triangular.
The columns of $Q$ are the normalized columns of $Q_1$.

The following result is an interesting consequence of the QR
decomposition of a matrix.

\begin{prop}
Let $S = \{\VEC{u}_1, \VEC{u}_2, \ldots, \VEC{u}_k\}$ be a subset of
$\RR^n$ and $V = \Span(S)$.  If
$A = \begin{pmatrix} \VEC{u}_1 & \VEC{u}_2 & \ldots & \VEC{u}_k\end{pmatrix}$
and $A = QR$ is the normalized QR decomposition of $A$, then
$P = Q Q^\top$ is the projection on $V$.
\end{prop}

\begin{proof}
The set $\displaystyle \left\{ \frac{1}{\|\VEC{v}_{j_1}\|}\VEC{v}_{j_1} ,
\frac{1}{\|\VEC{v}_{j_2}\|}\VEC{v}_{j_2} , \ldots ,
\frac{1}{\|\VEC{v}_{j_p}\|}\VEC{v}_{j_p} ,\right\}$ formed of the columns
of $Q$ is an orthonormal basis of $V$.
It follows from Proposition~\ref{C14L30} that $P = QQ^\top$ is
the projection on $V$.
\end{proof}

Since our goal is to find the eigenvalues of a matrix, the interesting
case of QR decomposition is when $A$ is a \nn matrix.  Thus $k=n$ in
the previous presentation of the QR decomposition.  Moreover, we
will assume that $A$ is invertible.  Thus, the set formed of the $n$
columns of $A$ is linearly independent.   This implies that
$Q_0 = Q_1$ and $R_0 = R_1$ in the previous discussion of the QR
decomposition.  Moreover, item~\ref{C14L29} of
Proposition~\ref{C14L30} implies that $Q$ is an orthogonal
matrix.

We now summarize the algorithm to compute the QR decomposition of an
\nn invertible matrix.

\begin{algo}[Normalized QR decomposition]
Let
$A = \begin{pmatrix} \VEC{u}_1 & \VEC{u}_2 & \ldots & \VEC{u}_n\end{pmatrix}$
and $\VEC{q}_i$ be the $i^{th}$ column of the matrix $Q$ in the QR
decomposition $A = QR$.
\begin{align*}
r_{1,1} &= \|\VEC{u}_1\| \\
\VEC{q}_1 &= \frac{1}{r_{1,1}}\, \VEC{u}_1
\end{align*}
For $i=1$, $2$, \ldots $n-1$
\begin{align*}
r_{j,i+1} &= \VEC{u}_{i+1}^\top \VEC{q}_j \quad \text{for} \quad
j=1,2,\ldots,i \\
r_{i+1,i+1} &= \left\| \VEC{u}_{i+1} - \sum_{j=1}^i r_{j,i+1}\VEC{q}_j
\right\| \\
\VEC{q}_{i+1} &= \frac{1}{r_{i+1.i+1}} \left(
\VEC{u}_{i+1} - \sum_{j=1}^i r_{j,i+1}\VEC{q}_j \right)
\end{align*}
\end{algo}

Since $r_{i,i}>0$ for all $i$ (i.e.\ the elements on the diagonal of
$R$ are all positive), $Q$ is uniquely determined.

\begin{rmk}
For full non-singuliar matrix $A$, the algorithm above will take
$O(n^3)$ multiplications to produce the QR decomposition of $A$.
However, if $A$ in the Hessemberg form, the algorithm will take only
$O(n^2)$ multiplications to produce the QR decomposition of $A$.  Even
better, if $A$ is a symmetric tridiagonal matrix, the algorithm will
take only $O(n)$ multiplications to produce the QR factorization of
$A$.
\label{C14L32}
\end{rmk}

\subsection{General QR Algorithm}

We have presented all the techniques needed to execute the following
algorithm.

\begin{algo}[QR Algorithm]
Let $A$ be an \nn matrix.
\begin{enumerate}
\item Let $A_0 = A$.
\item Given the \nn matrix $A_i$, find a QR decomposition $A_i = Q_i R_i$,
\item Let $A_{i+1} = R_i Q_i$.  Then $A_{i+1} = Q^\top A_i Q$ and
$A_{i+1}$ is orthogonally similar to $A_i$.
\item Repeat (2) and (3) with $i$ replace by $i+1$.
\end{enumerate}
The matrices $A_i$ are orthogonally similar to $A$.
\end{algo}

We shall now justify why this algorithm is useful to find the
eigenvalues of a matrix.

\begin{theorem}[Francis]
If $A$ is a \nn matrix with $n$ eigenvalues $\lambda_1$, $\lambda_2$,
\ldots, $\lambda_n$ such that $|\lambda_1|<|\lambda_2|<\ldots <|\lambda_n|$,
then the sequence $\displaystyle \{ A_i\}_{i=0}^\infty$ converges
toward an upper-triangular matrix $B$.
\end{theorem}

A proof of this result can be found in the article
{\it The QR Transformation: A Unitary Analogue to the LR
  Transformation, Part 1}, J. G. F. Francis, The Computer Journal,
Vol.\ {\bfseries 4}, Issue 3, 1961, pp.\ 265–271.

It follows from the previous theorem that the diagonal elements of
$A_i$ converge toward the eigenvalues of $A$ since the $A_i$'s are
conjuagte to $A$.

If some of the eigenvalues of $A$ have equal magnitude in absolute value,
then the diagonal of $B$ may contain subblocks whose eigenvalues are
the eigenvalues of equal magnitude.   If the subblocks are large
(i.e.\ larger than \nm{2}{2} matrix), it may be difficult to compute
these eigenvalues.

Since $|\lambda_1|<|\lambda_2|<\ldots <|\lambda_n|$ is a very
restrictive condition for the eigenvalues of $A$, we need a less
restrictive condition on the eigenvalues of $A$.  To do that, we first
consider the convergence of the sequence
$\displaystyle \{ A_i\}_{i=0}^\infty$.

\begin{rmk}
If $A$ is an Hessemberg form, then the matrices $A_i$ produced by the QR
algorithm are also in Hessemberg form.  We present a ``graphical''
proof of this claim.  It is as good as an algebraic proof without the
mess of the indices.  Suppose that $A_i$ is in Hessemberg
form and $A_i = Q_iR_i$, where $Q_i$ is orthogonal and $R_i$ is
upper-triangular.  Since $R_i^{-1}$ is also upper-triangular, we have that
\begin{align*}
Q_i &= A_i R_i^{-1} \\
&= \begin{pmatrix}
* & * & * & \ldots & * & * & * \\
* & * & * & \ldots & * & * & * \\
0 & * & * & \ldots & * & * & * \\
0 & 0 & * & \ldots & * & * & * \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 0 & * & *
\end{pmatrix}
\begin{pmatrix}
* & * & * & \ldots & * & * & * \\
0 & * & * & \ldots & * & * & * \\
0 & 0 & * & \ldots & * & * & * \\
0 & 0 & 0 & \ldots & * & * & * \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 0 & 0 & *
\end{pmatrix}
=
\begin{pmatrix}
* & * & * & \ldots & * & * & * \\
* & * & * & \ldots & * & * & * \\
0 & * & * & \ldots & * & * & * \\
0 & 0 & * & \ldots & * & * & * \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 0 & * & *
\end{pmatrix}
\end{align*}
is in Hessemberg form.  Hence,
\begin{align*}
A_{i+1} &= R_i Q_i \\
&= \begin{pmatrix}
* & * & * & \ldots & * & * & * \\
0 & * & * & \ldots & * & * & * \\
0 & 0 & * & \ldots & * & * & * \\
0 & 0 & 0 & \ldots & * & * & * \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 0 & 0 & *
\end{pmatrix}
\begin{pmatrix}
* & * & * & \ldots & * & * & * \\
* & * & * & \ldots & * & * & * \\
0 & * & * & \ldots & * & * & * \\
0 & 0 & * & \ldots & * & * & * \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 0 & * & *
\end{pmatrix}
=
\begin{pmatrix}
* & * & * & \ldots & * & * & * \\
* & * & * & \ldots & * & * & * \\
0 & * & * & \ldots & * & * & * \\
0 & 0 & * & \ldots & * & * & * \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 0 & * & *
\end{pmatrix}
\end{align*}
is in Hessember form.

Hence, if $A$ is in Hessemberg form, the matrices $A_i$ provided by the QR
algorithm are also in Hessemberg form.
\label{C14L33}
\end{rmk}

For the sake of determining the eigenvalues of a matrix $A$ in
Hessemberg form, the convergence of the elements on the subdiagonal
plays a fundamental role.  If they converge rapidly toward zero, then
we will rapidly get good approximations for the eigenvalues of $A$
even if the components above the diagonal have not reached their limit
yet.  Suppose that all the elements on the subdiagonal of $A_i$ are
null (the situation is rarely that simple), then the diagonal of $A_i$
has the eigenvalues of $A$ because $A_i$ is conjugate to $A$.  This is
true even if all the other components of $A_i$ have not reached their
limit yet.  This justify the following definition.

\begin{defn}
Let $A$ be a matrix in Hessemberg form.  We say that the sequence
$\displaystyle \{ A_i\}_{i=0}^\infty$ produce by the QR algorithm
{\bfseries converges}\index{Matrices!Converges} if
\[
  \max_{\substack{2\leq j \leq n-1\\ M = A_i}} |m_{j+1,j}\,m_{j,j-1}| \rightarrow 0
\quad \text{as} \quad  i\rightarrow \infty \ .
\]
\end{defn}

The following theorem demonstrates the importance of the elements on the
subdiagonal of the matrices $A_i$.

\begin{theorem}[Parlett]
Let $A$ be a \nn matrix in Hessemberg form.  The sequence of matrices
$\displaystyle \{ A_i\}_{i=0}^\infty$ produced with the QR algorithm
converges as defined in the previous definition if and only if each
set of eigenvalues of $A$ of the same magnitude in absolute value
contains at most two eigenvalues of even algebraic multiplicity or two
eigenvalues of odd algebraic multiplicity.
\end{theorem}

A proof of this theorem is given in the article {\it Global Convergence of
the Basic QR Algorithm On Hessemberg Matrices}, B. Parlett, Mathematics of
Computation, Vol.\ {\bfseries 22}, No.\ 104 (Oct. 1968), pp.\ 803-817.

The limit of the sequence $\displaystyle \{ A_i\}_{i=0}^\infty$ predicted by
the previous theorem will be a matrix having sub-blocks of dimension at most
\nm{2}{2} on the diagonal.  The eigenvalues of these sub-blocks are the
eigenvalues of $A$.

The convergence of the sequence $\displaystyle \{ A_i\}_{i=0}^\infty$ if $A$
is in Hessemberg matrices $A$ is not fast.  Moreover, the convergence of the
sequence $\displaystyle \{ A_i\}_{i=0}^\infty$ is not faster for a symmetric
tridiagonal matrices $A$ but the QR factorization of the $A_i$'s is fast (of
the order of $O(n)$ multiplications as we have seen in
Remark~\ref{C14L32}).  In the next section, we present an efficient
algorithm to find an orthogonal matrix $Q_i$ and an upper-triangular matrix
$R_i$ for the factorization $A_i = Q_i R_i$ in the case where $A_i$ is a
symmetric tridiagonal matrix.

Obviously, not all matrices are in Hessemberg form.  However, we have
seen that for any given matrix, we can use Householder matrices to
find a Hessemberg matrix conjugate to it.  Then the QR algorithm can
be applied to this Hessemberg matrix.  We summarize this algorithm in
the next theorem.  In this statement, we also use Householder matrices
to find the QR decomposition instead of Gram-Schmidt as we have done
before.  We will not elaborate on this approach in these notes.  It is
an interesting theoretical approach but not computationally efficient.

\begin{algo}[The QR Algorithm]
Let $A$ be a \nn matrix with entries in $\RR$.
\begin{enumerate}
\item Let $G_1$, $G_2$, \ldots, $G_{n-2}$ be $n-2$ Householder
matrices (given by Algorithm~\ref{C14L10}) such that
$A_1 = G^\top A G$ is a matrix in Hessemberg form for
$G=G_1 G_2 \cdots G_{n-2}$.
\item Given the \nn matrix $A_i$ in Hessemberg form, let $Q_1$, $Q_2$,
\ldots, $Q_n$ be $n$ Householder matrices (given by
Algorithm~\ref{C14L9}) such that $A_n = Q R$ for
$Q= Q_1 Q_2 \cdots Q_n$ and $R$ an upper-triangular \nn matrix.
\item Let $A_{i+1} = RQ$.  Then $A_{i+1} = Q^\top A_i Q$ and
$A_{i+1}$ is orthogonally similar to $A_i$.
\item Repeat (2) and (3) with $i$ replace by $i+1$.
\end{enumerate}
The matrices $A_i$ are orthogonally similar to $A$.
\end{algo}

\subsection{QR Factorization for Symmetric Tridiagonal Matrices}

Let
\[
P_1(\theta) =
\begin{pmatrix}
\cos(\theta) & \sin(\theta) & 0 \\
-\sin(\theta) & \cos(\theta) & 0 \\
0 & 0 & \Id_{n-2}
\end{pmatrix}
\ , \quad
P_{n-1}(\theta) =
\begin{pmatrix}
\Id_{n-2} & 0 & 0 \\
0 & \cos(\theta) & \sin(\theta) \\
0 & -\sin(\theta) & \cos(\theta) \\
\end{pmatrix} \ ,
\]
and
\[
P_j(\theta) =
\begin{pmatrix}
\Id_{j-1} & 0 & 0 & 0 \\
0 & \cos(\theta) & \sin(\theta) & 0 \\
0 & -\sin(\theta) & \cos(\theta) & 0 \\
0 & 0 & 0 & \Id_{n-j-1}
\end{pmatrix}
\]
for $j=2$, $3$ ,\ldots, $n-2$.  Suppose that $A$ is a symmetric tridiagonal
matrix.  Let $A_0 = A$.  We explain how to use the matrices $P_j(\theta)$ to
find a QR decomposition $A_i = Q_i R_i$ for $i \geq 0$.  Recall that
$A_{i+1} = Q_i R_i$.

Suppose that
\[
A_i =
\begin{pmatrix}
a_1 & b_1 & 0 & \ldots & 0 & 0 & 0 \\
b_1 & a_2 & b_2 & \ldots & 0 & 0 & 0 \\
0 & b_2 & a_3 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & b_{n-2} & a_{n-1} & b_{n-1} \\
0 & 0 & 0 & \ldots & 0 & b_{n-1} & a_n
\end{pmatrix}
\ .
\]
To compute $Q_i$ and $R_i$ in $A_i = Q_iR_i$, choose $\theta_1$ such
that
\[
B_1 = P_1(\theta_1) A_i =
\begin{pmatrix}
\alpha_1 & \beta_1 & \gamma_1 & 0 & \ldots & 0 & 0 \\
0 & x_2 & y_2 & 0 & \ldots & 0 & 0 \\
0 & b_2 & a_3 & b_3 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & a_{n-1} & b_{n-1} \\
0 & 0 & 0 & 0 & \ldots & b_{n-1} & a_n
\end{pmatrix} \ ;
\]
namely,
\begin{equation}\label{C14L34}
\begin{split}
\alpha_1 &= a_1 \cos(\theta_1) + b_1 \sin(\theta_1) \ , \\
\beta_1 &= b_1 \cos(\theta_1) + a_2 \sin(\theta_1) \ , \\
\gamma_1 &= b_2 \sin(\theta_1) \ , \\
x_2 &= - b_1 \sin(\theta_1) + a_2 \cos(\theta_1) \ , \\
y_2 & = b_2 \cos(\theta_1)
\end{split}
\end{equation}
and
\[
0 = -a_1 \sin(\theta_1) + b_1 \cos(\theta_1) \ .
\]
We have that $\cos^2(\theta_1)+\sin^2(\theta_1) = 1$ and
$0 = -a_1 \sin(\theta_1) + b_1 \cos(\theta_1)$ are satisfied by
\begin{equation} \label{C14L35}
\cos(\theta_1) = \frac{a_1}{\sqrt{a_1^2 + b_1^2}} \quad \text{and} \quad
\sin(\theta_1) = \frac{b_1}{\sqrt{a_1^2 + b_1^2}} \ .
\end{equation}
This is a possible choice.

Suppose that we have found $\theta_j$ and $B_j$ for $j=1$, $2$,
\ldots, $k$ with $k<n-2$ such that
\begin{align*}
B_k &= P_k(\theta_k) P_{k-1}(\theta_{k-1}) \ldots P_1(\theta_1) A_i \\
&= \left(
\begin{array}{cccccccccccc}
\alpha_1 & \beta_1 & \gamma_1 & 0 & \ldots & 0 & 0 & 0 & 0 & \ldots & 0 & 0 \\
0 & \alpha_2 & \beta_2 & \gamma_2 & \ldots & 0 & 0 & 0 & 0 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots &
\vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & \alpha_k & \beta_k & \gamma_k & 0 & \ldots & 0 & 0\\ 
0 & 0 & 0 & 0 & \ldots & 0 & x_{k+1} & y_{k+1} & 0 & \ldots & 0 & 0 \\
0 & 0 & 0 & 0 & \ldots & 0 & b_{k+1} & a_{k+2} & b_{k+2} & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots &
\vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 & 0 & \ldots & a_{n-1} & b_{n-1} \\
0 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 & 0 & \ldots & b_{n-1} & a_n
\end{array}\right) \ .
\end{align*}
Choose $\theta_{k+1}$ such that
\begin{align*}
B_{k+1} &= P_{k+1}(\theta_{k+1}) B_k \\
&= \left(
\begin{array}{cccccccccccc}
\alpha_1 & \beta_1 & \gamma_1 & 0 & \ldots & 0 & 0 & 0 & 0 & \ldots & 0 & 0 \\
0 & \alpha_2 & \beta_2 & \gamma_2 & \ldots & 0 & 0 & 0 & 0 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots &
\vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & \alpha_{k+1} & \beta_{k+1} & \gamma_{k+1} & 0 &
\ldots & 0 & 0 \\
0 & 0 & 0 & 0 & \ldots & 0 & x_{k+2} & y_{k+2} & 0 & \ldots & 0 & 0 \\
0 & 0 & 0 & 0 & \ldots & 0 & b_{k+2} & a_{k+3} & b_{k+3} & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots &
\vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 & 0 & \ldots & a_{n-1} & b_{n-1} \\
0 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 & 0 & \ldots & b_{n-1} & a_n
\end{array}\right) \ ;
\end{align*}
namely,
\begin{equation}\label{C14L36}
\begin{split}
\alpha_{k+1} &= x_{k+1} \cos(\theta_{k+1}) + b_{k+1} \sin(\theta_{k+1}) \ , \\
\beta_{k+1} &= y_{k+1} \cos(\theta_{k+1}) + a_{k+2} \sin(\theta_{k+1}) \ , \\
\gamma_{k+1} &= b_{k+2} \sin(\theta_{k+1}) \ , \\
x_{k+2} &= - y_{k+1} \sin(\theta_{k+1}) + a_{k+2} \cos(\theta_{k+1}) \ , \\
y_{k+2} & = b_{k+2} \cos(\theta_{k+1})
\end{split}
\end{equation}
and
\[
0 = -x_{k+1} \sin(\theta_{k+1}) + b_{k+1} \cos(\theta_{k+1}) \ .
\]
We have that $\cos^2(\theta_{k+1})+\sin^2(\theta_{k+1}) = 1$ and
$0 = -x_{k+1} \sin(\theta_{k+1}) + b_{k+1} \cos(\theta_{k+1})$ are
satisfied by
\begin{equation} \label{C14L37}
\cos(\theta_{k+1}) = \frac{x_{k+1}}{\sqrt{x_{k+1}^2 + b_{k+1}^2}} \quad
\text{and} \quad
\sin(\theta_{k+1}) = \frac{b_{k+1}}{\sqrt{x_{k+1}^2 + b_{k+1}^2}} \ .
\end{equation}

Proceeding inductively, we can find $\theta_j$ and
$B_j$ for $j=1$, $2$, \ldots, $n-2$ such that
\begin{align*}
B_{n-2} &= P_{n-2}(\theta_{n-2}) P_{n-1}(\theta_{n-1}) \ldots P_1(\theta_1) A_i \\
&=
\begin{pmatrix}
\alpha_1 & \beta_1 & \gamma_1 & 0 & \ldots & 0 & 0 & 0 \\
0 & \alpha_2 & \beta_2 & \gamma_2 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & \alpha_{n-2} & \beta_{n-2} & \gamma_{n-2} \\ 
0 & 0 & 0 & 0 & \ldots & 0 & x_{n-1} & y_{n-1} \\
0 & 0 & 0 & 0 & \ldots & 0 & b_{n-1} & a_{n} 
\end{pmatrix} \ .
\end{align*}
Choose $\theta_{n-1}$ such that
\begin{align*}
B_{n-1} &= P_{n-1}(\theta_{n-1}) B_{n-2} \\
&=
\begin{pmatrix}
\alpha_1 & \beta_1 & \gamma_1 & 0 & \ldots & 0 & 0 & 0 \\
0 & \alpha_2 & \beta_2 & \gamma_2 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & \alpha_{n-2} & \beta_{n-2} & \gamma_{n-2} \\ 
0 & 0 & 0 & 0 & \ldots & 0 & \alpha_{n-1} & \beta_{n-1} \\
0 & 0 & 0 & 0 & \ldots & 0 & 0 & \alpha_n 
\end{pmatrix} \ ;
\end{align*}
namely,
\begin{equation}\label{C14L38}
\begin{split}
\alpha_{n-1} &= x_{n-1} \cos(\theta_{n-1}) + b_{n-1} \sin(\theta_{n-1}) \ , \\
\beta_{n-1} &= y_{n-1} \cos(\theta_{n-1}) + a_n \sin(\theta_{n-1}) \ , \\
\alpha_n &= - y_{n-1} \sin(\theta_{n-1}) + a_n \cos(\theta_{n-1})
\end{split}
\end{equation}
and
\[
0 = -x_{n-1} \sin(\theta_{n-1}) + b_{n-1} \cos(\theta_{n-1}) \ .
\]
We have that $\cos^2(\theta_{n-1})+\sin^2(\theta_{n-1}) = 1$ and
$0 = -x_{n-1} \sin(\theta_{n-1}) + b_{n-1} \cos(\theta_{n-1})$ are
satisfied by
\begin{equation} \label{C14L39}
\cos(\theta_{n-1}) = \frac{x_{n-1}}{\sqrt{x_{n-1}^2 + b_{n-1}^2}} \quad
\text{and} \quad
\sin(\theta_{n-1}) = \frac{b_{n-1}}{\sqrt{x_{n-1}^2 + b_{n-1}^2}} \ .
\end{equation}

We end up with the upper-triangular matrix
\[
B_{n-1} = P_{n-1}(\theta_{n-1}) P_{n-2}(\theta_{n-2})
\ldots P_1(\theta_1) A_i \ .
\]
Let
\begin{align*}
R_i &= B_{n-1}
\intertext{and}
Q_i &= \left( P_{n-1}(\theta_{n-1}) P_{n-2}(\theta_{n-2}) \ldots P_1(\theta_1)
\right)^\top = P_1(\theta_1)^\top P_2(\theta_2)^\top \ldots
P_{n-1}(\theta_{n-1})^\top \\
&= P_1(-\theta_1) P_2(-\theta_2) \ldots P_{n-1}(-\theta_{n-1}) \ .
\end{align*}
We have $A_i = Q_iR_i$, where $R_i$ is an upper-triangular matrix and $Q_i$
is an orthogonal matrix.

To complete the justification of the QR algorithm above for the symmetric
tridiagonal matrices, we now show that $A_i$ is symmetric tridiagonal for
all $i$.  Since $A$ is symmetric, it follows by induction from
(\ref{C14L24}) that $A_i$ is symmetric (i.e.\ $A_i^\top = A_i$)
for all $i$.  Moreover, since $A_0 = A$ is in Hessemberg form, it follows
from Remark~\ref{C14L33} that $A_i$ is in Hessemberg form
for all $i$.  Thus, the matrix $A_i$ is symmetric tridiagonal for all $i$.

\subsection{Shifting Technique}

We have mentioned before that the convergence of the sequence
$\displaystyle \{ A_i\}_{i=0}^\infty$ provided by the QR algorithm is not
fast, even if $A_0 = A$ is symmetric tridiagonal.  To accelerate the
convergence of the sequences, we present a technique similar to the
shifting technique used for the inverse power method.

Suppose that we have computed
\[
A_i =
\begin{pmatrix}
a_{i;1} & b_{i;1} & 0 & \ldots & 0 & 0 & 0 \\
b_{i;1} & a_{i;2} & b_{i;2} & \ldots & 0 & 0 & 0 \\
0 & b_{i;2} & a_{i;3} & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & b_{i;n-2} & a_{i;n-1} & b_{i;n-1} \\
0 & 0 & 0 & \ldots & 0 & b_{i;n-1} & a_{i;n}
\end{pmatrix}
\ .
\]
To compute $A_{i+1}$, we consider the matrix $A_i - s_i \Id$, where
$s_i$ is the eigenvalue of
\[
\begin{pmatrix}
a_{i;n-1} & b_{i;n-1} \\
b_{i;n-1} & a_{i;n}
\end{pmatrix}
\]
which is closest to $a_{i;n}$.  Since $A_i - s_i \Id$ is symmetric
tridiagonal, we may use the QR factorization method of the previous section
to write $A_i - s_i \Id = Q_i R_i$, where $Q_i$ is an orthogonal matrix and
$R_i$ is an upper-triangular matrix.  The matrix $A_{i+1}$ is defined by
$A_{i+1} = R_i Q_i$ as usual.

We first prove by induction that
\begin{equation} \label{C14L40}
  A_{i+1} = Q_i^\top Q_{i-1}^\top \ldots Q_0^\top A_0 Q_0 Q_1 \ldots
  Q_i - \sum_{j=0}^i s_j \Id \ .
\end{equation}
Since $A_0 - s_0 \Id = Q_0R_0$ and $A_1= R_0Q_0$, we get
\[
A_1 = Q_0^\top \left(A_0 - s_0 \Id\right) Q_0 = Q_0^\top A_0 Q_0 -s_0 \Id \ .
\]
This proves (\ref{C14L40}) for $i=0$.  Suppose that (\ref{C14L40})
is true for $i=k$; namely,
\[
A_{k+1} = Q_k^\top Q_{k-1}^\top \ldots Q_0^\top A_0 Q_0 Q_1 \ldots Q_k -
\sum_{j=0}^k s_j \Id \ .
\]
Then $A_{k+1} - s_{k+1} \Id = Q_{k+1}R_{k+1}$ and $A_{k+2}= R_{k+1} Q_{k+1}$
yield
\begin{align*}
A_{k+2} &= Q_{k+1}^\top \left( A_{k+1} - s_{k+1} \Id\right) Q_{k+1} \\
&= Q_{k+1}^\top \left( Q_k^\top Q_{k-1}^\top \ldots Q_0^\top A_0 Q_0 Q_1
\ldots Q_k - \sum_{j=0}^k s_j \Id - s_{k+1} \Id \right) Q_{k+1} \\
&= Q_{k+1}^\top Q_k^\top \ldots Q_0^\top A_0 Q_0 Q_1 \ldots Q_{k+1} -
\left( \sum_{j=0}^{k+1} s_j \right) Q_{k+1}^\top \Id  Q_{k+1} \\
&= Q_{k+1}^\top Q_k^\top\ldots Q_0^\top A_0 Q_0 Q_1 \ldots Q_{k+1}
- \sum_{j=0}^{k+1} s_j \Id \ ,
\end{align*}
where we have used the hypothesis of induction for the second equality.
This proves that (\ref{C14L40}) is true for $i=k+1$.
By induction, (\ref{C14L40}) is true for all $i$.

Hence, the eigenvalues of $A$ are of the form
$\displaystyle \lambda + \sum_{j=0}^i s_j$, where $\lambda$ is an eigenvalue
of $A_{i+1}$.
If $b_{i+1;n-1}$ is negligible (to be defined by the user), we may assume
that $b_{i+1,n-1}=0$ and thus $\displaystyle a_{i+1;n} + \sum_{j=0}^i s_j$
is an eigenvalue of $A$.

To find the other eigenvalues of $A$, we consider the
\nm{(n-1)}{(n-1)} matrix
\[
C = \begin{pmatrix}
a_{i+1;1} & b_{i+1;1} & 0 & \ldots & 0 & 0 & 0 \\
b_{i+1;1} & a_{i+1;2} & b_{i+1;2} & \ldots & 0 & 0 & 0 \\
0 & b_{i+1;2} & a_{i+1;3} & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & b_{i+1;n-3} & a_{i+1;n-2} & b_{i+1;n-2} \\
0 & 0 & 0 & \ldots & 0 & b_{i+1;n-2} & a_{i+1;n-1}
\end{pmatrix} \ .
\]
The eigenvalues of $A_{i+1}$ other than (one copy of)
$\displaystyle a_{i+1;n} + \sum_{j=0}^i s_j$ ``are'' the eigenvalues
of $C$.  We used quotation marks in the previous sentence because it is
rigorously true only if $b_{i+1,n-1} = 0$, not just when $b_{i+1,n-1}$
is negligible.  We repeat the previous QR algorithm with shifting with
$A_0$ replaced by $C$ to find an eigenvalue $\lambda$ of $C$.  We have
that $\displaystyle \lambda + \sum_{j=0}^i s_j$ is an eigenvalue of $A$.

In general, we can repeat recursively this procedure to approximate all
eigenvalues of $A$.  The QR algorithm with shifting suffers from some of the
weaknesses that the standard QR algorithm has.

The following code implement the QR algorithm with shifting.  The
equations (\ref{C14L34}) to (\ref{C14L39}) inclusively have been used to
create this algorithm.

\begin{code}[QR Algorithm with Shifting]
To find the eigenvalues of a symmetric tridiagonal matrix $A$.\\
\subI{Input} The components $a_1$, $a_2$, \ldots, $a_n$, $b_1$, $b_2$,
\ldots, $b_{n-1}$ of the symmetric tridiagonal matrix $A$. \\
The maximum number $N$ of iterations for the QR decomposition. \\
The value $d$ such that numbers $b$ satisfying $|b|< d$ may be
considered as null.\\
\subI{Output} An approximation for each eigenvalue of $A$ if it is
possible.
\small
\begin{verbatim}
function E = QRshifting(a, b, N, d)
  n = length(a);
  E = repmat(NaN,n,1);
  nE = 0;
  s = 0;          % sum of the shifts

  for k = 1:N
    fprintf('%d ... ',k);

    % If n=1, we are done
    if ( n == 1 )
      nE = nE + 1;
      E(nE) = a(1) + s;
      return;
    end 

    % If the matrix can be splitted into two symmetric tridiagonal
    % matrices, we do so.
    for j = 1:n-1
      if ( abs(b(j)) < d )
        disp 'Splitting the matrix';
        E(nE+1:nE+j,1) = QRshifting(a(1:j),b(1:j-1), N, d) + s;
        E(nE+j+1:n,1) = QRshifting(a(j+1:n),b(j+1:n-1), N, d) + s;
        return;
      end
    end

    % We compute the eigenvalues of the matrix
    %    [ a_{n-1} b_{n-1} ]
    %    [ b_{n-1} a_n     ]
    % We use the appropriate form of the formula to find the roots
    % of a quadratic equation to avoid subtraction of almost equal
    % numbers.
    B = -(a(n-1) + a(n));
    C = a(n)*a(n-1) - b(n-1)*b(n-1);
    D = sqrt(B^2-4*C);
    if ( B > 0 )
      r1 = -2*C/(B+D);
      r2 = -(B+D)/2;
    else
      r1 = (D-B)/2;
      r2 = 2*C/(D-B);
    end

    % If we have only a 2 x 2 matrix, we have found approximations
    % for the last two eigenvalues of A.
    if ( n == 2)
      nE = nE + 1;
      E(nE,1) = r1 + s;
      nE = nE + 1;
      E(nE,1) = r2 + s;
      return;
    end

    % Chose the appropriate shift
    if ( abs(r1-a(n)) < abs(r2 -a(n)) )
      stmp = r1;
    else
      stmp = r2;
    end
    s = s + stmp;
    a = a - stmp;

    % Get the QR decomposition
    x = a(1);
    y = b(1);
    for j = 1:n-1
      alpha(j) = sqrt( x^2 +b(j)^2 );
      ccc(j) = x/alpha(j);
      sss(j) = b(j)/alpha(j);
      beta(j) = y*ccc(j) + a(j+1)*sss(j);
      x = - y*sss(j) + a(j+1)*ccc(j);
      if ( j ~= n-1 )
        gamma(j) = b(j+1)*sss(j);
        y = b(j+1)*ccc(j);
      end
    end
    alpha(n) = x;

    % Compute RQ knowing that the result is a symmetric tridiagonal matrix
    a(1) = alpha(1)*ccc(1) + beta(1)*sss(1);
    b(1) = alpha(2)*sss(1);
    for j = 2:n-1;
      a(j) = alpha(j)*ccc(j-1)*ccc(j) + beta(j)*sss(j);
      b(j) = alpha(j+1)*sss(j);
    end
    a(n) = alpha(n)*ccc(n-1);
  end
end
\end{verbatim}
\end{code}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "notes"
%%% End: 
