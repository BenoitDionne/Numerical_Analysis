\chapter[Initial Value Problems]{Initial Value Problems for Ordinary Differential Equations}
\label{chaptInitVal}

\section{Introduction to Ordinary Differential Equations}

We consider the initial value problem
\begin{equation} \label{ODE}
\begin{split}
\dydx{y}{t}(t) &= f(t,y(t)) \quad,\quad t_0 \leq t \leq t_f \\
y(t_0) & = y_0
\end{split}
\end{equation}
where $f:[t_0,t_f]\times\RR \rightarrow \RR$.

In this section, we develop numerical algorithms to approximate the
solution $y$ of (\ref{ODE}) on $[t_0,t_f]$.  Before attempting to
numerically solve (\ref{ODE}), we have to get a positive answer to the
following questions.

\begin{Quest*}
Is the initial value problem (\ref{ODE})
``well-posed?''\quad Namely, is there a solution to (\ref{ODE}) and, if
there is one, is it unique?  Moreover, does a small ``perturbation''
of (\ref{ODE}) implies only a ``small variation'' in the solution of
(\ref{ODE})?
\end{Quest*}

If the initial value problem is not well-posed, then there is not
point in attempting to numerically solve (\ref{ODE}).  Suppose that
(\ref{ODE}) is deduced from experimental data associated to a given
physical phenomenon, then (\ref{ODE}) is a ``perturbation'' of the
real ordinary differential equation governing this physical
phenomenon.  Hence, if the problem is not ``well-posed'', then the
analytical solution of (\ref{ODE}) may not be related
to the analytical solution of the real ordinary differential equation
governing the physical phenomenon.  The same can be said about the
numerical solution.  Moreover, even if (\ref{ODE}) is
the real ordinary differential equation governing the physical
phenomenon, solving (\ref{ODE}) numerically is equivalent to solving
analytically a ``perturbation'' of (\ref{ODE}).  Hence, if the problem
is not ``well-posed'', then the numerical solution of (\ref{ODE}) may
not be related to the analytic solution of (\ref{ODE}) but to the
analytic solution of the ``perturbation'' of (\ref{ODE})

Before giving conditions on $f$ that guarantee that an initial value
problem (\ref{ODE}) is ``well-posed'', we have to clarify the
meaning of ``perturbation'' and ``well-posed'' problem.

\begin{defn}
A {\bfseries perturbation}\index{Initial Value Problems!Perturbation}
of (\ref{ODE}) is an initial value problem
of the form
\begin{equation} \label{PODE}
\begin{split}
\dydx{z}{t}(t) &= f(t,z(t)) +\delta(t) \quad,\quad t_0 \leq t \leq t_f \\
z(t_0) & = y_0 + \delta_0
\end{split}
\end{equation}
where $\delta:[t_0,t_f]\rightarrow \RR$ is a continuous function and
$\delta_0$ is a constant.
\label{pertub_def}
\end{defn}

\begin{defn}
The initial value problem (\ref{ODE}) is
{\bfseries well posed}\index{Initial Value Problems!Well Posed} if:
\begin{enumerate}
\item There is a unique solution $y$ to (\ref{ODE}).
\item There exist positive constants $K$ and $E$ such that for any
positive $\epsilon \leq E$, the solution $z(t)$ of the perturbed
problem (\ref{PODE}) satisfies
\[
\left| y(t) - z(t) \right| < K\epsilon
\]
for all $t \in [t_0,t_f]$ if 
$|\delta(t)| < \epsilon$ for all $t \in [t_0,t_f]$ and
$|\delta_0| < \epsilon$ (Figure~\ref{wellposed}).
\end{enumerate}
\label{well_posed}
\end{defn}

\pdfF{init_value_probl/well_posed}{Uniform approximation of the solution of an
ODE}{Uniform approximation of $y$ by $z$ on $[t_0,t_f]$.}{wellposed}

The following theorem gives conditions for the initial value problem
(\ref{ODE}) to be well-posed.

\begin{theorem}
Let $D = \{(t,y) : t_0 \leq t \leq t_f \text{ and } -\infty < y < \infty \}$.
Suppose that $f:D \rightarrow \RR$ is continuous and that there exists
a constant $L$ such that
\begin{equation} \label{Lipschitz}
|f(t,y) - f(t,\tilde{y})| \leq L | y - \tilde{y}|
\end{equation}
for all $(t,y)$ and $(t,\tilde{y})$ in $D$.  Then the initial value
problem (\ref{ODE}) is well-posed.
\label{WellPossedTh}
\end{theorem}

\begin{rmk}
If (\ref{Lipschitz}) is satisfied, we say that $f$ satisfies a
{\bfseries Lipschitz condition}\index{Lipschitz Condition} with
respect to its second variable on $D$ or that $f$ is
{\bfseries Lipschitz continuous}\index{Lipschitz Continuous} with
respect to its second variable on $D$.  $L$ is called a
{\bfseries Lipschitz constant}\index{Lipschitz Constant}.
\end{rmk}

\begin{proof}[Proof (partial)]
The existence and uniqueness of the solution of the initial value
problem (\ref{ODE}) is usually proved in good introductory textbooks
on ordinary differential equations.  The main idea for the proof of
existence given by Peano is to construct a contraction mapping whose
fixed point is the local solution of the ordinary differential
equations.

We prove the second condition of the definition of well-posed
problem.

Let $r(t) = z(t)-y(t)$ where $y(t)$ is the solution of
(\ref{ODE}) and $z(t)$ is the solution of (\ref{PODE}).  If we
subtract (\ref{ODE}) from (\ref{PODE}), we get
\begin{align*}
r'(t) &= f(t,z(t)) - f(t,y(t)) + \delta(t)\\
r(t_0) &= \delta_0
\end{align*}
As required in Definition~\ref{well_posed}, let's assume that
$|\delta(t)| < \epsilon$ for all $t \in [t_0,t_f]$ and
$|\delta_0| < \epsilon$.  We get from (\ref{Lipschitz}) that
\[
r(t) - r(t_0) = \int_{t_0}^t r'(s) \dx{s} 
= \int_{t_0}^t f(s,z(s)) - f(s,y(s)) \dx{s}
+ \int_{t_0}^t \delta(s) \dx{s} \ .
\]
Hence,
\[
|r(t)| \leq |r(t_0)| + \int_{t_0}^t |f(s,z(s)) - f(s,y(s))| \dx{s}
+ \int_{t_0}^t |\delta(s)| \dx{s}
\leq \epsilon + L \int_{t_0}^t |r(s)| \dx{s} + \epsilon (t_f - t_0)
\]
for all $t \in [t_0,t_f]$.  It follows from Gronwall's
Lemma\footnote{Another fundamental result that one can find in a good
introductory textbook on ordinary differential equations.} that
\[
  |r(t)| \leq \epsilon ( 1 + t_f - t_0) e^{L(t-t_0)}
\leq \epsilon ( 1 + t_f - t_0) e^{L(t_f-t_0)}
\]
for all $t \in [t_0,t_f]$.  We conclude that
\[
  |r(t)| \leq K \epsilon
\]
with
\[
  K = ( 1 + t_f - t_0) e^{L(t_f-t_0)}
\]
for $t \in [t_0,t_f]$.
\end{proof}

\section{Euler's Method}

We introduce in this section the simplest numerical method to
approximate the solution of an initial value problem.  Even if it is
not the best numerical method, it is still a good method to introduce
most of the concepts and issues involved in the numerical
approximation of solutions of initial value problem.

Suppose that (\ref{ODE}) is well-posed.  The general procedure to
{\bfseries approximate the solution}\index{Initial Value
Problems!Approximate the Solution} of (\ref{ODE}) is as follows: 
\begin{enumerate}
\item Choose a positive integer $N$.
\item Select $N+1$
{\bfseries mesh points}\index{Initial Value Problems!Mesh Points}
$t_0 < t_1 < t_2 < \ldots < t_N = t_f$ (usually equally spaced.)
\item Find an approximation $w_i$ of $y_i = y(t_i)$ for $i=1$,
 $2$, \ldots, $N$.
\item Use linear interpolation at the points $(t_i,w_i)$
 (or higher order polynomial interpolation) to approximate $y(t)$ at
 $t\not=t_i$ for $i=0$, $1$, \ldots, $N$ (Figure~\ref{eulergraph}).
\end{enumerate}

Our first procedure to compute an approximation $w_i$ to $y_i$ is the
Euler's method.

\begin{defn}[Euler's Method]
Let $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for $i=0$,
$1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the
solution of the difference equation
\begin{equation} \label{Euleralgor}
\begin{split}
w_{i+1} & = w_i + h \, f(t_i, w_i) \quad, \quad 0 \leq i < N \\
w_0 & = y_0
\end{split}
\end{equation}
\label{EulerMethod}
\end{defn}

\pdfF{init_value_probl/euler}{Graph of the solution of an ordinary
differential equation and its approximation given by the Euler's
method}{Graph of the solution $y$ and its approximation given by the
Euler's method.}{eulergraph}

\begin{rmkList}
\begin{enumerate}
\item The mesh points in the presentation of the Euler's method are
equally spaced.  However, the mesh points $t_i$ do not have to be 
equally spaced.  We may simply require that $h_i = t_{i+1} - t_i$ for
$0\leq i < N$ satisfy
$\displaystyle \max_{0\leq i <N}\,|h_i| < K \min_{0\leq i <N}\,|h_i|$
for a constant $K$.
\item The Euler's method can be justified as follows.  Suppose that $f$
is continuously differential with respect to both variables.
From Theorem~\ref{TaylorTheo}, we have
\[
y(t_{i+1}) = y(t_i) + y'(t_i)\,(t_{i+1} -t_i) +
\frac{y''(\xi_i)}{2}\left(t_{i+1} - t_i\right)^2
\]
for some $\xi_i$ between $t_i$ and $t_{i+1}$.  If we substitute
$y'(t_i) = f(t_i,y(t_i))$, $y_i= y(t_i)$ and $h=t_{i+1}-t_i$ in
the previous equation, we get
\begin{equation} \label{Eulerexact}
y_{i+1} = y_i + f(t_i,y_i)\,h + \frac{y''(\xi_i)}{2}\,h^2
\end{equation}
for some $\xi_i$ between $t_i$ and $t_{i+1}$.  If we assume that
$y''(\xi_i)h^2/2$ is much smaller than $y_i + f(t_i,y_i)\,h$ for all
$i$ and for $h$ small enough, we get the Euler's method by removing this
term from (\ref{Eulerexact}).
\item The
{\bfseries local discretization error}\index{Initial Value Problems!Local
Discretization Error} for the Euler's method is $y''(\xi_i)h^2/2$.
\end{enumerate}
\end{rmkList}

\begin{egg}
Use Euler's method with $N=5$ to approximate the solution $y$ of
\begin{equation}\label{ODE_EX1}
\begin{split}
y'(t) & =  0.2\,ty \quad, \quad 1 \leq t \leq 1.5 \\
y(1) & =  1
\end{split}
\end{equation}

We have $t_0=1$, $t_f= t_5 =1.5$, $y_0 = 1$ and $f(t,y) = 0.2\,ty$.
Hence $h = (t_5-t_0)/5 = 0.1$, $t_i = t_0 + ih =1 + 0.1i$ and
the approximation $w_i$ of $y_i = y(t_i)$ is given by
\begin{align*}
w_0 & = 1 \\
w_{i+1} & = w_i + 0.02\,t_i w_i \quad, \quad 0 \leq i < 5 .
\end{align*}
The results of these computations are given in the following table.
\[
\begin{array}{clllll}
i & t_i & w_i & y_i & \text{absolute} & \text{relative} \\
  & & & & \text{error} & \text{error} \\
\hline
0 & 1.00 & 1.0000 & 1.0000 & 0.0 & 0.0 \\
1 & 1.10 & 1.02 & 1.0212220516 & -0.0012220516 & 0.0011966561 \\
2 & 1.20 & 1.04244 & 1.0449823549 & -0.0025423549 & 0.0024329166 \\
3 & 1.30 & 1.06745856 & 1.0714362091 & -0.0039776491 & 0.0037124461 \\
4 & 1.40 & 1.0952124826 & 1.1007590640 & -0.0055465814 & 0.0050388696 \\
5 & 1.50 & 1.1258784321 & 1.1331484531 & -0.0072700210 & 0.0064157710 \\
\hline
\end{array}
\]
Since the differential equation in (\ref{ODE_EX1}) is separable, it is
easy to find the exact solution $y(t) = e^{0.1t^2 -0.1}$ of
(\ref{ODE_EX1}).  We have used this formula to compute $y_i$.
Our approximation $w_5$ of $y_5$ has a relative error of about $0.64$
\%.  This is good.
\end{egg}

\begin{egg}
Use the Euler's method with $N=5$ to approximate the solution $y$ of
\begin{equation}\label{ODE_EX2}
\begin{split}
y'(t) & =  2\,ty \quad, \quad 1 \leq t \leq 1.5 \\
y(1) & =  1
\end{split}
\end{equation}

As in the previous example, we have $t_0=1$, $t_f= t_5 =1.5$  and
$y_0=1$.  However, $f(t,y) = 2\,ty$.  Hence $h = (t_5-t_0)/5 = 0.1$,
$t_i = t_0 + hi =1 + 0.1i$ and the approximation $w_i$ of
$y_i = y(t_i)$ is given by
\begin{align*}
w_0 & = 1 \\
w_{i+1} & = w_i + 0.2\,t_i w_i \quad, \quad 0 \leq i < 5 \ .
\end{align*}
The results of these computations are given in the following table.
\[
\begin{array}{clllll}
i & t_i & w_i & y_i & \text{absolute} & \text{relative} \\
 & & & & \text{error} & \text{error} \\
\hline
0 & 1.0 & 1.0000 & 1.0000 & 0.0 & 0.0 \\
1 & 1.1 & 1.2000 & 1.2336780600 & -0.0336780600 & 0.0272989048 \\
2 & 1.2 & 1.4640 & 1.5527072185 & -0.0887072185 & 0.0571306795 \\
3 & 1.3 & 1.81536 & 1.9937155332 & -0.1783555332 & 0.0894588673 \\
4 & 1.4 & 2.2873536 & 2.6116964734 & -0.3243428734 & 0.1241885789 \\
5 & 1.5 & 2.927812608 & 3.4903429575 & -0.5625303495 & 0.1611676435 \\
\hline
\end{array}
\]
Since the differential equation in (\ref{ODE_EX2}) is separable, it is
easy to find the exact solution $y(t) = e^{t^2 -1}$ of
(\ref{ODE_EX2}).  We have used this formula to compute $y_i$.
Our approximation $w_5$ of $y_5$ has a relative error of about $16.12$ \%.
This is not good.  The Euler's method does not give good
approximations of $y_i$ for $1 \leq i \leq 5$.

To find the reason behind these poor numerical results compared to those
of the previous example, we have to compare the graphs of the
solutions.  The graph of the solution of (\ref{ODE_EX1}) is concave
up and so is the graph of the solution of (\ref{ODE_EX2})
because $f(t,y) >0$ for $t>0$ and $y>0$ in both cases.  However,
the solution of (\ref{ODE_EX2}) increases a lot faster than the
solution of (\ref{ODE_EX1}).  It is therefore easy to imagine
(Figure~\ref{eulergraphex}) that the distance between the graph of the
solution of (\ref{ODE_EX2}) and the graph of its numerical
approximation increases faster than the distance
between the graph of the solution of (\ref{ODE_EX1}) and the graph of
its numerical approximation.
\label{Euler_EX2}
\end{egg}

\mathFD{init_value_probl/euler_ex12a}{7cm}{init_value_probl/euler_ex12b}{7cm}
{Graphs of the solutions of two differential equations and their
approximations given by the Euler's method.}{Graphs of the
solution (in black) of (\ref{ODE_EX1}) on the left and of
(\ref{ODE_EX2}) on the right with the graphs of their approximation (in
blue) given by the Euler's method.}{eulergraphex}

We now investigate the effect of local discretization and rounding
error on the Euler's method.  Due to rounding error, solving
(\ref{Euleralgor}) numerically is equivalent to solving
\begin{equation} \label{Eulerpertub}
\begin{split}
u_{i+1} & = u_i + h \, f(t_i, u_i) + \delta_{i+1}\\
u_0 & = y_0 + \delta_0
\end{split}
\end{equation}
exactly, where $\delta_0$ is the error in approximating $y_0$ and
$\delta_{1+i}$ is the rounding error in computing
$w_i+h\,f(t_i,w_i)$.  For $0 \leq i \leq N$, the value $u_i$
represents the computed value of $w_i$.

\begin{theorem}
Let $e_i = y_i - u_i$ for $i=0$, $1$, \ldots, $N$.  Suppose that:
\begin{enumerate}
\item There exists $\delta$ such that $|\delta_i| < \delta$ for $i=1$,
$2$, \ldots, $N$.  (Note that for a given computer, this assumption
makes sense.)
\item The function $f$ satisfies the Lipschitz condition
(\ref{Lipschitz}) on $[t_0,t_f]\times \RR$.
\item There exists $M>0$ such that $|y''(t)| < M$ for all $t$ in
$[t_0,t_f]$.
\end{enumerate}
Then,
\begin{equation}\label{abserrEuler}
|e_i| \leq \frac{1}{L}\left(\frac{Mh}{2} + \frac{\delta}{h} \right)\left(
e^{L(t_i-t_0)} - 1 \right) + |\delta_0|\,e^{L(t_i-t_0)} \ .
\end{equation}
\label{Eulererrorbound}
\end{theorem}

\begin{proof}
If we subtract the first equation of (\ref{Eulerpertub}) from
(\ref{Eulerexact}), we get
\begin{equation} \label{Eulererror}
\begin{split}
e_{i+1} &= e_i + h\left(f(t_i,y_i) - f(t_i,u_i)\right) +
\frac{y''(\xi_i)}{2}\,h^2 - \delta_{1+i} \\
e_0 &= -\delta_0
\end{split}
\end{equation}
Since $f$ satisfies the Lipschitz condition (\ref{Lipschitz}) on
$[t_0,t_f]\times \RR$, we have
\[
|f(t,y_i) - f(t,u_i)| \leq L | y_i - u_i| \;  .
\]
Hence, if we take the absolute value on both sides of the equations in
(\ref{Eulererror}), we get
\begin{equation} \label{Eulerabsol}
\begin{split}
|e_{i+1}| &\leq |e_i| + hL|e_i| + \frac{M}{2}\,h^2 + \delta \\
|e_0| &= |\delta_0|
\end{split}
\end{equation}

Consider the difference equation
\begin{equation} \label{Eulerdiff}
\begin{split}
\eta_{i+1} &= \eta_i + hL\eta_i + \frac{M}{2}\,h^2 + \delta \\
\eta_0 &= |\delta_0|
\end{split}
\end{equation}
The solution of (\ref{Eulerdiff}) is
\[
\eta_i = A(1+hL)^i + B \ ,
\]
where
\begin{align*}
A &= |\delta_0| + \frac{1}{hL}\left(\frac{M}{2}\,h^2 +
\delta\right) = |\delta_0| + \frac{1}{L}\left(\frac{Mh}{2} +
\frac{\delta}{h} \right)
\intertext{and}
B & = -\frac{1}{hL}\left(\frac{M}{2}\,h^2 + \delta\right) =
-\frac{1}{L}\left(\frac{Mh}{2} + \frac{\delta}{h}\right)\; .
\end{align*}

We can easily show by induction that $|e_i| \leq \eta_i$ for $i=0$,
$1$, \ldots, $N$.   This is true for $i=0$ because
$\eta_0 = |\delta_0| = |e_0|$.  Suppose that
$|e_i| \leq \eta_i$ for $i=0$, $1$, \ldots, $k$.  Then it follows from
(\ref{Eulerabsol}) that
\[
|e_{k+1}| \leq |e_k| + hL|e_k| + \frac{M}{2}\,h^2 + \delta \leq
\eta_k + hL\eta_k + \frac{M}{2}\,h^2 + \delta = \eta_{k+1} \ .
\]
Thus, $|e_i| \leq \eta_i$ for all $i$ by induction.

Hence,
\begin{align*}
|e_i| & \leq \eta_i = A(1+hl)^i + B
= \left( |\delta_0| + \frac{1}{L}\left(\frac{Mh}{2} +
\frac{\delta}{h} \right)\right)\left(1+hL\right)^i -
\frac{1}{L}\left(\frac{Mh}{2} + \frac{\delta}{h}\right) \\
 &\leq \left( |\delta_0| + \frac{1}{L}\left(\frac{Mh}{2} +
\frac{\delta}{h} \right)\right)\,e^{ihL} -
\frac{1}{L}\left(\frac{Mh}{2} + \frac{\delta}{h}\right) \\
&= \frac{1}{L}\left(\frac{Mh}{2} + \frac{\delta}{h} \right)\left(
e^{L(t_i-t_0)} - 1 \right) + |\delta_0|\,e^{L(t_i-t_0)} \ ,
\end{align*}
where we have used $(1+x)^n \leq e^{nx}$ for $x > 0$ in the second
inequality, and $ih = t_i -t_0$ in the last equality.
\end{proof}

\begin{rmkList}
\begin{enumerate}
\item If we assume the idealistic case where there are no rounding
and approximation errors, namely $\delta = \delta_0 = 0$, then
(\ref{abserrEuler}) in Theorem~\ref{Eulererrorbound} yields
\[
  |w_i - y(t_i)| = |e_i| \leq \frac{M}{2L}\,\left(e^{L(t_f-t_0)} -1\right)\, h
\]
for all $i$.  It follows that
\[
  \lim_{h\to 0} \, \max_{0\leq i \leq N} |w_i - y(t_i)| = 0 \ .
\]
\item As for numerical differentiation, Euler's method is sensitive
to rounding error.  Theorem~\ref{Eulererrorbound} suggests that
\[
|e_i| \approx \frac{1}{L}\left(\frac{Mh}{2} + \frac{\delta}{h} \right)\left(
e^{L(t_i-t_0)} - 1 \right) + |\delta_0|\,e^{L(t_i-t_0)} \ ,
\]
where the factor
$\displaystyle \frac{Mh}{2} + \frac{\delta}{h}$ goes to infinity as
$h$ goes to $0$.
\end{enumerate}
\end{rmkList}

\section{Higher-Order Taylor Methods}

The Euler's method is a nice method to introduce the concept of
numerical solution of initial value problems of the form (\ref{ODE}).
However, it is not a really good method.  We now turn our attention to
``better methods.''\quad  Before that, we need some concepts to define what
we mean by ``better methods.''

\begin{defn}
The
{\bfseries local truncation error}\index{Initial Value Problems!Local
Truncation Error} of a method of the form 
\begin{equation} \label{Taylormethod}
\begin{split}
w_{i+1} & = w_i + h \phi(t_i,w_i) \quad, \quad 0 \leq i < N \\
w_0 & = y_0
\end{split}
\end{equation}
to numerically solve (\ref{ODE}) is defined as
\begin{equation} \label{truncerror}
\tau_{i+1}(h) = (y_{i+1} - y_i)/h - \phi(t_i,y_i)
\end{equation}
with $y_i = y(t_i)$ for $i=0$, $1$, $2$,\ldots, $N$.

If there exist a function $\tau:\RR\to\RR$ such that
$|\tau_{i+1}(h) | \leq \tau(h)$ for all $i$, and a positive
integer $k$ (as large as possible) such that $\tau(h) = O(h^k)$ near
the origin, then we say that the method (\ref{Taylormethod}) is of
{\bfseries order k}\index{Initial Value Problems!Order of a Method}.
\label{localTruncErrorDef}
\end{defn}

\begin{egg}
For the Euler's method $\phi(t,y) = f(t,y)$ in (\ref{Taylormethod})
It follows from (\ref{Eulerexact}) that the local truncation error for
the Euler's method is
\[
\tau_{i+1}(h) = (y_{i+1} - y_i)/h - f(t_i,y_i) =
\frac{y''(\xi_i)}{2}\,h
\]
for some $\xi_i$ in $[t_{i-1}.t_i]$.  If there exists a constant $M$
such that $|y''(t)| \leq M$ for all $t\in [t_0,t_f]$, then
$|\tau_{i+1}(h)| \leq \tau(h) \equiv M|h|/2$ for all $i$ and
$\tau(h) = O(h)$ near the origin.  So, the Euler's method is of order $1$.
\end{egg}

\begin{rmk} Since $|h^p|$ is a decreasing function of $p$ for $|h|<1$
fixed, the local truncation error is generally smaller for high order
methods than for low order methods.
\end{rmk}

\begin{defn}[Taylor Method of order $2$]
Let $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for $i=0$,
$1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the
solution of the difference equation
\[
\begin{split}
w_{i+1} & = w_i + h\phi(t_i, w_i) \quad, \quad 0 \leq i < N \\
w_0 & = y_0
\end{split}
\]
where
\[
\phi(t,y) = f(t,y) + \frac{h}{2} \left(\pdydx{f}{t}(t,y) +
\left( \pdydx{f}{y}(t,y) \right)f(t,y) \right)
\; .
\]
\label{Tayloralgo}
\end{defn}

\begin{rmkList} \label{Taylorordern}
\begin{enumerate}
\item Assuming that $f$ is sufficiently differentiable, we may derive
Taylor methods of order $n>2$ by differentiating $n-1$ times with respect to
$t$ the expression $f(t,y(t))$.  The Taylor method of order $n$ can be
justified as follows.  From Theorem~\ref{TaylorTheo}, we have
\begin{align*}
y(t_{i+1}) &= y(t_i) + y'(t_i)\,(t_{i+1} -t_i) +
\frac{y''(t_i)}{2}\,(t_{i+1}-t_i)^2 + \ldots +
\frac{y^{(n)}(t_i)}{n!}\,(t_{i+1}-t_i)^n \\
&+ \frac{y^{(n+1)}(\xi_i)}{(n+1)!}\left(t_{i+1} - t_i\right)^{n+1}
\end{align*}
for some $\xi_i$ between $t_i$ and $t_{i+1}$.  We also have that
\begin{align*}
y'(t) &= f(t,y(t)) \ , \\
y''(t) &= \dfdx{f(t,y(t))}{t}
= \pdydx{f}{t}(t,y(t)) + \left( \pdydx{f}{y}(t,y(t)) \right)y'(t) \\
 &= \pdydx{f}{t}(t,y(t)) + \left( \pdydx{f}{y}(t,y(t)) \right)f(t,y(t))
\ ,\\
y^{(3)}(t) &= \ldots
\end{align*}
Since $h=t_{i+1} -t_i$, $y_i = y(t_i)$, $y'(t_i) = f(t_i,y_i)$,
$\displaystyle y''(t_i) =
\pdydx{f}{t}(t_i,y_i) + \pdydx{f}{y}(t_i,y_i)\, f(t_i,y_i)$, \ldots, we get
\begin{equation}\label{HigherMethexact}
\begin{split}
y_{i+1} &= y_i + h\underbrace{\left( f(t_i,y_i) + \frac{h}{2} \left(
\pdydx{f}{t}(t_i,y_i) + \pdydx{f}{y}(t_i,y_i) \, f(t_i,y_i)
\right) + \ldots \right)}_{=\phi(t_i,y_i)} \\
& \qquad + \frac{y^{(n+1)}(\xi_i)}{(n+1)!}\,h^{n+1}
\end{split}
\end{equation}
for some $\xi_i$ between $t_i$ and $t_{i+1}$.  If we assume that
$\displaystyle \frac{y^{(n+1)}(\xi_i)}{(n+1)!}\,h^{n+1}$ is small
for all $i$, we get the Taylor method of order $n$ by removing this
term from (\ref{HigherMethexact}).
\item The
{\bfseries local discretization error}\index{Initial Value
Problems!Local Discretization Error} for the Taylor method 
of order $n$ is
\[
\frac{1}{(n+1)!}\, y^{(n+1)}(\xi_i)h^{n+1} \ .
\]
\item The local truncation error is
\[
\tau_{i+1}(h) = \frac{y_{i+1} - y_i}{h} - \phi(t_i,y_i) = 
\frac{y^{(n+1)}(\xi_i)}{(n+1)!}h^n
\]
for $\xi_i \in [t_i,t_{i+1}]$.  If there exists a constant $M$ such
that $|y^{(n+1)}(t)| \leq M$ for all $t\in [t_0,t_f]$, then
$|\tau_{i+1}(h)| \leq \tau(h) \equiv M|h|^n/(n+1)!$ for all $i$ and
$\tau(h)  = O(h^n)$ near the origin.  This justifies the name Taylor
method of order $n$.
\item From a numerical point of view, the Taylor methods of order
$n>1$ are not very useful.  We may use these methods only when 
$\phi(t,y)$ can be easily computed symbolically.  Moreover, though the
local truncation error is smaller for the Taylor methods of order
$n>1$ than it is for the Euler's method, rounding error is generally
larger for the Taylor methods of order $n>1$ because of the number of
numerical operations necessary to evaluate $\phi(t_i,w_i)$ for $i=0$,
$1$, \ldots, $N-1$.
\end{enumerate}
\end{rmkList}

\section{Runge-Kutta Methods}

In this section, we develop numerical methods to approximate the
solution of (\ref{ODE}) that are of order greater than one and do
not require the evaluation of complicate functions like $\phi(t,y)$ in
the high order Taylor methods.

\begin{defn}[General Form of the Runge-Kutta Method]
Let $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for $i=0$,
$1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the solution of
the difference equation
\begin{align*}
w_{i+1} &= w_i + h \sum_{j=1}^s \gamma_j K_j \quad, \quad 0 \leq i < N \\
w_0 &= y_0
\end{align*}
where $s$ is a positive integer,
\[
K_j = f(t_i + \alpha_j h, w_i + h \sum_{m=1}^s \beta_{j,m} K_m)
\quad, \quad 1 \leq j \leq s
\]
and $\alpha_j$, $\beta_{j,m}$ and $\gamma_j$ are constants such that
\begin{equation}\label{GFRKMcond}
  \alpha_j = \sum_{m=1}^s \beta_{j,m} \quad \text{and} \quad
  \sum_{j=1}^s \gamma_j = 1 \ .
\end{equation}
The values $K_m$ are called the
{\bfseries stages}\index{Runge-Kutta!Stages} and the method is
described as a
{\bfseries $s$-stage Runge-Kutta method}\index{Runge-Kutta!$s$-stage Method}.

If $\beta_{j,m} = 0$ for $m\geq j$, the Runge-Kutta method is called
an {\bfseries explicit method}\index{Runge-Kutta!Explicit Method}.  Otherwise,
it is called an {\bfseries implicit method}\index{Runge-Kutta!Implicit Method}.
If $\beta_{j,m} = 0$ for $m > j$, the Runge-Kutta method is called a
{\bfseries semi-implicit method}\index{Runge-Kutta!Semi-Implicit Method}.
\label{GFRKM}
\end{defn}

The classical way to describe a Runge-Kutta method is with its
{\bfseries Butcher array}\index{Runge-Kutta!Butcher array}.
\[
\begin{array}{c|ccccc}
\alpha_1 & \beta_{1,1} & \beta_{1,2} & \ldots & \beta_{1,1} \\
\alpha_2 & \beta_{2,1} & \beta_{2,1} & \ldots & \beta_{2,1} \\
\alpha_3 & \beta_{3,1} & \beta_{3,2} & \ldots & \beta_{3,1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\alpha_s & \beta_{s,1} & \beta_{s,2} & \ldots & \beta_{s,s}\\
\hline
 & \gamma_1 & \gamma_2 & \ldots & \gamma_s
\end{array}
\]

The Butcher array for the explicit Runge-Kutta methods is as
follows.
\[
\begin{array}{c|ccccc}
\alpha_1 & & & & & \\
\alpha_2 & \beta_{2,1} & & & & \\
\alpha_3 & \beta_{3,1} & \beta_{3,2} & & & \\
\vdots & \vdots & \vdots & & & \\
\alpha_s & \beta_{s,1} & \beta_{s,2} & \ldots & \beta_{s,s-1} & \\
\hline
 & \gamma_1 & \gamma_2 & \ldots & \gamma_{s-1} & \gamma_s
\end{array}
\]

The Runge-Kutta methods do not only get some information from the
solution through $(t_0,y(t_0))$ but also from solutions that are near the
solution through $(t_0,y(t_0))$.  This information comes from $f(t,y)$.
Let's consider the explicit Runge-Kutta methods.  We first note that
$\alpha_1=0$ because of (\ref{GFRKMcond}).
\begin{itemize}
\item We have $K_1 = f(t_i,w_i)$.
\item $K_2 = f(t_i+\alpha_2 h,w_i + \beta_{2,1} h K_1)$,
where $w_i + \beta_{2,1} h K_1 = w_i + \alpha_2 h f(t_i,w_i)$ is
the approximation of $y(t)$ at $t=t_i+\alpha_2 h$ given by the Euler's
method.
\item $K_3 = f(t_i+\alpha_3 h, w_i + \beta_{3,1} h K_1 + \beta_{3,2} h K_2)$,
where $\beta_{3,1} K_1 + \beta_{3,2} K_2
= (\alpha_3 - \beta_{3,2})K_1 + \beta_{3,2} K_2$
is a weighted average of the approximations of $y'(t)$ at $t_i$ and
$t_i+\alpha_2 h$ respectively.
\item Similarly, 
$K_4 =  f(t_i+\alpha_4 h, w_i + \beta_{4,1} h K_1 + \beta_{4,2} h K_2
+ \beta_{4,3} h K_3)$, where
$\beta_{4,1} K_1 + \beta_{4,2} K_2 + \beta_{4,3} K_3
= (\alpha_4 - \beta_{4,2} - \beta_{4,3}) K_1 + \beta_{4,2} K_2 + \beta_{4,3} K_3$
is a weighted average of the approximations of $y'(t)$ at $t_i$,
$t_i + \alpha_2 h$ and $t_i + \alpha_3 h$.
\item And so on for all the $K_5$, $K_6$, \ldots
\end{itemize}
Hence, $K_j$ is an approximation
of $y'(t)$ at $t_i+\alpha_jh$ for $1\leq j \leq s$ and
$\displaystyle w_{i+1} = w_i + h \sum_{k=1}^s\,\gamma_k K_k$, where
$\displaystyle \sum_{k=1}^s\,\gamma_k K_k$ is a weighted average of
these approximations.

\begin{rmk}
We will see later that the conditions (\ref{GFRKMcond}) are necessary
conditions for an s-stage Runge-Kutta method to be of order $s$.
\end{rmk}

We now present some of the most famous explicit Runge-Kutta methods.

\begin{defn}[Runge-Kutta Methods of order two]
Let $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for $i=0$,
$1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the solution of
the difference equation
\begin{align*}
w_{i+1} & = w_i + h\left(\gamma_1 f(t_i,w_i) + \gamma_2
f(t_i+\alpha_2 h,w_i+ \beta_{2,1} hf(t_i,w_i))\right)
\quad, \quad 0 \leq i < N \\
w_0 & = y_0
\end{align*}
where $\alpha_2$, $\beta_{2,1}$, $\gamma_1$ and $\gamma_2$ are
constants satisfying $\gamma_1 + \gamma_2 = 1$, $\alpha_2 = \beta_{2,1}$
and $\alpha_2 \gamma_2 = 1/2$.
\label{RK2}
\end{defn}

\begin{rmkList}
\begin{enumerate}
\item Some well known Runge-Kutta methods of order two are:
\begin{description}
\item[Midpoint method:] $\alpha_2 = \beta_{2,1} = 1/2$, $\gamma_1=0$
and $\gamma_2=1$, 
Its Butcher array is
\[
\begin{array}{c|cc}
0 & & \\
1/2 & 1/2 & \\
\hline
 & 0 & 1
\end{array}
\]
\item[Modified Euler's method:] $\alpha_2 = \beta_{2,1} = 1$ and
$\gamma_1=\gamma_2=1/2$,
Its Butcher array is
\[
\begin{array}{c|cc}
0 & & \\
1 & 1 & \\
\hline
 & 1/2 & 1/2
\end{array}
\]
\item[Heun's method:] $\alpha_2 = \beta_{2,1}=2/3$, $\gamma_1=1/4$ and
$\gamma_2=3/4$.
Its Butcher array is
\[
\begin{array}{c|cc}
0 & & \\
2/3 & 2/3 & \\
\hline
 & 1/4 & 3/4
\end{array}
\]
\end{description}
\item The motivation for the Runge-Kutta methods of order two is as
follows.  We assume that all the mixed partial derivatives of
$f:[t_0,t_f]\times \RR \rightarrow \RR$ exist and are continuous up to
order two.  Up to $O(h^2)$, we replace the function $\phi(t,y)$ in the
Taylor method of order $2$ (Definition~\ref{Tayloralgo}) by an
expression of the form $\gamma_1 K_1 + \gamma_2 K_2$, where
$K_1 = f(t,y)$ and $K_2=f(t+\alpha_2 h,y+\beta_{2,1} h K_1)$ for some
$\gamma_1$, $\gamma_2$, $\alpha_2$ and $\beta_{2,1}$.

Using Taylor expansion theorem in two variables, we have
\[
f(t+\alpha_2 h,y+\beta_{2,1} h K_1) = f(t,y) +
\alpha_2 h\, \pdydx{f}{t}(t,y) + \beta_{2,1} h K_1 \, \pdydx{f}{y}(t,y)
+ O(h^2) \ .
\]
Hence
\begin{equation}\label{RKmatch}
\begin{split}
\gamma_1 K_1 + \gamma_2 K_2 &= (\gamma_1+\gamma_2)f(t,y)
+ \alpha_{2} \gamma_2 h \, \pdydx{f}{t}(t,y) \\
&\quad + \beta_{2,1} \gamma_2 h f(t,y) \, \pdydx{f}{y}(t,y) + O(h^2) \ .
\end{split}
\end{equation}
If we match the coefficients of $f(t,y)$,
$\displaystyle h\, \pdydx{f}{t}(t,y)$ and 
$\displaystyle h f(t,y)\, \pdydx{f}{y}(t,y)$ in 
(\ref{RKmatch}) with those in
\[
\phi(t,y) = f(t,y) + \frac{1}{2} h \, \pdydx{f}{t}(t,y) + \frac{1}{2}
h f(t,y) \, \pdydx{f}{y}(t,y) \ ,
\]
we get $\gamma_1+\gamma_2=1$, $\alpha_{2} \gamma_2 = 1/2$ and
$\beta_{2,1} \gamma_2 = 1/2$.
\item If we assume that all the mixed partial derivatives of
$f:[t_0,t_f]\times \RR \rightarrow \RR$ exist and are continuous up to
order two, then the local truncation errors of the Runge-Kutta methods
in Definition~\ref{RK2} are bounded by a function $\tau$ (found for the
Taylor method of order $2$) such that $\tau(h) = O(h^2)$ near the
origin as their name suggests.
\end{enumerate}
\end{rmkList}

\begin{egg}
Use the modified Euler's method with $N=5$ to approximate the solution
$y$ of the initial value problem (\ref{ODE_EX2}) of
Example~\ref{Euler_EX2}.

We have $t_0=1$, $t_f= t_5 =1.5$, $y_0=1$ and $f(t,y) = 2\,ty$.  Hence
$h = (t_5-t_0)/5 = 0.1$ and $t_j = t_0 + hi =1 + 0.1i$.  The
approximation $w_i$ of $y_i = y(t_i)$ is given by
\begin{align*}
& \quad w_0 = 1 \\
&\begin{array}{r@{\,=\,}l}
w_i^\ast &  w_i + 0.2\,t_i w_i \\
w_{i+1} & w_i +  0.1 \left( t_i w_i + t_{i+1}  w_i^\ast \right)
\end{array} \bigg\} \quad, \quad 0 \leq i < 5
\end{align*}
The results of these computations are given in the following table:
\[
\begin{array}{clllll}
i & t_i & w_i & y_i & \text{absolute} & \text{relative} \\
 & & & & \text{error} & \text{error} \\
\hline
0 & 1.00 & 1.00000000 & 1.0000 & 0.0 & 0.0 \\
1 & 1.10 & 1.23200000 & 1.23367806 & 0.00167806 & 0.00136021 \\
2 & 1.20 & 1.54788480 & 1.55270722 & 0.00482242 & 0.00310581 \\
3 & 1.30 & 1.98315006 & 1.99371553 & 0.01056553 & 0.00529942 \\
4 & 1.40 & 2.59078717 & 2.61169647 & 0.02090931 & 0.00800602 \\
5 & 1.50 & 3.45092851 & 3.49034296 & 0.03941445 & 0.01129243 \\
\hline
\end{array}
\]
We get approximation of $y_i$ that are much better than those given by
the Euler's method in Example~\ref{Euler_EX2}.
\label{ModEuler_EX}
\end{egg}

\begin{defn}[Runge-Kutta Method of Order Four]
Let $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for $i=0$,
$1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the
solution of the difference equation
\begin{align*}
w_{i+1} & = w_i + \frac{h}{6}(K_1 + 2\,K_2+2\,K_3 + K_4) \quad, \quad
0 \leq i < N \\
w_0 & = y_0
\end{align*}
where $\displaystyle K_1 = f(t_i,w_i)$,
$\displaystyle K_2 = f(t_i+ h/2,w_i+ h K_1/2)$,
$\displaystyle K_3 = f(t_i+h/2,w_i+ h K_2/2)$ and
$\displaystyle K_4 = f(t_{i+1},w_i+ h K_3)$.
\end{defn}

This method is often called the
{\bfseries classical Runge-Kutta method}\index{Runge-Kutta!Classical Method}.
A graphical interpretation of the Runge-Kutta method classic is given
in Figure~\ref{RKclassic}.  Its Butcher array is
\[
\begin{array}{c|cccc}
0 & & & & \\
1/2 & 1/2 & & & \\
1/2 & 0 & 1/2 & & \\
1 & 0 & 0 & 1 & \\
\hline
& 1/6 & 1/3 & 1/3 & 1/6
\end{array}
\]

\pdfF{init_value_probl/runge_kutta}{Runge-Kutta classic}{The expression
$K_1/6 + 2K_2/3 + 2K_3/3 + K_4/6$ in the formula for the Runge-Kutta
classic is a weighted average of the four slopes shown in the figure
above.}{RKclassic}

\begin{rmkList}
\begin{enumerate}
\item The motivation for the Runge-Kutta methods of order four is as
follows.  We assume that all the mixed partial derivatives of
$f:[t_0,t_f]\times \RR \rightarrow \RR$ exist and are continuous up to
order four.  Up to $O(h^4)$, we replace the function $\phi(t,y)$ in the
Taylor method of order $4$ (Definition~\ref{Tayloralgo}) by an
expression of the form $\gamma_1 K_1 + \gamma_2 K_2 + \gamma_3 K_3 +
\gamma_4 K_4$ where
$K_1=f(t,y)$, $K_2=f(t+\alpha_2 h,y+\beta_{2,1} h K_1)$,
$K_3=f(t+\alpha_3 h,y+\beta_{3,1} h K_1 + \beta_{3,2} h K_2)$ and
$K_4=f(t+\alpha_4 h,y+\beta_{4,1} h K_1 + \beta_{4,2} h K_3 +
\beta_{4,3} h K_3)$ for some $\gamma_j$,
$\alpha_j$ and $\beta_{j,m}$.
After a long computation that can be found in \cite{La}, we find
the following conditions on $\gamma_j$, $\alpha_j$ and $\beta_{j,m}$:
\begin{align*}
& \beta_{2,1} = \alpha_2 \ , \quad
\beta_{3,1} + \beta_{3,2} = \alpha_3 \ , \quad
\beta_{4,1} + \beta_{4,2} + \beta_{4,3} = \alpha_4 \ ,\\
& \sum_{j=1}^4 \gamma_j = 1 \ , \quad
\sum_{j=2}^4 \gamma_j \alpha_j^k = \frac{1}{k+1} \quad \text{for}
\quad k =1,2,3 \ , \\
&\gamma_3 \alpha_2 \beta_{3,2} + \gamma_4 ( \alpha_2 \beta_{4,2} +
\alpha_3 \beta_{4,3}) = \frac{1}{6} \ , \quad
\gamma_3 \alpha_2 \alpha_3 \beta_{3,2} + \gamma_4 \alpha_4 ( \alpha_2
\beta_{4,2} + \alpha_3 \beta_{4,3}) = \frac{1}{8} \ ,\\
&\gamma_3 \alpha_2^2 \beta_{3,2} + \gamma_4 ( \alpha_2^2
\beta_{4,2} + \alpha_3^2 \beta_{4,3}) = \frac{1}{12}
\quad \text{and} \quad
\gamma_4 \alpha_2 \beta_{3,2} \beta_{4,3} = \frac{1}{24} \ .
\end{align*}
The Runge-Kutta of order four given in the definition above
corresponds to a particular choice for these constants.
We will present in the following sections other techniques to develop
Runge-Kutta methods.
\item If we assume that all the mixed partial derivatives of
$f:[t_0,t_f]\times \RR \rightarrow \RR$ exist and are continuous up to
order four, then the local truncation error of the Runge-Kutta method
in the previous definition is $O(h^4)$ as its name suggests.
\item Note that $\displaystyle \sum_{j=1}^s \gamma_j = 1$ is a
necessary condition for the s-stage Runge-Kutta method to be of
order $s$.
\item There is no explicit Runge-Kutta method of order $s$ for
$s\geq 5$ that have at most $s$ stages.  We have the following
relation between the order of an explicit Runge-Kutta method and the
number $s$ of stage.
\[
\begin{array}{l|cccccccccc}
\text{Order} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline
\text{Minimum stage number} & 1 & 2 & 3 & 4 & 6 & 7 & 9 & 11 &
12 \leq s \leq 17 & 13 \leq s \leq 17
\end{array}
\]
\end{enumerate}
\end{rmkList}

\begin{egg}
Use Runge-Kutta classic of order four with $N=5$ to approximate the
solution $y$ of (\ref{ODE_EX2}) in Example~\ref{Euler_EX2}.

We have $t_0=1$, $t_f= t_5 =1.5$, $y_0=1$ and $f(t,y) = 2\,ty$.  Hence
$h = (t_5-t_0)/5 = 0.1$ and $t_i = 1 + 0.1i$ for $i=0$, $1$, \ldots,
$5$.  The approximation $w_i$ of $y_i = y(t_i)$ is given by
\begin{align*}
w_0 & = 1.0 \\
w_{i+1} & = w_i + \frac{0.1}{6}(K_1 + 2\,K_2 + 2\,K_3 + K_4) \quad,
\quad 0 \leq i < 5
\end{align*}
where
\begin{align*}
K_1 &=2\, t_i w_i \\
K_2 &=2(t_i + 1/20)(w_i + K_1/20) \\
K_3 &=2(t_i + 1/20)(w_i + K_2/20) \\
K_4 &=2t_{i+1}(w_i+ K_3/10)
\end{align*}
The results of this computation are given in the following table:
\[
\begin{array}{clllll}
i & t_i & w_i & y_i & \text{absolute} & \text{relative} \\
 & & & & \text{error} & \text{error} \\
\hline
0 & 1.00 & 1.0000000000 & 1.0000000000 & 0.0 & 0.0 \\
1 & 1.10 & 1.2336743500 & 1.2336780600 & 0.000003710 & 0.0000030072 \\
2 & 1.20 & 1.5526953980 & 1.5527072185 & 0.000011820 & 0.0000076128 \\
3 & 1.30 & 1.9936867693 & 1.9937155332 & 0.000028764 & 0.0000144273 \\
4 & 1.40 & 2.6116332332 & 2.6116964734 & 0.000063240 & 0.0000242142 \\
5 & 1.50 & 3.4902106364 & 3.4903429575 & 0.000132321 & 0.0000379106 \\
\hline
\end{array}
\]
We get approximations of $y_i$ that are much better than those given by
the modified Euler's method in Example~\ref{ModEuler_EX} and the
Euler's method in Example~\ref{Euler_EX2}.
\end{egg}

\begin{code}[Runge-Kutta of Order Four]
To approximate the solution of the initial value problem
\[
\begin{split}
y'(t) &= f(t,y(t)) \quad, \quad t \geq t_0 \\
y(0) &= y_0
\end{split}
\]
\subI{Input} The function $f(t,y)$ (funct in the code below). \\
The step-size $h$.\\
The number of steps $N$.\\
The initial time $t_0$ (t0 in the code below) and the initial
conditions $y_0$ (y0 in the code below) at $t_0$.\\
\subI{Output} The approximations $w_i$ (ww(i+1) in the code below)
of $y(t_i)$ at $t_i$ (tt(i+1) in the code below).
\small
\begin{verbatim}
function [tt,ww] = rgkt4(funct,h,N,t0,y0)
  tt(1) = t0;
  ww(1) = y0;
  h2 = h/2;
  for j=1:N
    tt(j+1) = tt(1)+j*h;
    k1 = h*funct(tt(j),ww(j));
    k2 = h*funct(tt(j)+h2,ww(j)+k1/2);
    k3 = h*funct(tt(j)+h2,ww(j)+k2/2);
    k4 = h*funct(tt(j+1),ww(j)+k3);
    ww(j+1) = ww(j) + (k1+2*(k2+k3)+k4)/6;
  end
end
\end{verbatim}
\label{rgkt4}
\end{code}

\subsection{Derivation of Runge-Kutta Methods -- Collocation
Method}\label{DeRKMCollMeth}

We present a method to derive some Runge-Kutta methods in this
section.  A more general method will be presented in the next section.

As usual, we consider the initial value problem (\ref{ODE})
and assume that we have a partition $t_0 < t_1 < \ldots < t_N = t_f$
of $[t_0,t_f]$ such that $t_{i+1} - t_i = h$ for $i=0$, $1$, \ldots,
$N-1$.

\begin{defn}[Collocation Method]
We consider $k$ distinct nodes $\alpha_1 < \alpha_2 < \ldots < \alpha_k$
in $[0,1]$.  Assuming that we have $w_i \approx y(t_i)$, we seek a
polynomial $p$ of degree $k$ such that
\[
\begin{split}
p(t_i) &= w_i \\
p'(t_i+\alpha_j h) &= f(t_i+\alpha_j h,p(t_i+\alpha_j h)) \quad ,
\quad 1 \leq j \leq k\ .
\end{split}
\]
The approximation $w_{i+1}$ of $y(t_{i+1})$ is given by $p(t_{i+1})$.
We repeat this construction for $i=0$, $1$, \ldots, $N-1$.
\label{RK_coll}
\end{defn}

The idea behind the collocation method is to use a polynomial of
degree $k$ on each interval $[t_i,t_{i+1}]$ to approximate the solution
between $t_i$ and $t_{i+1}$.

\begin{theorem}
Let
\[
\ell_m(t) = \prod_{\substack{j = 1 \\ j \neq m}}^k\,
\frac{t-\alpha_j}{\alpha_m-\alpha_j}
\]
for $1 \leq m \leq k$ and
\[
\beta_{j,m} = \int_0^{\alpha_j}\,\ell_m(t)\dx{t}
\quad \text{and} \quad
\gamma_j = \int_0^1\,\ell_j(t)\dx{t}
\]
for $1 \leq j,m \leq k$.  Then, the collocation method presented
in Definition~\ref{RK_coll} is an implicit Runge-Kutta method with
Butcher array
\[
\begin{array}{c|cccc}
\alpha_1 & \beta_{1,1} & \beta_{1,2} & \ldots & \beta_{1,k} \\
\alpha_2 & \beta_{2,1} & \beta_{2,2} & \ldots & \beta_{2,k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\alpha_k & \beta_{k,1} & \beta_{k,2} & \ldots & \beta_{k,k} \\
\hline
 & \gamma_1 & \gamma_2 & \ldots & \gamma_k
\end{array}
\]
\label{entries_Butcher}
\end{theorem}

\begin{proof}
Suppose that $p$ is the polynomial given in Definition~\ref{RK_coll}.
Let
\[
  q(t) = \sum_{m=1}^k\,p'(t_i+\alpha_m h)\,
  \ell_m\left( \frac{t-t_i}{h}\right) \ .
\]
$q$ and $p'$ are two polynomials of degree $k-1$ that coincide
at the $k$ points $t_i+\alpha_j h$ for $1 \leq j \leq k$; namely,
$q(t_i+\alpha_j h) = p'(t_i+\alpha_j h)$ for $1 \leq j \leq k$.
Therefore $q(t) = p'(t)$ for all $t \in [t_i, t_{i+1}]$.  Hence
\[
p'(t) = \sum_{m=1}^k\,p'(t_i+\alpha_m h)\,\ell_m\left( \frac{t-t_i}{h}\right)
= \sum_{m=1}^k\,f(t_i+\alpha_m h,p(t_i+\alpha_m h))\,
\ell_m\left(\frac{t-t_i}{h} \right)
\]
and
\begin{align}
p(t) &= p(t_i) + \int_{t_i}^t\,p'(s)\dx{s}
= w_i + \sum_{m=1}^k\left( f(t_i+\alpha_m h,p(t_i+\alpha_m h))\,
\int_{t_i}^t\,\ell_m\left(\frac{s-t_i}{h}\right)\,\dx{s} \right)
\nonumber \\
&= w_i + h\,\sum_{m=1}^k\left( f(t_i+\alpha_m h,p(t_i+\alpha_m h))\,
\int_{0}^{(t-t_i)/h}\,\ell_m(s)\dx{s} \right) \ . \label{RK_coll_equ}
\end{align}

Let $K_m = f(t_i+\alpha_m h,p(t_i+\alpha_m h))$ for $1 \leq m \leq k$.
If we substitute $t=t_i+\alpha_j h$ in (\ref{RK_coll_equ}), we get
\begin{equation}\label{polColRK}
p(t_i+\alpha_jh) = w_i + h\,\sum_{m=1}^k\, \beta_{j,m}\,K_m
\end{equation}
for $1 \leq j \leq k$.   Thus,
\begin{equation} \label{coll_Ks}
  K_j = f\left(t_i+\alpha_j h, w_i + h\,\sum_{m=1}^k\, \beta_{j,m}\,K_m\right)
\end{equation}
for $1 \leq j \leq k$.

If we now substitute $t=t_{i+1}$ in (\ref{RK_coll_equ}), we get
\begin{equation} \label{coll_bs}
w_{i+1} = w_i + h\,\sum_{j=1}^k\, \gamma_j\,K_j \ .
\end{equation}
(\ref{coll_Ks}) and (\ref{coll_bs}) define the expected implicit
Runge-Kutta method.
\end{proof}

\begin{rmkList}\label{RKcolInterpol}
\begin{enumerate}
\item Not all Runge-Kutta methods comes from collocation methods.  For
instance
\[
\begin{array}{c|cc}
0 & 0 & 0 \\
2/3 & 1/3 & 1/3 \\
\hline
 & 1/4 & 3/4
\end{array}
\quad\text{and} \quad
\begin{array}{c|cc}
0 & 1/4 & -1/4 \\
2/3 & 1/4 & 5/12 \\
\hline
 & 1/4 & 3/4
\end{array}
\]
are two Runge-Kutta methods but there is a unique collocation method
associated to the nodes $\alpha_1 =0$ and $\alpha_2 = 2/3$.
\item The choice of $\beta_{j,m}$ for $1 \leq m,j\leq k$ is such that
\begin{equation}\label{calpolcondA}
\int_0^{\alpha_j} q(t) \dx{t} = \sum_{m=1}^k \beta_{j,m}\, q(\alpha_m)
\end{equation}
is true for all polynomials $q$ of degree less than $k$ because all
polynomials of degree less than $k$ can be written as
\[
  q(t) = \sum_{m=1}^k q(\alpha_m)\, \ell_m(t)
\]
since we assume that the $\alpha_m$ are distinct.  Similarly, we have
\begin{equation}\label{calpolcondB}
\int_0^1 q(t) \dx{t} = \sum_{j=1}^k \gamma_j\, q(\alpha_j)
\end{equation}
is true for polynomial $q$ of degree less than $k$.
Question~\ref{initQ12} expands on this subject.  In particular, the
$\beta_{i,m}$ and $\gamma_m$ are uniquely determined by
(\ref{calpolcondA}) and (\ref{calpolcondB}).
\end{enumerate}
\end{rmkList}

We state the next proposition in the context of an initial value
problem in $\RR^n$; namely, $f:[t_0,t_f]\times \RR^n \to \RR^n$ with
$n>1$.  The statement of this proposition is more interesting in this
context despite the fact that we will only use it for $n=1$. 

\begin{prop}[Alekseev-Gr\"{o}bner Lemma]
Let $\VEC{y}:[a,b] \rightarrow \RR^n$ be the solution of
\begin{align*}
\VEC{y}'(t) &= f(t,\VEC{y}(t)) \quad, \quad t_0 \leq t \leq t_f \\
\VEC{y}(t_0) & = \VEC{y}_0
\end{align*}
where the function $f:[t_0,t_f]\times \RR^n \to \RR^n$ is continuously
differentiable.  Suppose that $\VEC{v}:[t_0,t_f]\rightarrow \RR^n$ is
a continuously differentiable function and $\VEC{v}(t_0) = \VEC{y}_0$.
Then $\VEC{v}$ satisfies
\[
\VEC{v}(t) = \VEC{y}(t) + \int_{t_0}^t\,A(t,s,\VEC{v}(s))\,
\left( \VEC{v}'(s) - f(s,\VEC{v}(s)) \right) \dx{s}
\]
for $t_0 \leq t \leq t_f$, where $A$ is the Jacobian matrix
with respect to $\VEC{w}$ of the solution
$\VEC{u} = \VEC{u}(t,s,\VEC{w})$ of
\begin{align*}
\VEC{u}'(t) &= f(t,\VEC{u}(t)) \quad , \quad t \geq s \\
\VEC{u}(s) &= \VEC{w}
\end{align*}
for every $s \geq t_0$.
\end{prop}

We will not prove this proposition.  However, we will illustrate it
for $n=1$.

\begin{egg}
Consider the initial value problem
\begin{align*}
y'(t) &= a y(t) \quad , \quad t_0 \leq t \leq t_f\\
y(t_0) &= y_0
\end{align*}
Its solution is $y(t) = e^{a(t-t_0)}y_0$ for $t_0 \leq t \leq t_f$.

Suppose that $v:[t_0,t_f]\rightarrow \RR$ is a continuously
differentiable function such that $v(t_0) = y_0$.  Consider the
initial value problem
\begin{align*}
u'(t) &= a u(t) \quad , \quad t \geq s\\
u(s) &= w
\end{align*}
Its solution is $u(t) = e^{a(t-s)}w$ for $t \geq s$.

According to Alekseev-Gr\"{o}bner Lemma, we have
\begin{equation}\label{AlekGrobegg}
v(t) = e^{a(t-t_0)}y_0 + \int_{t_0}^t\,e^{a(t-s)}\,
\left(v'(s) - a v(s)\right) \dx{s}
\end{equation}
for $t_0 \leq t \leq t_f$.  This is a well known result because $v(t)$,
the solution of
\begin{align*}
v'(t) & = a v(t) + g(t) \quad , \quad t_0 \leq t \leq t_f \\
v(t_0) &= y_0
\end{align*}
where $g(t) = v'(t) - av(t)$ for $t_0 \leq t \leq t_f$, is given by
(\ref{AlekGrobegg}).
\end{egg}

\begin{theorem}
Let $\displaystyle q(t) = \prod_{j=1}^k\,(t-\alpha_j)$, where the
$\alpha_j$ are the nodes given in Definition~\ref{RK_coll}.  Suppose
that $m$ is the largest integer such that $0 < m \leq k$ and
\begin{equation} \label{coll_ord_cond} 
\int_0^1 \, q(t)\,t^j\dx{t}  = 0
\end{equation}
for $0 \leq j < m$.  Then, the collocation method
in Definition~\ref{RK_coll} is of order $k+m$
(Definition~\ref{localTruncErrorDef}) if we assume that $f$ is
sufficiently continuously differentiable.
\label{RKcollOrder}
\end{theorem}

\begin{proof}
From Alekseev-Gr\"{o}bner lemma with $t_0$ replaced by $t_i$, $t$ by
$t_{i+1}$ and $v(t)$ by the collocation polynomial $p(t)$ in
Definition~\ref{RK_coll}, we get
\begin{equation}\label{RKcollOrderEq1}
w_{i+1} - y(t_{i+1}) = \int_{t_i}^{t_{i+1}} g(s) \dx{s}
= h\,\int_0^1 g(t_i+s h) \dx{s} \ ,
\end{equation}
where
\[
 g(s) = A(t_{i+1},s,p(s))\,\left( p'(s) - f(s,p(s)) \right) \ .
\]

As we have seen at the beginning of Section~\ref{GaussGuadrSect}, if
we use the nodes $\alpha_j$ and the weight $\gamma_j$ for $1\leq j \leq k$
given in Theorem~\ref{entries_Butcher}, we get from
(\ref{RKcollOrderEq1}) that
\[
w_{i+1} - y(t_{i+1}) = h\, \sum_{j=1}^k\,\gamma_j\, g(t_i+\alpha_j h) +
h\,E_i \ ,
\]
where $E_i$ is the discretization error of the quadrature formula.
However, in the Definition~\ref{RK_coll} of the collocation method, we
have that
\[
p'(t_i+\alpha_j h) - f(t_i+\alpha_j h,p(t_i+\alpha_j h)) =0
\]
for $1 \leq j \leq k$.  Thus $w_{i+1} - y(t_{i+1}) = h\,E_i$.
From (\ref{coll_ord_cond}) and Question~\ref{diffQ25}, we have
that the quadrature formula is exact for polynomials of degree less
than $k+m$.  It follows from Question~\ref{diffQ26}
that there exist a constant $K = K(k,m,\gamma_j)$ such that
\begin{align*}
|E_i| &= \bigg| \int_0^1 g(t_i+s h) \dx{s} -
\sum_{j=1}^k\,\gamma_j\, g(t_i+\alpha_j h) \bigg|
\leq K \max_{0\leq s \leq 1}\, \left| \dfdxn{g(t_i + s h)}{s}{k+m}\right| \\
& = K h^{k+m}\,\max_{0\leq s \leq 1}\, \left| g^{(k+m)}(t_i + s h) \right| 
\leq K h^{k+m}\,\max_{a\leq t \leq b}\, \left| g^{(k+m)}(t) \right|
\end{align*}
for $0\leq i <N$.  Let
$\displaystyle \phi(t_i,w_i) = \sum_{j=1}^k\gamma_j K_j$ be the
formula for the Runge-Kutta method provided by the collocation method.
Then
\begin{align*}
\tau_{i+1}(h) &= \frac{y(t_{i+1}) - y(t_i)}{h} - \phi(t_i,y(t_i))
= \frac{(w_{i+1} - h E_i) - (w_i - h E_{i-1})}{h} -
\phi(t_i,w_i - h E_{i-1}) \\
&= \underbrace{\frac{w_{i+1} - w_i}{h} - \phi(t_i,w_i)}_{=0}
- E_i + E_{i-1} - h \pdydx{\phi}{w}(t_i,\xi_i)E_{i-1}
\end{align*}
for some $\xi_i$ between $w_i$ and $w_i - E_{i-1}$ is we assume that
$|h|\leq 1$.

Since $E_i = O(h^{k+m})$ near the origin for all $i$, we may assume that there
exists an interval $[c,d]$ such that $w_i$ and $w_i-E_i$ are in $[c,d]$
for all $i$ and all small $h$ (i.e.\ for all partitions
$a \leq t_0 < t_1 < \dots < t_N = b$ with $t_{i+1}-t_i=h$ small
enough).  Since $\phi$ is composed of $f$ and some of its partial
derivatives, there exists a constant $M$ such that
$\left|\displaystyle \pdydx{\phi}{w}(t,w)\right| \leq M$ for all
$(t,w) \in [a,b]\times [c,d]$.
Hence
\[
|\tau_{i+1}(h)| \leq \tau(h) = \left( 2 + |h| M\right) K
|h|^{k+m}\,\max_{a\leq t \leq b}\, \left| g^{(k+m)}(t) \right| 
= O(h^{k+m})
\]
near the origin.  Proving that the collocation method is of
order at least $k+m$.

To prove that the collocation method is not of order greater than
$k+m$, it suffices to apply the collocation method to
\begin{align*}
y'(t) &= (k+m+1)t^{k+m} \\
y(0) &= 0  \qedhere
\end{align*}
\end{proof}

\begin{cor}
Let $\displaystyle q(t) = \prod_{j=1}^k\,(t-\alpha_j)$, where the
$\alpha_j$ are the nodes used in the collocation method given in
Definition~\ref{RK_coll}.  Suppose that
\[ 
\int_0^1 \, q(t)\,t^i\,\dx{t} = 0
\]
for $i=0$, $1$, \ldots, $k-1$.  Then, the collocation method is of
order $2k$.
\label{best_IRK}
\end{cor}

\begin{egg}[Gauss-Legendre Methods]
Suppose that $q(t) = t- 1/2$, the Gauss-Legendre polynomial of degree
$1$, and $\alpha_1 = 1/2$.  The previous Corollary is satisfied with
$k=1$.   From Theorem~\ref{entries_Butcher}, we get the Runge-Kutta
method of order two associated to the Butcher array\footnote{This is
the case $k=1$ in Theorem~\ref{entries_Butcher}.  We then have that
$\ell_1(t) = 1$ for all $t$ by definition.  The empty product is
defined to be $1$, the neutral element for the multiplication, as the
empty sum is defined to be $0$, the neutral element for the addition.}
\[
\begin{array}{c|c}
1/2 & 1/2 \\
\hline
 & 1
\end{array}
\]
This is the Implicit Midpoint Rule.

Suppose that $q(t) = t^2 -t + 1/6$, the Gauss-Legendre polynomial of
degree $2$.  $q(t) = (t-\alpha_1)(t-\alpha_2)$, where
$\alpha_1 = (3- \sqrt{3})/6$ and $\alpha_2 = (3 + \sqrt{3})/6$.  The
previous Corollary is satisfied with $k=2$.  From
Theorem~\ref{entries_Butcher}, we get the Runge-Kutta method of
order four associated to the Butcher array
\[
\begin{array}{c|cc}
(3-\sqrt{3})/6 & 1/4 & (3-2 \sqrt{3})/12 \\
(3+\sqrt{3})/6 & (3 + 2 \sqrt{3})/12 & 1/4 \\
\hline
 & 1/2 & 1/2
\end{array}
\]
\label{Gauss_Leg_RK}
\end{egg}

\subsection{Derivation of Runge-Kutta Methods -- Rooted Trees}

In this section, we will use trees to derive Runge-Kutta methods.
None of the results from graph theory will be proved.  This could be
the subject for another book.   Moreover, we will consider initial
value problems like (\ref{ODE}) where
$f:[t_o,t_f]\times \RR^n \to \RR^n$ with $n>1$.  Namely, we will
consider the initial value problem

\begin{equation} \label{ODEndim}
\begin{split}
\dydx{\VEC{y}}{t}(t) &= f(t,\VEC{y}(t)) \quad,
\quad t_0 \leq t \leq t_f \\
\VEC{y}(t_0) & = \VEC{y}_0 \in \RR^n
\end{split}
\end{equation}
where $f:[t_0,t_f]\times\RR^n \rightarrow \RR^n$ and $n>1$.

It is true that most of what we have said for initial value problems
with $f:[t_0,t_f]\times \RR \to \RR$ is also true for initial value
problems with $f:[t_0,t_f]\times \RR^n \to \RR^n$ and $n>1$.  But there
are some differences.  One property that is influenced by the
dimension of the space is the order of the method.  As we will show
later in this section, some Runge-Kutta methods do not have the same
order in $\RR$ than in $\RR^n$ with $n>1$.

Some good references on the subject of this section are
\cite{B,I,La,Lb}.  The proof of many of the results stated in this
section can be found in those references.  They also include good
references to the publications on the rooted tree approach. 

\subsubsection{Elementary differentials}

For simplicity and without too much lost of generality, we assume that
$f$ in (\ref{ODEndim}) is independent of $t$.

Let $\LL^{m}(\RR^n,\RR^n)$ be the space of multilinear mappings from
$\RR^n \times \RR^n \times \cdots \times \RR^n$ ($m$ times) to $\RR^n$.

\begin{defn}
The {\bfseries Frechet derivative}\index{Frechet Derivative}
of degree $n$ of $f:\RR^n \rightarrow \RR^n$ is the mapping
$\fdiff{f}{m} : \RR^n \to \LL^{m}(\RR^n,\RR^n)$ defined by
\begin{align*}
&\fdiff{f}{m}(\VEC{y})(\VEC{k}_1,\VEC{k}_2,\ldots,\VEC{k}_m) \\
& \qquad = \sum_{i=1}^n \left( \sum_{j_1=1}^n \, \sum_{j_2=1}^n \ldots
\sum_{j_m=1}^n \pdydxdots{f}{y_{j_1}}{y_{j_2}}{y_{j_m}}{m}{}{}{}
(\VEC{y}) \, k_{1,j_1}\,k_{2,j_2}\ldots k_{m,j_m}  \right) \VEC{e}_i\ ,
\end{align*}
where $\VEC{k}_i = \begin{pmatrix}
k_{i,1} & k_{i,2} & \ldots&\ k_{1,n} \end{pmatrix}^\top$.
\end{defn}

\begin{egg}
If $f:\RR^2 \rightarrow \RR^2$,
\[
\fdiff{f}{2}(\VEC{y})(\VEC{k}_1, \VEC{k_2}) =
\begin{pmatrix}
\displaystyle
\sum_{j_1=1}^2 \sum_{j_2=1}^2 \pdydxnm{f_1}{y_{j_1}}{y_{j_2}}{2}{}{}(\VEC{y})
\, k_{1,j_1}\,k_{2,j_2} \\[1em]
\displaystyle
\sum_{j_1=1}^2 \sum_{j_2=1}^2 \pdydxnm{f_2}{y_{j_1}}{y_{j_2}}{2}{}{}(\VEC{y})
\, k_{1,j_1}\,k_{2,j_2}
\end{pmatrix} \ .
\]
\end{egg}

\begin{defn}
The
{\bfseries elementary differentials}\index{Runge-Kutta!Elementary Differentials}
of $f:\RR^n \rightarrow \RR^n$ and their
{\bfseries order}\index{Runge-Kutta!Order of Elementary Differentials}
are defined recursively.
\begin{enumerate}
\item $f:\RR^n \rightarrow \RR^n$ is the only elementary
differential of order $1$.
\item if $g_1:\RR^n\rightarrow \RR^n$,
$g_2:\RR^n\rightarrow \RR^n$, \ldots,
$g_r:\RR^n\rightarrow \RR^n$
are $r$ elementary differentials of $f$ of order $m_1$, $m_2$,
\ldots, $m_r$ respectively, then 
$\fdiff{f}{r}(\cdot)(g_1(\cdot),g_2(\cdot), \ldots,g_r(\cdot))
:\RR^n\rightarrow \RR^n$,
defined by
\begin{align*}
&\fdiff{f}{r}(\VEC{y})\big(g_1(\VEC{y}),g_2(\VEC{y}),\ldots,g_r(\VEC{y})\big) \\
& \qquad = \sum_{i=1}^n \left( \sum_{j_1=1}^n \, \sum_{j_2=1}^n \ldots
\sum_{j_r=1}^n \pdydxdots{f}{y_{j_1}}{y_{j_2}}{y_{j_r}}{r}{}{}{}
(\VEC{y}) \, g_{1,j_1}(\VEC{y})\,g_{2,j_2}(\VEC{y}), \ldots
g_{r,j_r}(\VEC{y})  \right) \VEC{e}_i
\end{align*}
for $\VEC{y} \in \RR^n$, is an elementary differential of order
$\displaystyle 1 +\sum_{i=1}^r\,m_i$ of $\VEC{f}$.
\end{enumerate}
For simplicity, $\fdiff{f}{r}(\cdot)(g_1(\cdot),g_2(\cdot),\ldots,g_r(\cdot))$
is denoted $\{g_1\ g_2\ \ldots\ g_r\}$.
\end{defn}

The order of an elementary differential is not related to the degree
of the Frechet derivatives of $f$ that are used in the definition of the
elementary differential.

\begin{egg}
The elementary differential of $f:\RR^n \to \RR^n$ of order $2$ is
$\{f\} = \fdiff{f}{}(\cdot) (f(\cdot))$ defined by
\[
\fdiff{f}{}(\VEC{y})( f(\VEC{y})) =
\sum_{i=1}^n \left(\sum_{j=1}^n\,
\pdydx{f_i}{y_j}(\VEC{y}) f_j(\VEC{y}) \right) \VEC{e}_i \ .
\]
This is $\VEC{y}''(t)$ if $\VEC{y}'(t) = f(\VEC{y}(t))$.  This is the
motivation for the definition of the order of an elementary
differential.  Only partial derivative of order $1$ of $f$ are used
but it is associated to the second order derivative of $\VEC{y}$ when
$\VEC{y}'(t) = f(\VEC{y}(t))$.  Note that $\fdiff{f}{} \equiv \fdiff{f}{1}$.

Here are two elementary differentials of $f:\RR^n \to \RR^n$ of order
$2$:\\
$\{\{f\}\} = \fdiff{f}{}(\cdot)\big( \fdiff{f}{}(\cdot) (f(\cdot)) \big)$
defined by
\[
\fdiff{f}{}(\VEC{y})\big( \fdiff{f}{}(\VEC{y}) (f(\VEC{y})) \big)
= \sum_{i=1}^n \left(  \sum_{k=1}^n\,
\pdydx{f_i}{y_k}(\VEC{y}) \left(\sum_{j=1}^n\,
\pdydx{f_k}{y_j}(\VEC{y}) f_j(\VEC{y}) \right) \right) \VEC{e}_i
\]
for $\VEC{y} \in \RR^n$, and
$\{f \ f \} = \fdiff{f}{2}(\cdot)\big( f(\cdot) , f(\cdot) \big)$
defined by
\[
\fdiff{f}{2}(\VEC{y})\big( f(\VEC{y}), f(\VEC{y}) \big)
= \sum_{i=1}^n \left(  \sum_{j_1=1}^n\,\sum_{j_2=1}^n\,
\pdydxnm{f_i}{y_{j_1}}{y_{j_1}}{2}{}{}(\VEC{y}) f_{j_1}(\VEC{y})\,
f_{j_2}(\VEC{y}) \right) \VEC{e}_i
\]
for $\VEC{y} \in \RR^n$.

From now on, we will ignore the dependent variable $\VEC{y}$ and write
$\{\{f\}\} = \fdiff{f}{}\,(\fdiff{f}{}\,(f))$ and
$\{f \ f\} = \fdiff{f}{2}\,(f,f)$ to simplify the notation.
\end{egg}

\subsubsection{Rooted Trees}\label{TreeSSSection}

In this section, we briefly introduce some concepts about rooted trees
without giving any proof.  We only introduce the concepts that will
provide the tools to compute elementary differentials.  The proofs of
the results mentioned in this section can be found in \cite{La}.

The easiest way to define the {\bfseries rooted trees}\index{Rooted Tree}
is to give some examples of them. 

A rooted tree of order 1:
\begin{picture}(2,0)
\put(1,0){\circle*{0.1}}
\end{picture}

A rooted tree of order 2:
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(1,0){\line(0,1){1}}
\put(1,1){\circle*{0.1}}
\end{picture}

A rooted tree of order 7:
\begin{picture}(3,3)
\put(1.5,0){\circle*{0.1}}
\put(1,1){\circle*{0.1}}
\put(2,1){\circle*{0.1}}
\put(2.5,2){\circle*{0.1}}
\put(1.5,2){\circle*{0.1}}
\put(1,3){\circle*{0.1}}
\put(2,3){\circle*{0.1}}
\put(1.5,0){\line(-1,2){0.52}}
\put(1.5,0){\line(1,2){0.52}}
\put(2,1){\line(-1,2){0.52}}
\put(2,1){\line(1,2){0.52}}
\put(1.5,2){\line(-1,2){0.52}}
\put(1.5,2){\line(1,2){0.52}}
\end{picture}

We can combine rooted trees together to form another rooted tree.\\
Let $\tau_1 =$
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\end{picture} ,
$\tau_2 =$
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(1,0){\line(1,2){0.52}}
\put(1.5,1){\line(-1,2){0.52}}
\end{picture}
and $\tau_3 =$
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(1,1){\circle*{0.1}}
\put(0.5,2){\circle*{0.1}}
\put(1.5,2){\circle*{0.1}}
\put(1,0){\line(0,1){1}}
\put(1,1){\line(-1,2){0.52}}
\put(1,1){\line(1,2){0.52}}
\end{picture} .\\
The new rooted tree $[\tau_1\ \tau_2\ \tau_3]$ is defined by
\begin{picture}(4,3)
\put(2,0){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(2,2){\circle*{0.1}}
\put(2,1){\circle*{0.1}}
\put(2.5,2){\circle*{0.1}}
\put(2,3){\circle*{0.1}}
\put(2.5,1){\circle*{0.1}}
\put(3,2){\circle*{0.1}}
\put(2.5,3){\circle*{0.1}}
\put(3.5,3){\circle*{0.1}}
\put(2,0){\color{blue}\line(0,1){1}}
\put(2,0){\color{blue}\line(-1,2){0.52}}
\put(2,0){\color{blue}\line(1,2){0.52}}
\put(1.5,1){\line(-1,2){0.52}}
\put(1.5,1){\line(1,2){0.52}}
\put(2,1){\line(1,2){0.52}}
\put(2.5,2){\line(-1,2){0.52}}
\put(2.5,1){\line(1,2){0.52}}
\put(3,2){\line(-1,2){0.52}}
\put(3,2){\line(1,2){0.52}}
\end{picture} .   The three rooted trees were combined using the
rooted tree in blue.

\begin{rmk}
It is interesting to know that if $a_i$ is the number of rooted trees
of order $i$, then
\[
a_1 + a_2 u + a_2 u^2 + a_3 u^3+ \ldots =
(1-u)^{-a_1}\,(1-u^2)^{-a_2}\,(1-u^3)^{-a_3} \ldots
\]
\label{CombTree}
\end{rmk}

\begin{defn}
Let $\tau$ be a rooted tree.  We define the following values associated to
the rooted tree $\tau$.
\begin{enumerate}
\item $r(\tau)$ is the {\bfseries order}\index{Rooted Tree!Order} of $\tau$.
\item $\sigma(\tau)$ is the {\bfseries symmetry}\index{Rooted
Tree!Symmetry} of $\tau$. 
\item $\gamma(\tau)$ is the {\bfseries density}\index{Rooted
Tree!Density} of $\tau$. 
\item $\alpha(\tau)$ is the number of ``distinct ways of
numbering the nodes'' of $\tau$ such that the numbers increase along the
branches if we start from the root.
\end{enumerate}
The order, symmetry and density are defined recursively.

If $\tau$ is a rooted tree of order one, then
$r(\tau) = \sigma(\tau) = \gamma(\tau) = 1$.

If
\[
\tau = [\underbrace{\tau_1\ \tau_1\ \ldots\ \tau_1}_{\text{$n_1$ times}}
\,\underbrace{\tau_2\ \tau_2\ \ldots\ \tau_2}_{\text{$n_2$ times}}
\ldots
\underbrace{\tau_q\ \tau_q\ \ldots\ \tau_q}_{\text{$n_q$ times}}] \ ,
\]
then
\begin{align*}
r(\tau) &= 1 + n_1 \,r(\tau_1) + n_2 \,r(\tau_2) + \ldots +
n_q \,r(\tau_q) \\
\sigma(\tau) &= n_1!\,n_2!\dots n_q!\,(\sigma(\tau_1))^{n_1}
(\sigma(\tau_2))^{n_2}\ldots (\sigma(\tau_q))^{n_q} \\
\gamma(\tau) &= r(\tau)\,(\gamma(\tau_1))^{n_1} \,
(\gamma(\tau_2))^{n_2}\ldots (\gamma(\tau_q))^{n_q}
\end{align*}
\end{defn}

\begin{rmk}
We explain the expression ``distinct ways of numbering the nodes'' 
of a rooted tree with the help of some examples.  The following
numberings are not considered to be distinct:
\begin{picture}(2,1.5)
\put(1,0){\circle*{0.1}}
\put(1.1,-0.2){1}
\put(1,1){\circle*{0.1}}
\put(1.1,1){2}
\put(0.5,1){\circle*{0.1}}
\put(0.2,1){3}
\put(1.5,1){\circle*{0.1}}
\put(1.6,1){4}
\put(1,0){\line(0,1){1}}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\end{picture}
and
\begin{picture}(2,1.5)
\put(1,0){\circle*{0.1}}
\put(1.1,-0.2){1}
\put(1,1){\circle*{0.1}}
\put(1.1,1){3}
\put(0.5,1){\circle*{0.1}}
\put(0.2,1){4}
\put(1.5,1){\circle*{0.1}}
\put(1.6,1){2}
\put(1,0){\line(0,1){1}}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\end{picture}.
However, the following numberings are distinct:
\begin{picture}(2,2.5)
\put(1,0){\circle*{0.1}}
\put(1.2,-0.2){1}
\put(0.5,1){\circle*{0.1}}
\put(0.2,1){2}
\put(1.5,1){\circle*{0.1}}
\put(1.6,1){3}
\put(1,2){\circle*{0.1}}
\put(1.1,2){4}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\put(1.5,1){\line(-1,2){0.52}}
\end{picture}
and
\begin{picture}(2,2.5)
\put(1,0){\circle*{0.1}}
\put(1.2,-0.2){1}
\put(0.5,1){\circle*{0.1}}
\put(0.2,1){4}
\put(1.5,1){\circle*{0.1}}
\put(1.6,1){2}
\put(1,2){\circle*{0.1}}
\put(1.1,2){3}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\put(1.5,1){\line(-1,2){0.52}}
\end{picture}
\end{rmk}

The number of distinct ways of numbering the nodes of a rooted tree
$\tau$ is given by the following theorem.

\begin{theorem}
If $\tau$ is a rooted tree, then
\[
\alpha(\tau) = \frac{r(\tau)!}{\sigma(\tau)\gamma(\tau)} \ .
\]
\end{theorem}

We give in Table~\ref{OSDA} the order, symmetry, density and
number of distinct ways of numbering the nodes for some of
the basic rooted trees.

\begin{longtable}{cccccc}
rooted tree & name & order & symmetry & density & numbering \\
\hline
& & & & & \\
\endfirsthead
rooted tree & name & order & symmetry & density & numbering \\
\hline
& & & & & \\
\endhead
\endfoot
& & & & & \\
\caption{The order, symmetry, density and number of distinct ways of
numbering the rooted trees of order $1$ to $4$ inclusively. \label{OSDA}}
\endlastfoot
\begin{picture}(2,0)
\put(1,0){\circle*{0.1}}
\end{picture}
 & $\tau$ & 1 & 1 & 1 & 1 \\[1em]
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(1,0){\line(0,1){1}}
\put(1,1){\circle*{0.1}}
\end{picture}
 & $[\tau]$ & 2 & 1 & 2 & 1 \\[1em]
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\end{picture}
 & $[\tau\ \tau]$ & 3 & 2 & 3 & 1 \\[1em]
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(0.5,1){\line(1,2){0.52}}
\end{picture}
 & $[[\tau]]$ & 3 & 1 & 6 & 1 \\[1em]
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(1,1){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,0){\line(0,1){1}}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\end{picture}
 & $[\tau\ \tau\,\tau]$ & 4 & 6 & 4 & 1 \\[1em]
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\put(1.5,1){\line(-1,2){0.52}}
\end{picture}
 & $[\tau\ [\tau]]$ & 4 & 1 & 8 & 3 \\[1em]
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(1,1){\circle*{0.1}}
\put(1.5,2){\circle*{0.1}}
\put(0.5,2){\circle*{0.1}}
\put(1,0){\line(0,1){1}}
\put(1,1){\line(-1,2){0.52}}
\put(1,1){\line(1,2){0.52}}
\end{picture}
 & $[[\tau\ \tau]]$ & 4 & 2 & 12 & 1 \\[1em]
\begin{picture}(2,3)
\put(1,0){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(1.5,3){\circle*{0.1}}
\put(1,0){\line(1,2){0.52}}
\put(1.5,1){\line(-1,2){0.52}}
\put(1,2){\line(1,2){0.52}}
\end{picture}
 & $[[[\tau]]]$ & 4 & 1 & 24 & 1
\end{longtable}

\subsubsection{Relation Between Elementary Differentials and Rooted
  Trees}

We define a mapping $F$ which associates to each rooted tree $\tau$ an
elementary differential $F(\tau)$ of a function $f$.  The easiest way
to explain how $F$ associates elementary differentials to rooted trees
is to give some examples.

\begin{longtable}{cc}
rooted tree & elementary differential \\
$\tau$ & $F(\tau)$ \\
\hline
& \\
\endfirsthead
$\tau$ & $F(\tau)$ \\
\hline
& \\
\endhead
\endfoot
\endlastfoot
\begin{picture}(2,0)
\put(1,0){\circle*{0.1}}
\end{picture}
&
$f$ \\[1em]
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(1,0){\line(0,1){1}}
\put(1,1){\circle*{0.1}}
\end{picture}
&
$\{f\} = \fdiff{f}{}\,(f)$ \\[1em]
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,0){\line(1,2){0.52}}
\put(1,0){\line(-1,2){0.52}}
\end{picture}
&
$\{f\ f\} = \fdiff{f}{2}\, (f,f)$ \\[1em]
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(0.5,1){\line(1,2){0.52}}
\end{picture}
&
$\{\{f\}\} = \fdiff{f}{}\,(\fdiff{f}{}\,(f))$ \\[1em]
\vdots & \vdots
\end{longtable}

The following proposition follows easily from the definitions.

\begin{prop}
\begin{enumerate}
\item If $\tau$ is the rooted tree associated to the elementary
differential $g$ of $f$ (i.e.\ $g = F(\tau)$), then $g$ and $\tau$
have the same order.
\item If $g_1$, $g_2$, \ldots, $g_s$ are elementary differentials of
$f$ associated to the rooted trees $\tau_1$, $\tau_2$, \ldots, $\tau_s$
respectively, then the elementary differential
$\{g_1 \ g_2\ \ldots \ g_s\} = \fdiff{f}{s}\,(g_1,g_2,\ldots,g_s)$ is
associated to the rooted tree $[\tau_1\ \tau_2\ \ldots\ \tau_s]$.
\end{enumerate}
\end{prop}

\begin{egg}
If $\tau_1$, $\tau_2$ and $\tau_3$ are the rooted trees defined at the
beginning of Section~\ref{TreeSSSection}, we have that
$g_1 = \{ f\ f\}$ is associated to $\tau_1$,
$g_2 = \{\{f\}\}$ is associated to $\tau_2$
and
$g_3 = \{\{f\ f\}\}$ is associated to $\tau_3$.
Thus $\{ g_1\ g_2 \ g_3\}$ is associated to the rooted tree
$[\tau_1\ \tau_2\ \tau_3]$.
\end{egg}

\begin{rmk}
If we have $f:\RR \rightarrow \RR$, the relation between rooted trees
and elementary differentials is simple.  For instance, let $\tau_f$ be
the rooted tree
\begin{picture}(3,4.2)
\put(1.5,0){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1.5,2){\circle*{0.1}}
\put(1,3){\circle*{0.1}}
\put(2,3){\circle*{0.1}}
\put(0.5,4){\circle*{0.1}}
\put(1.5,4){\circle*{0.1}}
\put(1.5,0){\line(0,1){1}}
\put(1.5,1){\line(0,1){1}}
\put(1.5,2){\line(-1,2){0.52}}
\put(1.5,2){\line(1,2){0.52}}
\put(1,3){\line(-1,2){0.52}}
\put(1,3){\line(1,2){0.52}}
\end{picture} .
We associate to this rooted tree the rooted tree
\begin{picture}(3,4.2)
\put(1.5,0){\circle*{0.1}}
\put(1.6,0){$f_y$}
\put(1.5,1){\circle*{0.1}}
\put(1.6,1){$f_y$}
\put(1.5,2){\circle*{0.1}}
\put(1.6,1.9){$f_{yy}$}
\put(1,3){\circle*{0.1}}
\put(1.1,2.9){$f_{yy}$}
\put(2,3){\circle*{0.1}}
\put(2.1,2.9){$f$}
\put(0.5,4){\circle*{0.1}}
\put(0.1,3.8){$f$}
\put(1.5,4){\circle*{0.1}}
\put(1.6,3.8){$f$}
\put(1.5,0){\line(0,1){1}}
\put(1.5,1){\line(0,1){1}}
\put(1.5,2){\line(-1,2){0.52}}
\put(1.5,2){\line(1,2){0.52}}
\put(1,3){\line(-1,2){0.52}}
\put(1,3){\line(1,2){0.52}}
\end{picture}.
Let $\tau$ be the rooted tree of order one.  Then
$\tau_f = [[[\tau\ [\tau \ \tau]]]]$ and the elementary
differential of $f$ associate to 
$\tau_f = $ is $\{\{\{f\ \{f\ f\}\}\}\} = f^2_{yy} \,f^2_y\,f^3$.
We only have to multiply the derivatives that appear in the second
rooted tree.
\end{rmk}

\subsubsection{Runge-Kutta Methods}

We now use rooted trees and elementary differentials to develop
Runge-Kutta methods.

\begin{theorem}
We consider the initial value problem (\ref{ODEndim}) where $f$ does
not depend on the time.  Thus, $f:\RR^n \to \RR^n$ and
$\VEC{y}'(t) = f(\VEC{y}(t))$.  We have
that
\[
\VEC{y}^{(q)}(t) = \sum_{r(\tau) = q} \,\alpha(\tau) F(\tau) \ ,
\]
where $F(\tau)$ is evaluated at $\VEC{y}(t)$.
\end{theorem}

\begin{egg}
\begin{align*}
\VEC{y}^{(4)} &= \{f\ f\ f\} + 3 \{f \ \{f\}\} + \{\{f \ f\}\} +
\{\{\{f\}\}\} \\
&= \fdiff{f}{3}(f,f,f) + 3\,\fdiff{f}{2}(f,\fdiff{f}{}\,(f)) +
\fdiff{f}{}(\fdiff{f}{2}(f,f)) +
\fdiff{f}{}\,(\fdiff{f}{}\,(\fdiff{f}{}\,(f))) \ .
\end{align*}
If we have $f:\RR\rightarrow \RR$, we then get
\[
y^{(4)} = f_{yyy}f^3 + 3 f_{yy} f_{y} f^2 + f_{yy} f_{y} f^2 +
f_{y}^3f
= f_{yyy}f^3 + 4 f_{yy} f_{y} f^2 + f_{y}^3f \ .
\]
\end{egg}

From now on, we consider the general definition of the Runge-Kutta
methods given in Definition~\ref{GFRKM}.

We define a mapping $\Psi$ which associates to each rooted tree $\tau$
a sum $\Psi(\tau)$ constructed from some elements of the Butcher array
\[
\begin{array}{c|cccc}
\alpha_1 & \beta_{1,1} & \beta_{1,2} & \ldots & \beta_{1,s} \\
\alpha_2 & \beta_{2,1} & \beta_{2,2} & \ldots & \beta_{2,s} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\alpha_s & \beta_{s,1} & \beta_{s,2} & \ldots & \beta_{s,s} \\
\hline
 & \gamma_1 & \gamma_2 & \ldots & \gamma_s
\end{array}
\]
If $\tau$ is a rooted tree, $\Psi(\tau) = \psi_{s+1}(\tau)$, where the
function $\psi_{s+1}$ is defined recursively as follows.
Let $\beta_{s+1,j} = \gamma_j$ for $1 \leq j \leq s$.
\begin{enumerate}
\item If $\tau$ is the rooted tree of order $1$,  then
$\displaystyle \psi_j(\tau) \equiv \sum_{k=1}^s\,\beta_{j,k}$
for $1\leq j \leq s+1$.
\item If $\tau = [\tau_1\ \tau_2\ \ldots\ \tau_q]$ , where $\tau_1$,
$\tau_2$, \ldots , $\tau_q$ are rooted trees, then
\[
\psi_i(\tau) \equiv \sum_{j=1}^s\,\beta_{i,j}\,\psi_j(\tau_1)\,\psi_j(\tau_2)
\ldots\psi_j(\tau_q)
\]
for $1\leq i \leq s+1$.
\end{enumerate}

There is an easy way to compute $\Psi(\tau)$.  We illustrate it with
some examples in Table~\ref{compPsi} below.

\begin{longtable}{ccc}
$\tau$ &  &  $\Psi(\tau)$ \\
\hline
& & \\
\endfirsthead
$\tau$ &  &  $\Psi(\tau)$ \\
\hline
& & \\
\endhead
\endfoot
\caption{Computation of $\Psi(\tau)$ for some basic tress \label{compPsi}}
\endlastfoot
\begin{picture}(2,0)
\put(1,0){\circle*{0.1}}
\end{picture}
&
\begin{picture}(2,0)
\put(1,0){\circle*{0.1}}
\put(1.1,0){$j$}
\end{picture}
&
$\displaystyle \sum_{j=1}^s\,\beta_{s+1,j} = \sum_{j=1}^s\,\gamma_j = 1$
\\
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(1,0){\line(0,1){1}}
\put(1,1){\circle*{0.1}}
\end{picture}
&
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(1.1,0){$j_1$}
\put(1,1){\circle*{0.1}}
\put(1.1,1){$j_2$}
\put(1,0){\line(0,1){1}}
\end{picture}
&
\parbox{7cm}{
\begin{align*}
&\sum_{j_1=1}^s\left(\beta_{s+1,j_1}\sum_{j_2=1}^s \,\beta_{j_1,j_2}\right) \\
&=\sum_{j_1=1}^s\,\gamma_{j_1} \,\alpha_{j_1}
\end{align*}} \\
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,0){\line(1,2){0.52}}
\put(1,0){\line(-1,2){0.52}}
\end{picture}
&
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(1.1,0){$j_1$}
\put(0.5,1){\circle*{0.1}}
\put(0.6,1){$j_2$}
\put(1.5,1){\circle*{0.1}}
\put(1.6,1){$j_3$}
\put(1,0){\line(1,2){0.52}}
\put(1,0){\line(-1,2){0.52}}
\end{picture}
&
\parbox{9cm}{
\begin{align*}
&\sum_{j_1=1}^s\left( \beta_{s+1,j_1} \left(\sum_{j_2=1}^s\,\beta_{j_1,j_2}\right)
\left( \sum_{j_3=1}^s\,\beta_{j_1,j_3}\right)\right) \\
&= \sum_{j_1=1}^s\gamma_{j_1} \alpha_{j_1}^2 
\end{align*}}
\\
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(0.5,1){\line(1,2){0.52}}
\end{picture}
&
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(1.1,0){$j_1$}
\put(0.5,1){\circle*{0.1}}
\put(0,1){$j_2$}
\put(1,2){\circle*{0.1}}
\put(1.1,2){$j_3$}
\put(1,0){\line(-1,2){0.52}}
\put(0.5,1){\line(1,2){0.52}}
\end{picture}
&
\parbox{9cm}{
\begin{align*}
&\sum_{j_1=1}^s\left( \beta_{s+1,j_1} \left(\sum_{j_2=1}^s\,\beta_{j_1,j_2}
\left( \sum_{j_3=1}^s\,\beta_{j_2,j_3} \right) \right)\right) \\
&= \sum_{j_1=1}^s\left( \gamma_{j_1} \left(\sum_{j_2=1}^s\,\beta_{j_1,j_2}
\alpha_{j_2} \right)\right)
\end{align*}}
\\
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\put(1.5,1){\line(-1,2){0.52}}
\end{picture}
&
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(1.1,-0.1){$j_1$}
\put(0.5,1){\circle*{0.1}}
\put(0,1){$j_2$}
\put(1.5,1){\circle*{0.1}}
\put(1.6,1){$j_3$}
\put(1,2){\circle*{0.1}}
\put(1.2,1.9){$j_4$}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\put(1.5,1){\line(-1,2){0.52}}
\end{picture}
&
\parbox{9.8cm}{
\begin{align*}
&\sum_{j_1=1}^s\left( \beta_{s+1,j_1} \left(\sum_{j_2=1}^s\,\beta_{j_1,j_2}\right)
\left( \sum_{j_3=1}^s\,\beta_{j_1,j_3} \left(\sum_{j_4=1}^s\,
\beta_{j_3,j_4} \right)\right)\right) \\
&= \sum_{j_1=1}^s\left( \gamma_{j_1} \alpha_{j_1}
\left( \sum_{j_3=1}^s\,\beta_{j_1,j_3} \alpha_{j_3} \right)\right) 
\end{align*}}
\end{longtable}

We define the function
\[
\VEC{Y}_i(z) = \VEC{y}(t_i) + \VEC{y}'(t_i)\,z +
\frac{1}{2!} \VEC{y}''(t_i) \, z^2 +
\frac{1}{3!} \VEC{y}^{(3)}(t_i) \, z^3 + \ldots
\]
We have that $\VEC{y}_{i+1} = \VEC{y}(t_{i+1}) = \VEC{Y}(h)$.
We are assuming that the Taylor series of $\VEC{y}$ at $t_i$ has a
radius of convergence greater than $h$.
From the Runge-Kutta method, we also define the function 
\[
\VEC{W}_i(z) = \VEC{w}_i + z \sum_{j=1}^s\,\gamma_j K_j \ ,
\]
where
\[
K_j = f\left(t_i + \alpha_j z, \VEC{w}_i + z
  \sum_{k=1}^s\,\beta_{j,k} K_k\right)
\]
for $1 \leq j \leq s$.  We have that $\VEC{w}_{i+1} = \VEC{W}_i(h)$.

Our goal is to match the series expansions of $\VEC{W}_i$ and
$\VEC{Y}_i$ near the origin to generate Runge-Kutta methods of high
order.   To be rigorous, the only think that we need is Taylor
polynomial expansions of $\VEC{Y}_i$ and $\VEC{W}_i$ of degree
sufficiently large.  We use the series expansions with a large enough
radius of convergence to simplify the presentation.

\begin{prop}
We have that
\[
\dydxn{\VEC{W}_i}{z}{q}(0) =
\sum_{r(\tau)=q}\,\alpha(\tau)\,\gamma(\tau)\,\Psi(\tau)\,F(\tau)
\ ,
\]
where $F(\tau)$ is evaluated at $\VEC{W}_i(0) = \VEC{w}_i$
\end{prop}

\begin{theorem}
A Runge-Kutta method is of order $p$ if $\Psi(\tau) = 1/\gamma(\tau)$
for all rooted trees $\tau$ of order less than or equal to $p$, and
$\Psi(\tau) \not= 1/\gamma(\tau)$ for at least one rooted tree of order
$p+1$.
\label{OrdRel}
\end{theorem}

\begin{proof}
We have
\[
\VEC{Y}_i(z) = \VEC{y}_i + \VEC{y}'(t_i) \,z +
\frac{1}{2!}\VEC{y}''(t_i) \, z^2 +
\frac{1}{3!}\VEC{y}^{(3)}(t_i) \, z^3 + \ldots 
= \VEC{y}_i + \sum_{q=1}^\infty\,\frac{1}{q!}\bigg(
\sum_{r(\tau)=q}\,\alpha(\tau)\,F(\tau) \bigg) z^q \ ,
\]
where $F(\tau)$ is evaluated at $\VEC{y}_i$,
and
\[
\VEC{W}_i(z) = \VEC{w}_i + \dydx{\VEC{W}_i}{z}(0) \,z +
\frac{1}{2}\,\dydxn{\VEC{W}_i}{z}{2}(0) \,z^2 + \ldots
= \VEC{w}_i + \sum_{q=1}^\infty\,\frac{1}{q!}\bigg(
\sum_{r(\tau)=q}\,\alpha(\tau)\,\gamma(\tau)\,\Psi(\tau)\,F(\tau)
\bigg) z^q  \ ,
\]
where $F(\tau)$ is evaluated at $\VEC{W}_i(0) = \VEC{w}_i$.

To compute the local truncation error, we make use of the localisation
assumption $\VEC{y}_i = \VEC{w}_i$.

Hence, if $\gamma(\tau)\,\Psi(\tau) = 1$ for all rooted trees of order
less than or equal to $p$, then the series expansions of $\VEC{Y}_i$
and $\VEC{W}_i$ have identical terms in $z^q$ for $q\leq p$.  We thus
have that
\[
\VEC{Y}(z) = \VEC{W}_i(z) + \sum_{q=p+1}^\infty\bigg(
\frac{1}{q!}\,\sum_{r(\tau)=q}\,\alpha(\tau) \left( 1
-\gamma(\tau)\,\Psi(\tau) \right)F(\tau)\bigg) z^q \ ,
\]
where $F(\tau)$ is evaluated at $\VEC{y}_i = \VEC{w}_i$.

Hence, the local truncation error is
\begin{equation}\label{TreeTruncErr}
\begin{split}
\tau_{i+1}(h) &=
\frac{\VEC{y}(t_{i+1}) - \VEC{y}(t_i)}{h} - \phi(t_i,\VEC{y}(t_i))
= \frac{\VEC{Y}_i(h) - \VEC{W}_i(h)}{h} \\
& = \frac{h^p}{(p+1)!}\,\sum_{r(\tau)=p+1}\,\alpha(\tau) \left( 1
-\gamma(\tau)\,\Psi(\tau) \right)F(\tau) + O(h^{p+1}) \ ,
\end{split}
\end{equation}
where $F(\tau)$ is evaluated at $\VEC{y}_i = \VEC{w}_i$.
Therefore, the local truncation error is of order at least $p$.  It
will be exactly of order $p$ if there exists a rooted tree of order
$p+1$ such that $\gamma(\tau)\,\Psi(\tau) \not= 1$
(Remark~\ref{NOTiff} below).
\end{proof}

\begin{egg}
We consider the $3$-stage explicit Runge-Kutta methods.  Since the
methods are explicit, we have
$\alpha_1 = 0$ and $\beta_{i,j} = 0$ for $j\geq i$.
We look for methods of order at least $3$.

For the rooted tree $\tau$ of order one, we get from
$\Psi(\tau) = 1/\gamma(\tau)$ that
$\displaystyle 1 = \sum_{j=1}^3 \gamma_j$.

For the rooted tree $\tau$ of order two, we get from
$\Psi(\tau) = 1/\gamma(\tau)$ that
$\displaystyle \frac{1}{2} = \sum_{j=2}^3\,\alpha_j\,\gamma_j$ since
$\alpha_1 = 0$.

For the tree $\tau =$
\begin{picture}(2,1)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1.5,1){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(1,0){\line(1,2){0.52}}
\end{picture}
of order three, we get from
$\Psi(\tau) = 1/\gamma(\tau)$ that
\[
\frac{1}{3} =
\sum_{i=1}^3\,\gamma_i \left( \sum_{j=1}^3\,\beta_{i,j}\right)
\left(\sum_{k=1}^3\,\beta_{i,k}\right)
= \sum_{i=1}^3\,\gamma_i \alpha_i^2
= \sum_{i=2}^3\,\gamma_i \alpha_i^2
\]
since $\alpha_1 = 0$.

For the tree $\tau=$
\begin{picture}(2,2)
\put(1,0){\circle*{0.1}}
\put(0.5,1){\circle*{0.1}}
\put(1,2){\circle*{0.1}}
\put(1,0){\line(-1,2){0.52}}
\put(0.5,1){\line(1,2){0.52}}
\end{picture}
of order three, we get from
$\Psi(\tau) = 1/\gamma(\tau)$ that
\[
\frac{1}{6} =
\sum_{i=1}^3\,\gamma_i \left( \sum_{j=1}^3\,\beta_{i,j} \left(
\sum_{k=1}^3\,\beta_{j,k}\right)\right) \\
= \sum_{i=1}^3\,\gamma_i\left(\sum_{j=1}^3 \,\beta_{i,j}\,\alpha_j\right)
= \gamma_3\,\beta_{3,2}\,\alpha_2
\]
since $\alpha_1 = 0$ and $\beta_{i,j} = 0$ for $j\geq i$.

Two possible Butcher arrays that satisfy
$\displaystyle \gamma_1 + \gamma_2 + \gamma_3 = 1$,
$\alpha_2 \gamma_2 + \alpha_3 \gamma_3 = 1/2$ and
$\gamma_3 \beta_{3,2} \alpha_2 = 1/6$ are:

Heun's method:
\[
\begin{array}{c|ccc}
0 & \\
1/3 & 1/3 \\
2/3 & 0 & 2/3 \\
\hline
 & 1/4 & 0 & 3/4
\end{array}
\]

Kutta's method of order three:
\[
\begin{array}{c|ccc}
0 & \\
1/2 & 1/2 \\
1 & -1 & 2 \\
\hline
 & 1/6 & 2/3 & 1/6
\end{array}
\]

These method are therefore of order at least three.  The reader can
verify that, for each of these methods, there is a rooted three of
order four $\tau$ such that $\Psi(\tau) \neq 1/\gamma(\tau)$.  So the
method are of order three.
\end{egg}

\begin{egg}
We consider the $2$-stage implicit Runge-Kutta methods of order four.
We have the relations
\begin{align*}
\alpha_1 &= \beta_{1,1} + \beta_{1,2} \\
\alpha_2 &= \beta_{2,1} + \beta_{2,2}
\end{align*}
from the definition of the Runge-Kutta methods.  From
Theorem~\ref{OrdRel}, we also have the following relations.

From the rooted tree of order one, we get $1 = \gamma_1 + \gamma_2$.

From the rooted tree of order two, we get
$\displaystyle \frac{1}{2} = \sum_{j=1}^2 \gamma_j \alpha_j$.

From the two rooted trees of order three, we get
$\displaystyle \frac{1}{3} = \sum_{j=1}^2 \gamma_j \alpha_j^2$
and
$\displaystyle \frac{1}{6}
= \sum_{j=1}^2 \sum_{k=1}^2 \gamma_j\beta_{j,k} \alpha_k$.

From the four rooted trees of order four, we get
$\displaystyle \frac{1}{4} = \sum_{j=1}^2 \gamma_j \alpha_j^3$,
$\displaystyle \frac{1}{8}
= \sum_{j=1}^2 \sum_{k=1}^2 \gamma_j\alpha_j\beta_{j,k} \alpha_k$,
$\displaystyle \frac{1}{12}
= \sum_{j=1}^2 \sum_{k=1}^2 \gamma_j\beta_{j,k} \alpha_k^2$ and
$\displaystyle \frac{1}{24}
= \sum_{j=1}^2 \sum_{k=1}^2 \sum_{l=1}^2 \gamma_j\beta_{j,k} \beta_{k,l}\alpha_l$.

Miraculously, there is a unique solution (modulo conjugacy) of all
these equations.  We get the Butcher array
\[
\begin{array}{c|cc}
(3-\sqrt{3})/6 & 1/4 & (3-2 \sqrt{3})/12 \\
(3 + \sqrt{3})/6 & (3 + 2\sqrt{3})/12 & 1/4 \\
\hline
 & 1/2 & 1/2
\end{array}
\]
that we have already found in Example~\ref{Gauss_Leg_RK}, using
collocation methods.
\label{Gauss_Leg_RKcontinued}
\end{egg}

\begin{rmk}
We show that the $4$-stage Runge-Kutta method given by the
Butcher array 
\[
\begin{array}{c|ccccc}
0 & \\
1/2 & 1/2 \\
-1 & 1/2 & -3/2 \\
1 & 0 & 4/3 & -1/3 \\
\hline
 & 1/6 & 2/3 & 0 & 1/6
\end{array}
\]
is of order $4$ if $f:\RR \rightarrow \RR$ and of order $3$ if
$f:\RR^n\rightarrow\RR^n$ with $n>1$.

In the proof of Theorem~\ref{OrdRel}, we use the conditions
$\Psi(\tau)\gamma(\tau) = 1$ for all rooted trees $\tau$ of order
$q \leq p$ to ensure that
\begin{equation} \label{YWorderCond}
\sum_{r(\tau)=q}\,\alpha(\tau)\,F(\tau)
= \sum_{r(\tau)=q}\,\alpha(\tau)\,\gamma(\tau)\,\Psi(\tau)\,F(\tau)
\end{equation}
for $q \leq p$.  This was sufficient to ensure that the coefficient of 
coefficient of $z^q$ in $\VEC{Y}_i(z)$ was equal to the coefficient of
$z^q$ in $\VEC{W}_i(z)$.  However, for $n=1$, the condition
$\Psi(\tau)\gamma(\tau) = 1$ for all rooted trees $\tau$ of order $q$
is not always necessary to satisfy (\ref{YWorderCond}).

We leave it to the reader to verify that, for the $4$-stage
Runge-Kutta method above, the condition
$\Psi(\tau)\gamma(\tau) = 1$ for all rooted trees $\tau$ of order
$q \leq 3$ is satisfied.  However, this is not true for $q=4$.
Despite that, the $4$-stage Runge-Kutta method is of order four for
$n=1$ but not for $n>1$.

Let $\tau_1 = [[\tau\ \tau]]$ and $\tau_2 = [\tau\ [\tau]]$ where
$\tau$ is the tree of order one.  $\tau_1$ and $\tau_2$ are two
trees of order four (Table~\ref{OSDA}).

If $f:\RR\rightarrow\RR$, then
\[
F(\tau_1) = \{\{f\ f\}\} = \fdiff{f}{}(\fdiff{f}{2}(f,f))) =
f_{yy}\,f_y\,f^2
\]
and
\[
F(\tau_2) = \{f\ \{f\}\} = \fdiff{f}{2}(f,\fdiff{f}{}(f)) =
f_{yy}\,f_y\,f^2 \; .
\]
Since $F(\tau_1) = F(\tau_2)$, we can replace 
$\Psi(\tau_j)\gamma(\tau_j) = 1$ for $j=1$ and $2$ by
\[
\alpha(\tau_1)\gamma(\tau_1)\Psi(\tau_1) +
\alpha(\tau_2)\gamma(\tau_2)\Psi(\tau_2) = \alpha(\tau_1) +
\alpha(\tau_1)
\]
with $\Psi(\tau)\gamma(\tau) = 1$ for the other rooted trees $\tau$ of
order four.  This ensures that the coefficient of $z^4$ in
$\VEC{Y}_i$ is still equal to the coefficient of $z^4$ in $\VEC{W}_i$.

The condition $\alpha(\tau_1)\gamma(\tau_1)\Psi(\tau_1) +
\alpha(\tau_2)\gamma(\tau_2)\Psi(\tau_2) = \alpha(\tau_1) +
\alpha(\tau_1)$, instead of the two conditions
$\Psi(\tau_j)\gamma(\tau_j) = 1$ for $j=1$ and $2$, was used to
obtain the $4$-stage Runge-Kutta method above.  We leave it to the
reader to verify that the $4$-stage Runge-Kutta method above verify this
condition.

When $f:\RR^n \rightarrow \RR^n$ with $n>1$, the relation
$F(\tau_1)= F(\tau_2)$ is not necessary true and so the condition
$\alpha(\tau_1)\gamma(\tau_1)\Psi(\tau_1) +
\alpha(\tau_2)\gamma(\tau_2)\Psi(\tau_2) = \alpha(\tau_1) +
\alpha(\tau_1)$ may not guarantee that the coefficient of
$z^4$ in $\VEC{Y}_i$ is equal to the coefficient of $z^4$
in $\VEC{W}_i$.

As a simple example for $F(\tau_1) \neq F(\tau_2)$, consider the
initial value problem
\begin{equation} \label{fR2R1order3}
\begin{split}
y'(t) & = f(t,y(t)) \quad , \quad  t_0 \leq t \leq t_f \\
y(t_0) &= y_0
\end{split}
\end{equation}
where $f:\RR^2 \rightarrow \RR$.   We can rewrite this initial value
problem as a system
\begin{align*}
\VEC{z}'(t) & = \tilde{f}(\VEC{z}(t)) \quad , \quad  t_0 \leq s \leq t_f \\
\VEC{z}(t_0) &= \VEC{z}_0
\end{align*}
where
$\displaystyle \tilde{f}(\VEC{z}) = \begin{pmatrix}
f(z_2,z_1) \\ 1 \end{pmatrix}$ and
$\displaystyle \VEC{z}_0 = \begin{pmatrix}
y_0 \\ t_0 \end{pmatrix}$.
Hence,
\begin{align*}
F(\tau_1) &= \{\{\tilde{f} \ \tilde{f}\}\} =
\begin{pmatrix}
f_{z_1}\left( f_{z_2z_2} + 2f\,f_{z_1z_2} +f^2\,f_{z_1z_1}\right) \\
0
\end{pmatrix} \\
\intertext{and}
F(\tau_2) &= \{\tilde{f}\ \{\tilde{f}\}\} =
\begin{pmatrix}
\left( f_{z_2z_1} + f\,f_{z_1z_1}\right)\left(f_{z_2} +f\,f_{z_1}\right) \\
0
\end{pmatrix}
\end{align*}
are generally different.
\label{NOTiff}

\noindent {\bfseries Note}: This also shows that the
$4$-stage Runge-Kutta method above is only of order three for
the initial value problem (\ref{fR2R1order3}).
\end{rmk}

\begin{rmk}
Consider the following three statements about a fixed Runge-Kutta
method applied to the initial value problem (\ref{ODEndim}).
\begin{description}
\item[$A$] The method is of order $p$ with $f:\RR^n \rightarrow \RR^n$,
$n>1$.  Note that $f$ does not depend on time.
\item[$B$] The method is of order $p$ with $f:\RR \times \RR \rightarrow \RR$.
Note that $f$ depends on time. 
\item[$C$] The method is of order $p$ with $f:\RR \rightarrow \RR$.  Note
that $f$ does not depend on time. 
\end{description}
It has been proved that
\begin{enumerate}
\item $A \Leftrightarrow B \Leftrightarrow C$  if $1\leq p \leq 3$.
\item $A \Leftrightarrow B \Rightarrow C$ and $C \not\Rightarrow B$ if
$p=4$.
\item $A \Rightarrow B \Rightarrow C$, $C \not\Rightarrow B$ and
$B \not\Rightarrow A$ if $p > 4$.
\end{enumerate}
\end{rmk}

\subsubsection{Maximal Order of Explicit Runge-Kutta Methods}

\begin{theorem}
An $s$-stage explicit Runge-Kutta method cannot be of order greater
than $s$.
\end{theorem}

\begin{proof}
Let $\tau$ be the rooted tree of order one.  Consider the rooted tree
$\tau_p$ of order $p$ defined by
\[
\tau_p = \underbrace{[[\ldots[}_{\text{$p-1$ times}} \tau \quad ]\ldots]]
\ .
\]
We have that $\gamma(\tau_p) = p!$ and
\[
\Psi(\tau_p) = \sum_{j_1=1}^s\,\sum_{j_2=1}^s\ldots\sum_{j_p=1}^s\,
\gamma_{j_1}\,\beta_{j_1,j_2}\dots \beta_{j_{p-1},j_p} \ .
\]
Since $\beta_{i,j} = 0$ for $j\geq i$, $\Psi(\tau_p)=0$ unless
$s \geq j_1 > j_2 > \ldots > j_p$.  This is possible only if
$s \geq p$.  So, for $p>s$, $\Psi(\tau_p) = 0$ and we cannot get
$1 = \gamma(\tau_p)\Psi(\tau_p)$.
\end{proof}

We have an even stronger result for the $5$-stage explicit Runge-Kutta
methods.

\begin{theorem}
There is no $5$-stage explicit Runge-Kutta method of order five.
\end{theorem}

\subsection{Variable Step-Size Methods}

Up until now, we have only considered methods with equally spaced mesh
points $t_i$ for $i=0$, $1$, $2$, \ldots, $N$.  It will be advantageous to
have some control on the {\bfseries step-size}\index{Runge-Kutta!Step-Size}
(i.e.\ the distance) between two consecutive mesh points.  A large
step-size could be used on the portions of the interval $[t_0,t_f]$
where the solution $y$ of (\ref{ODE}) varies slowly and a small
step-size could be used on the portions of the interval $[t_0,t_f]$
where the solution $y$ of (\ref{ODE}) varies rapidly.

A method often used to control the step-size between each pair of mesh
points is the
{\bfseries Runge-Kutta-Fehlberg method}\index{Runge-Kutta-Fehlberg Method}.
Let
\begin{equation} \label{RKMfour}
\begin{split}
w_0 &= y_0 \\
w_{i+1} &= w_i + h\phi(t_i,w_i)
\end{split}
\end{equation}
be a Runge-Kutta method of order four and
\begin{equation} \label{RKMfive}
\begin{split}
\tilde{w}_i &= w_i \\
\tilde{w}_{i+1} &= \tilde{w}_i + h \tilde{\phi}(t_i,\tilde{w}_i)
\end{split}
\end{equation}
be a Runge-Kutta method of order five.  The functions $\phi$ and
$\tilde{\phi}$ associated to the Runge-Kutta-Fehlberg method will be
given below.

Let $\tau_{i+1}(h)$ be the local truncation error for the Runge-Kutta
method of order four (\ref{RKMfour}).  Combining the Runge-Kutta
methods of orders four and five, (\ref{RKMfour}) and (\ref{RKMfive})
respectively, we can determine the step-size $h$ between $t_i$ and
$t_{i+1}$ such that $\tau_{i+1}(h)<\epsilon$ for a $\epsilon$ given.

The Runge-Kutta-Fehlberg method can be summarized as follows:

\begin{algo}[Runge-Kutta-Fehlberg Method]
\begin{enumerate}
\item $w_0 = y_0$.
\item Stop if $t_i = t_f$.
\item Suppose that $w_i$ is an approximation of $y_i= y(t_i)$
and $h > 0$ is given.  Compute a first
approximation $w_{i+1}$ of $y_{i+1}$ using (\ref{RKMfour}) and a
second approximation $\tilde{w}_{i+1}$ of $y_{i+1}$ using
(\ref{RKMfive}) with $\tilde{w}_i = w_i$.
\item If $|(\tilde{w}_{i+1} - w_{i+1})/h| < \epsilon $, accept $w_{i+1}$
as an approximation of $y_{i+1}= y(t_i+h)$.  Substitute $h$ by
$qh$ where
$q = \left|\epsilon h /(\tilde{w}_{i+1} - w_{i+1})\right|^{1/4}$.
\item If $|(\tilde{w}_{i+1} - w_{i+1})/h| \geq \epsilon $, reject
$w_{i+1}$ as an approximation of $y(t_i+h)$.  Substitute
$h$ by $qh$ where
$q=\left|\epsilon\,h/(\tilde{w}_{i+1}-w_{i+1})\right|^{1/4}$.
\item If $h > t_f -t_i$, replace $h$ by $t_f - t_i$.
\item If $w_{i+1}$ has been accepted, go back to 2 with $i$ replaced
by $i+1$ and the new value of $h$.  If $w_{i+1}$ has been rejected, go
back to 2 with $i$ again and the new smaller value of $h$.
\end{enumerate}
\end{algo}

The function $\phi(t_i,w_i)$ in (\ref{RKMfour}) is defined by
\[
\phi(t_i,w_i) = \frac{25}{216} K_1 + \frac{1408}{2565} K_3+
\frac{2197}{4104} K_4 - \frac{1}{5} K_5
\]
and the function $\tilde{\phi}(t_i,\tilde{w}_i)$ in (\ref{RKMfive})
(recall that $w_i=\tilde{w}_i$) is defined by
\[
\tilde{\phi}(t_i,w_i) = \frac{16}{135} K_1 + \frac{6656}{12825} K_3+
\frac{28561}{56430} K_4 - \frac{9}{50} K_5 + \frac{2}{55} K_6 \ ,
\]
where
\begin{align*}
K_1 &= f\left(t_i,w_i\right) \; , \\
K_2 &= f\left(t_i+\frac{h}{4},w_i+ \frac{h\,K_1}{4}\right)\; , \\
K_3 &= f\left(t_i+\frac{3h}{8},w_i+ \frac{3h\,K_1}{32} +
\frac{9h\,K_2}{32}\right) \; ,\\
K_4 &= f\left(t_i + \frac{12h}{13},w_i+ \frac{1932h\,K_1}{2197} -
\frac{7200h\,K_2}{2197} + \frac{7296h\,K_3}{2197}\right) \; ,\\
K_5 &= f\left(t_i + h,w_i+ \frac{439h\,K_1}{216} - 8h\,K_2 +
\frac{3680h\,K_3}{513} - \frac{845h\,K_4}{4104}\right)
\intertext{and}
K_6 &= f\left(t_i + \frac{h}{2},w_i - \frac{8h\,K_1}{27} + 2h\,K_2 -
\frac{3544h\,K_3}{2565} + \frac{1859h\,K_4}{4104} -
\frac{11h\,K_5}{40} \right) \ .
\end{align*}

Both Runge-Kutta methods can be summarized in the following Butcher
array
\[
\begin{array}{r|rrrrrr}
0 & & & & & & \\
1/4 & 1/4 & & & & & \\
3/8 & 3/32 & 9/32 & & & & \\
12/13 & 1932/2197 &
-7200/2197 & 7296/2197 & & & \\
1 & 439/216 & -8 & 3680/513 & -845/4104 & & \\
1/2 & -8/27 & 2 & -3544/2565 & 1859/4104 & -11/40 & \\
\hline
 & 25/216 & 0 & 1408/2565 & 2197/4104 & -1/5 & \\
\hline
 & 16/135 & 0 & 6656/12825 & 28561/56430 & -9/50 & 2/55
\end{array}
\]

\begin{rmk}
A non-rigorously justification of the Runge-Kutta-Fehlberg method is
as follows.  Let $\tilde{\tau}_{i+1}(h)$ be the local truncation error
for the Runge-Kutta method of order five (\ref{RKMfive}).  Suppose
that $y_i \approx w_i = \tilde{w}_i$, then
\[
y_{i+1} - w_{i+1} = y_{i+1} - w_i -h \phi(t_i,w_i)
\approx y_{i+1} - y_i - h \phi(t_i,y_i) = h \tau_{i+1}(h) \ .
\]
Similarly,
\[
y_{i+1} - \tilde{w}_{i+1} \approx h \tilde{\tau}_{i+1}(h) \;  .
\]
Hence,
\begin{align*}
\tau_{i+1}(h) &\approx \frac{1}{h} \left(y_{i+1} - w_{i+1} \right)
= \frac{1}{h}\left(y_{i+1} - \tilde{w}_{i+1} + \tilde{w}_{i+1} -
 w_{i+1} \right) \\
&= \frac{1}{h}\left(y_{i+1} - \tilde{w}_{i+1}\right) +
 \frac{1}{h}\left( \tilde{w}_{i+1} - w_{i+1} \right)
\approx \tilde{\tau}_{i+1}(h) +
 \frac{1}{h}\left( \tilde{w}_{i+1} -w_{i+1} \right) \ .
\end{align*}
Since $\tau_{i+1}(h) = O(h^4)$ and $\tilde{\tau}_{i+1}(h) = O(h^5)$,
$(\tilde{w}_{i+1} - w_{i+1})/h$ is the dominant term on the right hand
side for $h$ small.  Thus, we may assume that
\begin{equation} \label{approx_tau}
\tau_{i+1}(h) \approx \frac{1}{h}\left( \tilde{w}_{i+1} - w_{i+1}
\right) \ .
\end{equation}

If $\left|(\tilde{w}_{i+1} - w_{i+1})/h\right| < \epsilon$, we may
assume that $|\tau_{i+1}(h)| < \epsilon$.  Therefore, $w_{i+1}$ is an
acceptable approximation of $y(t_i+h)$.

If $\left|(\tilde{w}_{i+1} - w_{i+1})/h\right| \geq \epsilon$, then
$w_{i+1}$ is probably not an acceptable approximation of $y(t_i+h)$.
We choose a new (smaller) step-size $h$.  We repeat (\ref{RKMfour}) and
(\ref{RKMfive}) starting at $(t_i,w_i)$ again and using the new
step-size.

How do we select a new step-size $h$ to go from $t_i$ to $t_{i+1}$?
Formula (\ref{approx_tau}) is used to find a new value of $h$ such
that $\left|(\tilde{w}_{i+1} - w_{i+1})/h\right| < \epsilon$.
Since $\tau_{i+1}(h) = O(h^4)$, we may assume that
$\tau_{i+1}(h)\approx Ch^4$ for some constant $C$ and $h$ small.  Let
$q$ be a positive constant and suppose that (\ref{RKMfour}) is used
to approximate $y(t_i+qh)$.
The local truncation error in this case is
\[
\tau_{i+1}(hq) \approx Cq^4h^4 \approx q^4 \tau_{i+1}(h) \approx
\frac{q^4}{h}\left( \tilde{w}_{i+1} - w_{i+1} \right) \;  .
\]
If we require $|\tau_{i+1}(qh)| < \epsilon$, then
$q^4\left| (\tilde{w}_{i+1} - w_{i+1})/h \right| < \epsilon$ or
\begin{equation} \label{q_value}
q < \left|\frac{\epsilon h}{\tilde{w}_{i+1} - w_{i+1}} \right|^{1/4} \ .
\end{equation}
The new step-size that is used is $qh$ where $q$ satisfies
(\ref{q_value}).
\end{rmk}

\begin{rmk}
In step $4$ of the Runge-Kutta-Fehlberg method, we replace
$h$ by $qh$ even
if $w_{i+1}$ is accepted.  The reason is simple.
If $\left|(\tilde{w}_{i+1}-w_{i+1})/h\right|$ is small, then $q$
should be greater than one and the step-size is increased.  This
corresponds to taking a large step-size when $y$ varies slowly.
\end{rmk}

We now implement the Runge-Kutta-Fehlberg method.

\begin{code}[Runge-Kutta-Fehlberg Method]
To approximate the solution of the initial value problem
\[
\begin{split}
y'(t) &= f(t,y(t)) \quad, \quad t_0 \leq t \leq t_f \\
y(0) &= y_0
\end{split}
\]
\subI{Input} The maximal step-size hmax.\\
The minimal step-size hmin.\\
The maximal tolerated error T.\\
The initial time $t_0$ (t0 in the code below) and final time $t_f$ (tf
in the code below).\\
The initial conditions $y_0$ (y0 in the code below) at $t_0$.\\
The function $f(t,y)$ (funct in the code below). \\
\subI{Output} The approximations $w_i$ (gw(i+1) in the code below)
of $y(t_i)$ at $t_i$ (gt(i+1) in the code below) with
the requested tolerance and the step-size between  hmin
and  hmax  if it is possible.
\small
\begin{verbatim}
function [gt,gw] = rgktfb(funct,t0,y0,tf,hmin,hmax,T)
  h = hmax;
  gt(1) = t0;
  gw(1) = y0;
  t = t0;
  w = y0;

  while (0 == 0)
    k1 = h*funct(t,w);
    k2 = h*funct(t+h/4,w+k1/4);
    k3 = h*funct(t+3*h/8,w+3*k1/32+9*k2/32);
    k4 = h*funct(t+12*h/13,w+1932*k1/2197-7200*k2/2197+7296*k3/2197);
    k5 = h*funct(t+h,w+439*k1/216-8*k2+3680*k3/513-845*k4/4104);
    k6 = h*funct(t+0.5*h,w-8*k1/27+2*k2-3544*k3/2565+1859*k4/4104-11*k5/40);

    sigma = abs(k1/360-128*k3/4275-2197*k4/75240+k5/50+2*k6/55);
    if (sigma < T)
      % We accept w as an approximation of y(t) .  w is an approximation
      % of  y(t)  given by a Runge-Kutta method of order four.
      t = t+h;
      w = w+25*k1/216+1408*k3/2565+2197*k4/4104-k5/5;
      gt = [gt;t];
      gw = [gw;w];
    end
    
    % We have reached  tf  and the program should stop.
    if (t >= tf)
      return;
    end

    if (sigma == 0)
      % We choose a large value for  q  if the error seems to be negligable.
      q = 5;
    else
      q = (T/sigma)^(0.25);
    end

    % We choose the step-size less than hmax and larger than hmin
    % such that the local error should still be less than  T.
    if (q < 0.1)
      % We do not reduce the step-size h to less than 1/10
      % its original size.
      h = 0.1*h;
    elseif (q > 4)
      % We do not increase the step-size h to more than 4 times
      % its original size or hmax.
      h = min(4*h,hmax);
    else
      h = min(h*q,hmax);
    end

    % We make sure than the step-size is not smaller than hmin.
    if (h < hmin)
      break;
      gt = NaN;
      gw = NaN;
    end

    % We adjust the step-size if we are going to exceed  tf  at the next
    % step.
    if (t + h > tf)
      h = tf - t;
    end
  end
end
\end{verbatim}
\end{code}

\section{Multistep Methods}\label{SectMultMethod}

Up until now, we have only considered
{\bfseries one-step methods}\index{Multistep Methods!One-Step Methods};
namely, methods where the approximation $w_{i+1}$ of $y_{i+1}$ is
obtained from the approximation $w_i$ of $y_i$.  We fix $m>0$ and
consider methods where the approximation $w_{i+1}$ of $y_{i+1}$ is
obtained from a combination of the approximations $w_j$ of $y_j$ for
$j=i+1$, $i$, $i-1$, \ldots, $i-m$.

In this section, we assume that $f:[t_0,t_f]\times \RR \rightarrow \RR$
in (\ref{ODE}) is nice; namely, all the mixed derivatives of $f$ that
we need exist and are continuous.  This implies that the solution $y$
of (\ref{ODE}) is sufficiently differentiable.

\begin{defn}[General Form of a Multistep Method]
Let $0<m<N$, $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for
$i=0$, $1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the
solution of the difference equation
\begin{equation} \label{MULTISTEP}
\begin{split}
 w_{i+1} & = \sum_{j=0}^m a_j w_{i-j} + h
\sum_{j=-1}^m b_j f(t_{i-j},w_{i-j})
\quad, \quad i=m,m+1,\ldots,N-1 \\
w_i & = y_i \quad, \quad i=0,1,\ldots,m
\end{split}
\end{equation}
for some given constants $a_i$ and $b_i$.
If $b_{-1} = 0$, the method is called an
{\bfseries explicit}\index{Multistep Methods!Explicit} or
{\bfseries open method}\index{Multistep Methods!Open}.  If $b_{-1} \not=
0$. the method is called an
{\bfseries implicit}\index{Multistep Methods!Implicit} or
{\bfseries closed method}\index{Multistep Methods!Closed}.
\label{GFMSM}
\end{defn}

\begin{defn}
For a multistep method, the
{\bfseries local truncation error}\index{Multistep Methods!Local
Truncation Error} is defined by
\begin{align*}
\tau_{i+1}(h) &= \frac{1}{h}\left(y_{i+1} - \sum_{j=0}^ma_j y_{i-j} \right)
- \sum_{j=-1}^m b_j f(t_{i-j},y_{i-j}) \quad, \quad m \leq i < N \ .
\end{align*}
If, for all well-posed initial value problems (\ref{ODE}), there
exists a function $\tau:\RR\to\RR$ such that
$|\tau_{i+1}(h) | \leq \tau(h) = O(h^p)$ near the origin for all $i$,
we say that the
{\bfseries method is of order}\index{Multistep Methods!Order of a Method} $p$.
\label{local_trunc_error_def}
\end{defn}

\subsection{Classical Methods}

We consider (\ref{ODE}) with the usual partition
$t_0 < t_1 < \ldots < t_N = t_f$ and $h=(t_f-t_0)/N$.

If we approximate $f(t, y(t))$ on $t_i \leq t \leq t_{i+1}$ by the
average value
\[
\frac{f(t_{i+1},y(t_{i+1})) + f(t_i,y(t_i))}{2} \ ,
\]
then
\begin{align*}
y(t_{i+1}) &= y(t_i) + \int_{t_i}^{t_{i+1}}\,y'(t) \dx{t}
= y(t_i) + \int_{t_i}^{t_{i+1}}\,f(t,y(t)) \dx{t} \\
&\approx y(t_i) + \int_{t_i}^{t_{i+1}}\,
\frac{f(t_{i+1},y(t_{i+1})) + f(t_i,y(t_i)) }{2} \dx{t} \\
&= y(t_i) + \frac{f(t_{i+1},y(t_{i+1})) + f(t_i,y(t_i)) }{2}\,h \ .
\end{align*}
If we suppose that $w_i \approx y(t_i)$, we get the following method.

\begin{defn}[Trapezoidal Method] \label{trapmethdef}
Consider the initial value problem (\ref{ODE}).
Let $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for $i=0$,
$1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the
solution of the difference equation
\begin{align*}
w_{i+1} & = w_i + \frac{h}{2} \big( f(t_{i+1}.w_{i+1}) + f(t_i,w_i)
\big) \quad, \quad 0 \leq i < N \\
w_0 &= y_0
\end{align*}
\end{defn}

The trapezoidal method is an implicit rule because $w_{i+1}$ appears
on both sides of the equation.   Note that a one-step method like the
trapezoidal method is still a multistep method.

To compute the order of the trapezoidal method, we use Taylor series
expansions of $y(t)$ and $y'(t)$ for $t$ near $t_i$.  Namely,
\begin{align*}
\tau_{i+1}(h) &= \frac{y(t_{i+1}) - y(t_i)}{h} - \frac{1}{2}\big(
f(t_i,y(t_i)) + f(t_{i+1},y(t_{i+1})) \big) \\
&= \frac{y(t_{i+1}) - y(t_i)}{h} -
\frac{1}{2} \big( y'(t_i) + y'(t_{i+1}) \big) \\
&= \frac{ \big(y(t_i) + y'(t_i)\,h + y''(t_i)\,h^2/2 +
y^{(3)}(\xi_i)\,h^3/6 \big) - y(t_i)}{h} \\
& \qquad -\frac{1}{2} \left( y'(t_i) +
\left(  y'(t_i) + y''(t_i)\,h + \frac{1}{2}\,y^{(3)}(\eta_i)\,h^2
\right) \right)
= M(\xi_i,\eta_i)\, h^2
\end{align*}
for some $\xi_i$ and $\eta_i$ between $t_i$ and $t_{i+1}$, where
\[
  M(\xi,\eta) = \left( \frac{1}{6}\,y^{(3)}(\xi)
-\frac{1}{4}\,y^{(3)}(\eta) \right) \ .
\]
If $f$ is twice continuously differentiable on $[t_0,t_f]\times\RR$, then
$|y^{(3)}(t)|$ is continuous on $[t_0,t_f]$ and reaches is maximum at a point
on the interval $[t_0,t_f]$.  Let $K$ be the maximum of $|y^{(3)}(t)|$ on
$[t_0,t_f]$, then
\[
|\tau_{i+1}(h)| \leq \tau(h) \equiv
\left(\frac{1}{6}+\frac{1}{4} \right) K h^2
= \frac{5 K }{12}\, h^2  = O(h^2)
\]
for all $i$.  Hence, the trapezoidal method is of order $2$.

\begin{rmk}
The Trapezoidal Method is part of a family of methods called the
{\bfseries Theta Method}\index{Theta Method}.

We consider (\ref{ODE}) with the usual partition
$a=t_0 < t_1 < \ldots < t_N = t_f$ and $h=(t_f-t_0)/N$.
The Theta Method is defined by
\begin{align*}
w_{i+1} &= w_i + h \left( (1-\theta)\,f(t_{i+1},w_{i+1}) +
\theta\,f(t_i,w_i) \right)
\quad, \quad 1 \leq i < N \\
w_0 &= y(t_0)
\end{align*}
If $\theta = 0$, we get the
{\bfseries Backward Euler's method}\index{Backward Euler's Method}.  If 
$\theta = 1/2$, we get the Trapezoidal Method.   If
$\theta = 1$, we get the Euler's method.

We compute the local truncation error of the Theta Method with the help of
the Taylor series expansions of $y(t)$ and $y'(t)$ for $t$ near $t_i$.
We have
\begin{align*}
\tau_{i+1}(h) &= \frac{y(t_{i+1}) - y(t_i)}{h} - \big(
\theta\,f(t_i,y(t_i)) + (1-\theta)\,f(t_{i+1},y(t_{i+1})) \big) \\
&= \frac{y(t_{i+1}) - y(t_i)}{h} -
\big( \theta\,y'(t_i) + (1-\theta)\,y'(t_{i+1}) \big) \\
&= \frac{ \big( y(t_i) + y'(t_i)\,h + y''(t_i)\,h^2/2 +
y^{(3)}(\xi_i)\,h^3/6 \big) - y(t_i)}{h} \\
& \quad - \left( \theta\,y'(t_i) + (1-\theta)
\left(  y'(t_i) + y''(t_i)\,h + \frac{1}{2}\,y^{(3)}(\eta_i)\,h^2
\right) \right) \\
&= \left( \theta-\frac{1}{2} \right)\,y''(t_i)\,h +
\left( \frac{1}{6}\,y^{(3)}(\xi_i) + \frac{\theta -1}{2}\,y^{(3)}(\eta_i)
\right) \, h^2
\end{align*}
for some $\xi_i$ and $\eta_i$ between $t_i$ and $t_{i+1}$.
If we assume that $f$ is twice continuously differentiable on
$[t_0,t_f]\times \RR$, then $|y''(t)|$ and $|y^{(3)}(t)|$ are
continuous on $[t_0,t_f]$ and reaches their maximum at a point on the
interval $[t_0,t_f]$.  Let $M_2$ be the maximum of $|y''(t)|$ on
$[t_0,t_f]$ and $M_3$ be the maximum of $|y^{(3)}(t)|$ on $[t_0,t_f]$.
For $\theta = 1/2$, we have
\[
|\tau_{i+1}(h)|
= \left| \frac{1}{6}\,y^{(3)}(\xi) + \frac{|\theta -1|}{2}\,y^{(3)}(\eta)
\right| h^2 \leq \tau(h) \equiv
\left( \frac{1}{6} + \frac{1}{2} \right) M_3 \, h^2
= O(h^2) \ .
\]
Hence, the Theta Method with $\theta = 1/2$ is of order $2$.
However, for $\theta \neq 1/2$, we have
\begin{align*}
|\tau_{i+1}(h)|
& = \left| \left( \theta-\frac{1}{2} \right)\,y''(t_i)\,h
+ \left( \frac{1}{6}\,y^{(3)}(\xi) + \frac{|\theta -1|}{2}\,y^{(3)}(\eta)
\right) h^2 \right| \\
&\leq \tau(h) \equiv
\frac{1}{2}\,M_2 h + \left( \frac{1}{6} + \frac{1}{2} \right) M_3 \, h^2
= \left(\frac{M_2}{2} + \frac{2M_3}{3}\,h \right) h
= O(h) \ .
\end{align*}
Hence, the Theta Methods with $\theta \neq 1/2$ are of order $1$.
The Trapezoidal Method is the best method of this family.
\end{rmk}

\subsection{General Approach}

The general procedure to derive explicit multistep methods is as
follows.  We assume that $m$, $N$, $h$ and $t_i$ are as in
Definition~\ref{GFMSM}.

We consider the
{\bfseries Newton backward divided difference formula}\index{Newton
Backward Divided Difference Formula} 
of the interpolating polynomial $p$ of $g(t) = f(t,y(t))$ at
$t_i$, $t_{i-1}$, \ldots, $t_{i-m}$.  Namely,
\begin{equation} \label{NBDDF}
\begin{split}
p(t) & = g[t_i] + g[t_i,t_{i-1}](t-t_i) +
g[t_i,t_{i-1},t_{i-2}](t-t_i)(t-t_{i-1}) + \ldots \\
 &+ g[t_i,t_{i-1},\ldots,t_{i-m}](t-t_i)(t-t_{i-1})\ldots(t-t_{i-m+1}) \; .
\end{split}
\end{equation}
We have
\begin{equation} \label{errorNBDDF}
g(t) = p(t) +
g[t_i,t_{i-1},\ldots,t_{i-m},t]\prod_{j=i-m}^i\,(t-t_j) \;  .
\end{equation}

If we substitute $t=t_i + sh$ in (\ref{NBDDF}), we get
\begin{equation} \label{NBDF}
p(t) = \sum_{j=0}^m\,(-1)^j \binom{-s}{j}\nabla^j g_i \;  ,
\end{equation}
where $g_k = g(t_k)$ for $0 \leq k \leq N$,
\[
\binom{r}{j} = \begin{cases}
  1 & \text{ if $j=0$} \\
\displaystyle \frac{r(r-1)(r-2)\ldots(r-j+1)}{j!} & \text{ if $j>0$}
  \end{cases}
\]
for $r \in \RR$, and $\nabla^k g_i$ for $k \in \NN$ is the $k^{th}$
{\bfseries backward difference}\index{Backward Difference} of $g_i$ defined by
\begin{align*}
\nabla g_i &= g_i - g_{i-1} \; , \\
\nabla^2 g_i &= \nabla (g_i - g_{i-1})
= \nabla g_i - \nabla g_{i-1} = g_i - 2 g_{i-1} + g_{i-2}
\end{align*}
and in general
\[
\nabla^k g_i = \nabla^{k-1}\left(\nabla g_i \right)
\]
for $k>1$.  (\ref{NBDF}) is called the
{\bfseries Newton backward difference formula}\index{Newton Backward
Difference Formula} for the interpolating 
polynomial of $g$ at $t_i$, $t_{i-1}$, \ldots, $t_{i-m}$.

If we substitute $t=t_i + sh$ in the error term of the polynomial
interpolation $p$ of $g$ given in (\ref{errorNBDDF}), we get
\[
g[t_i,t_{i-1},\ldots,t_{i-m},t]\prod_{j=i-m}^i\,(t-t_j) =
(-1)^{m+1} \binom{-s}{m+1} g^{(m+1)}(t_i+\eta_i(s) h) h^{m+1}
\]
for some $\eta_i(s)$ in the smallest interval containing $s$, $0$,
$-1$, \ldots, $-m$; namely, $t_i+\eta_i(s)h$ is in the smallest
interval containing $t$, $t_i$, $t_{i-1}$, \ldots , $t_{i-m}$.

Given $0\leq q \leq m$, since
\[
y_{i+1} - y_{i-q} = \int_{t_{i-q}}^{t_{i+1}} y'(t) \dx{t}
= \int_{t_{i-q}}^{t_{i+1}} g(t)\dx{t} \ ,
\]
we get
\begin{equation} \label{explformula}
\begin{split}
y_{i+1} - y_{i-q} &
= h \sum_{j=0}^m\,(-1)^j \nabla^j g_i \int_{-q}^1  \binom{-s}{j} \dx{s} \\
&\qquad  + (-1)^{m+1}  h^{m+2}\int_{-q}^1 \binom{-s}{m+1}
g^{(m+1)}(t_i+\eta_i(s)h) \dx{s} \ .
\end{split}
\end{equation}
The explicit multistep methods comes from this formula if we ignore
the local discretization error
\[
h^{m+2}\int_{-q}^1 (-1)^{m+1} \binom{-s}{m+1} g^{(m+1)}(t_i+\eta_i(s)h) \dx{s} \ .
\]

The case $m=3$ and $q=0$ in (\ref{explformula}) gives
\begin{align*}
y_{i+1} & = y_i + \frac{h}{24} \left( 55 \, f(t_i.y_i) - 59 \,
f(t_{i-1},y_{i-1}) + 37\, f(t_{i-2},y_{i-2}) - 9 \,f(t_{i-3},y_{i-3})
\right) \\
&\qquad + (251/720)\,y^{(5)}(\xi_i)\,h^5
\end{align*} 
for some $\xi_i \in [t_{i-3},t_{i+1}]$ and $3 \leq i < N$.
We had to use the Mean Value Theorem for Integrals, Theorem~\ref{Th4},
to get the discretization error $(251/720)\,y^{(5)}(\xi_i)\,h^5$; namely,
\begin{align*}
\int_0^1 (-1)^4 \binom{-s}{4} g^{(4)}(t_i+\eta_i(s)h) \dx{s}
&=
\int_0^1 (-1)^4 \frac{(-s)(-s-1)(-s-2)(-s-3)}{4!}
y^{(5)}(t_i+\eta_i(s)h) \dx{s} \\
&= \frac{1}{24} \int_0^1 \underbrace{s(s+1)(s+2)(s+3)}_{\geq 0} \,
y^{(5)}(t_i+\eta_i(s)h) \dx{s} \\
&= \frac{1}{24}\, y^{(5)}(t_i+\tilde{\eta}_ih) \int_0^1 s(s+1)(s+2)(s+3) \dx{s}
\end{align*}
for some $\tilde{\eta}_i \in [-3,1]$.
If we let $\xi_i = t_i+\tilde{\eta}_i h$ and compute the integral, we get\\
$(251/720)\,y^{(5)}(\xi_i)\,h^5$.

We get the following famous explicit method.

\begin{defn}[Adams-Bashforth Method of Order Four]
Let $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for $i=0$,
$1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the
solution of the difference equation
\begin{align*}
w_{i+1} & = w_i + \frac{h}{24} \left( 55 \, f(t_i.w_i) - 59 \,
f(t_{i-1},w_{i-1}) + 37\, f(t_{i-2},w_{i-2}) \right. \\
&\qquad \left. - 9 \,f(t_{i-3},w_{i-3}) \right)
\quad, \quad 3 \leq i < N \\
w_i & =y_i \quad, \quad 0\leq i < 4
\end{align*}
The local truncation error $\tau_{i+1}(h)$ is
$(251/720)\,y^{(5)}(\xi_i)\,h^4$ for some
$\xi_i \in [t_{i-3},t_{i+1}]$ and $3 \leq i < N$.
\label{ABFSM}
\end{defn}

The procedure to derive implicit multistep methods is as
follows.  We assume that $m$, $N$, $h$ and $t_i$ are as in
Definition~\ref{GFMSM}.

We consider the Newton backward divided difference formula of the
interpolating polynomial $p$ of $g(t) = f(t,y(t))$ at $t_{i+1}$,
$t_i$, \ldots, $t_{i-m}$.  Namely,
\begin{equation} \label{implNBDDF}
\begin{split}
p(t) & = g[t_{i+1}] + g[t_{i+1},t_i](t-t_{i+1}) +
g[t_{i+1},t_i,t_{i-1}](t-t_{i+1})(t-t_i) + \ldots \\
 &+ g[t_{i+1},t_i,\ldots,t_{i-m}](t-t_{i+1})(t-t_i)\ldots(t-t_{i-m+1}) \ .
\end{split}
\end{equation}
We have
\begin{equation} \label{implerrorNBDDF}
g(t) = p(t) +
g[t_{i+1},t_i,\ldots,t_{i-m},t]\prod_{j=i-m}^{i+1}\,(t-t_j) \ .
\end{equation}

If we substitute $t=t_i + sh$ in (\ref{implNBDDF}), we get
\[
p(t) = \sum_{j=0}^{m+1}\,(-1)^j \binom{1-s}{j}\nabla^j g_{i+1} \ .
\]

If we substitute $t=t_i + sh$ in the error term of the polynomial
interpolation $p$ of $g$ given in (\ref{implerrorNBDDF}), we get
\[
g[t_{i+1},t_i,\ldots,t_{i-m},t]\prod_{j=i-m}^{i+1}\,(t-t_j) =
(-1)^{m+2} \binom{1-s}{m+2} g^{(m+2)}(t_i+\eta_i(s)h) h^{m+2}
\]
for some $\eta_i(s)$ in the smallest interval containing
$s$, $1$, $0$, $-1$, \ldots, $-m$.

Given $0\leq q \leq m$, since
\[
y_{i+1} - y_{i-q} = \int_{t_{i-q}}^{t_{i+1}} y'(t)\dx{t}
= \int_{t_{i-q}}^{t_{i+1}} g(t)\dx{t} \ ,
\]
we get
\begin{equation} \label{implformula}
\begin{split} 
y_{i+1} - y_{i-q} &= 
h \sum_{j=0}^{m+1}\,(-1)^j \nabla^j g_{i+1} \int_{-q}^1  \binom{1-s}{j}
\dx{s} \\
& \qquad + (-1)^{m+2} h^{m+3} \int_{-q}^1 \binom{1-s}{m+2}
g^{(m+2)}(t_i+\eta_i(s)h) \dx{s} \ .
\end{split}
\end{equation}
The implicit multistep methods comes from this formula if we ignore
the local discretization error
\[
h^{m+3} \int_{-q}^1 (-1)^{m+2} \binom{1-s}{m+1}
g^{(m+2)}(t_i+\eta_i(s)h) \dx{s} \ .
\]

The case $m=2$ and $q=0$ in (\ref{implformula}) gives
\begin{align*}
y_{i+1} & = y_i + \frac{h}{24} \left( 9\, f(t_{i+1},y_{i+1}) + 19 \,
f(t_i.y_i) - 5 \,f(t_{i-1},y_{i-1}) + f(t_{i-2},y_{i-2})\right) \\
&\qquad  -(19/720) \,y^{(5)}(\xi_i)\,h^5
\end{align*}
for some $\xi_i \in [t_{i-2},t_{i+1}]$ and $2 \leq i < N$.
As for the previous explicit method, we had to use the Mean Value
Theorem for Integrals to get the discretization error
$-(19/720) \,y^{(5)}(\xi_i)\,h^5$.

We get the following famous implicit method.

\begin{defn}[Adams-Moulton Method of Order Four]
Let $h=(t_f-t_0)/N$, $t_i=t_0+ih$ and $y_i = y(t_i)$ for $i=0$,
$1$, $2$, \ldots, $N$.  The approximation $w_i$ of $y_i$ is the
solution of the difference equation
\begin{align*}
w_{i+1} & = w_i + \frac{h}{24} \left( 9\, f(t_{i+1},w_{i+1}) + 19 \,
f(t_i.w_i)  - 5 \,f(t_{i-1},w_{i-1}) \right . \\
& \qquad \left. + f(t_{i-2},w_{i-2})\right) \quad, \quad 2 \leq i < N \\
w_i &= y_i \quad, \quad 0 \leq i < 3
\end{align*}
The local truncation error $\tau_{i+1}(h)$ is
$-(19/720) \,y^{(5)}(\xi_i)\,h^4$ for some
$\xi_i \in [t_{i-2},t_{i+1}]$ and $2 \leq i < N$.
\label{AMTSM}
\end{defn}

By varying $m$ and $q$ in (\ref{explformula}) and
(\ref{implformula}), we can find many more multistep methods.

\begin{egg}
It is generally impossible to solve explicitly for $w_{i+1}$ the
finite difference equations of the implicit multistep methods.  For instance,
the Adams-Moulton method of order four applied to the initial value
problem
\begin{align*}
y'(t) &= e^{y(t)} \quad, \quad 0\leq t \leq 0.25 \\
y(0) &= 1
\end{align*}
gives the equation
\[
w_{i+1} = w_i + \frac{h}{24}\left(9e^{w_{i+1}} + 19e^{w_i} -
  5e^{w_{i-1}} + e^{w_{i-2}} \right)
\]
which cannot be solved explicitly for $w_{i+1}$.
\end{egg}

\begin{rmkList}
\begin{enumerate}
\item Iterations are used to find the approximation $w_{i+1}$ of
$y_{i+1}$ in the implicit multistep methods (\ref{MULTISTEP}).
Suppose that a first approximation
$\displaystyle w_{i+1}^{[0]}$ of $w_{i+1}$ is given
--- We will provide in the next section a method to obtain a first
approximation.  The solution $w_{i+1}$ of (\ref{MULTISTEP}) is
approximated using the iterative system
\begin{equation} \label{MULTI_ITER}
\begin{split}
w_{i+1}^{[k+1]} &= \sum_{j=0}^m a_j w_{i-j}
+ h b_{-1} f\left(t_{i+1}, w_{i+1}^{[k]}\right)
+ h \sum_{j=0}^m  b_{j} f(t_{i-j}, w_{i-j})
\end{split}
\end{equation}
for $k=0$, $1$, \ldots

We now show that if $h$ is small enough such that $|b_{-1}h| L <1$,
where $L$ is the Lipschitz constant associated to $f$ as in
(\ref{Lipschitz}), then
$\displaystyle \left\{ w_{i+1}^{[k]}\right\}_{k=0}^\infty$ converges
to the unique solution $w_{i+1}$ of (\ref{MULTISTEP}).  The reader
will recognize that the following proof is ``basically identical'' to
the proof of the Fixed Point Theorem, Theorem~\ref{FxPtTh}.

The iterative system (\ref{MULTI_ITER}) can be rewritten as
\begin{equation} \label{MultistepSF}
w_{i+1}^{[k+1]} = A_i + h b_{-1} G_i\left(w_{i+1}^{[k]}\right) + h F_i \ ,
\end{equation}
where
\[
  A_i = \sum_{j=0}^m a_j w_{i-j} \quad \text{and} \quad
  F_i = \sum_{j=0}^m b_j f(t_{i-j}.w_{i-j})
\]
are constant, and
$\displaystyle G_i\left(w_{i+1}^{[k]}\right)
= f\left(t_{i+1},w_{i+1}^{[k]}\right)$.

We first prove that if there is a solution to
\begin{equation} \label{MultistepSFsol}
w = A_i + h b_{-1} G_i(w) + h F_i \ ,
\end{equation}
then it is unique.  Suppose that $w$ and $w^\ast$ are two distinct
solutions of (\ref{MultistepSFsol}); namely, if
\begin{align*}
w &= A_i + b_{-1} h G_i(w) + h F_i\\
\intertext{and}
w^\ast &= A_i + b_{-1} h G_i(w^\ast) + h F_i \ .
\end{align*}
Then
\[
|w - w^\ast| = |b_{-1} h (G_i(w) - G_i(w^\ast))|
\leq |b_{-1} h| L |w -w^\ast| < |w - w^\ast| \ .
\]
This is a contradiction.

We prove that the sequence
$\displaystyle \left\{ w_{i+1}^{[k]} \right\}_{k=0}^\infty$
defined by (\ref{MultistepSF}) converges.  In fact, we prove that it
is a Cauchy sequence.  Let $\epsilon$ be a small number.  We find a
positive integer $N$  such that
$\displaystyle \left|w_{i+1}^{[r]} - w_{i+1}^{[s]} \right|< \epsilon$ whenever 
$r,s \geq N$.

First, we prove by induction that
\begin{equation} \label{multStepFPa}
\left| w_{i+1}^{[k+1]} - w_{i+1}^{[k]} \right|
\leq |h b_{-1} L|^k\, \left|w_{i+1}^{[1]} - w_{i+1}^{[0]} \right| \ .
\end{equation}
for all $k$.  We have that (\ref{multStepFPa}) is obviously true for
$k=0$.  Suppose that (\ref{multStepFPa}) is true for $k$, then
\begin{align*}
\left|w_{i+1}^{[k+2]} - w_{i+1}^{[k+1]}\right| &=
\left|h b_{-1} \left( G\left(w_{i+1}^{[k+1]}\right)
- G\left(w_{i+1}^{[k]} \right) \right)\right|
\leq |h b_{-1} L|\ \left| w_{i+1}^{[k+1]} - w_{i+1}^{[k]} \right| \\
&\leq |h b_{-1} L|\ |h b_{-1} L|^k \left|w_{i+1}^{[1]} - w_{i+1}^{[0]} \right|
= |h b_{-1} L|^{k+1} \left| w_{i+1}^{[1]} -w_{i+1}^{[0]} \right| \ ,
\end{align*}
where the first inequality comes from the Lipschitz continuity of $G$
and the second inequality comes from the hypothesis of induction.  Hence,
(\ref{multStepFPa}) is true for $k+1$.  This complete the proof by
induction.

Hence, if $|h b_{-1} L|<1$ and
\[
r > s \geq N >
\frac{\ln(\epsilon) + \ln(1 - |h b_{-1} L|) - 
\ln\left(\left|w_{i+1}^{[1]} - w_{i+1}^{[0]} \right|\right)}
{\ln(|h b_{-1} L|)} \ ,
\]
we have
\begin{align*}
&\left|w_{i+1}^{[r]} - w_{i+1}^{[s]} \right| \leq
\left| w_{i+1}^{[r]} - w_{i+1}^{[r-1]}\right| +
\left| w_{i+1}^{[r-1]} - w_{i+1}^{[r-2]} \right| + \ldots +
\left| w_{i+1}^{[s+1]} - w_{i+1}^{[s]} \right| \\
&\quad \leq \left( |h b_{-1} L|^{r-s-1} + 
|h b_{-1} L|^{r-s-2} + \ldots +
|h b_{-1} L| +  1 \right)
|h b_{-1} L|^s \left|w_{i+1}^{[1]} - w_{i+1}^{[0]} \right| \\
&\quad = \frac{ 1- |h b_{-1} L|^{r-s}}{1-|h b_{-1} L|} 
|h b_{-1} L|^s \left|w_{i+1}^{[1]} - w_{i+1}^{[0]} \right|
\leq \frac{|h b_{-1} L|^s}{1-|h b_{-1} L|}
\left| w_{i+1}^{[1]} - w_{i+1}^{[0]} \right|  < \epsilon
\end{align*}

Finally, let $w_{i+1}$ be the limit of
$\displaystyle \left\{ w_{i+1}^{[k]} \right\}_{k=0}^\infty$.  We show that
$w_{i+1}$ is a solution of (\ref{MultistepSFsol}).  Since $G$ is a
continuous function, if we take the limit with respect to $k$ on both
sides of (\ref{MultistepSF}), we get
\[
w_{i+1} = \lim_{k\to \infty} w_{i+1}^{[k+1]} =
\lim_{k\to \infty} \left( A_i + h b_{-1}
G_i\left( w_{i+1}^{[k]} \right) + h F_i\right)
= A_i + h b_{-1} G_i(w_{i+1}) + h F_i \ .
\]
\label{bm1hLone}
\item Implicit multistep methods may seem to be inefficient methods to
find an approximation $w_i$ of $y_i$ because iterations have to be
done to find this approximation.  However, for some multistep methods,
the number of iterations necessary to find a good approximation
of $y_i$ is small and the step-size $h$ can be taken relatively
large.  Moreover, implicit multistep methods are usually
``stable'' as we will see later.  They are also very useful to solve
``stiff'' differential equations as we will also see soon.
\end{enumerate}
\label{bm1hL}
\end{rmkList}

\begin{rmk}
Instead of using the Newton backward divided difference formula
of the interpolating polynomial $p$ of $g(t) = f(t,y(t))$ to derive
explicit and implicit multistep methods to approximate the solution of
(\ref{ODE}), we can use the Lagrange Interpolating Polynomial
\[
p(t) = \sum_{j=i-m}^i\left( f(t_j,y(t_j)) \prod_{\substack{k=i-m\\k\neq j}}^i
\left(\frac{t-t_k}{t_j-t_k}\right) \right)
\]
to get the formula
\[
w_{i+1} = w_{i-q} + \sum_{j=i-m}^i \left( f(t_j,w_j)
\int_{t_{i-q}}^{t_{i+1}} \prod_{\substack{k=i-m\\k\neq j}}^i
\left(\frac{t-t_k}{t_j-t_k}\right) \dx{t} \right)
\]
for the explicit multistep methods, and the Lagrange Interpolating
Polynomial
\[
p(t) = \sum_{j=i-m}^{i+1}\left( f(t_j,y(t_j))
\prod_{\substack{k=i-m\\k\neq j}}^{i+1} \left(\frac{t-t_k}{t_j-t_k}\right) \right)
\]
to get the formula
\[
w_{i+1} = w_{i-q} +
\sum_{j=i-m}^{i+1} \left( f(t_j,w_j)
\int_{t_{i-q}}^{t_{i+1}} \prod_{\substack{k=i-m\\k\neq j}}^{i+1}
\left(\frac{t-t_k}{t_j-t_k}\right) \dx{t} \right)
\]
for the implicit multistep methods.  The two integrals above can be
computed using the substitution $t = t_i + sh$.  We get respectively
\begin{align*}
\int_{t_{i-q}}^{t_{i+1}} \prod_{\substack{k=i-m\\k\neq j}}^i
\left(\frac{t-t_k}{t_j-t_k}\right) \dx{t}
&= h\int_{-q}^1 \prod_{\substack{k=0\\k\neq i-j}}^m
\left(\frac{t_i+sh -t_{i-k}}{t_{i-(i-j)}-t_{i-k}}\right) \dx{s}
= h \int_{-q}^1 \prod_{\substack{k=0\\k\neq i-j}}^m
\left(\frac{s+k}{-(i-j)+k}\right) \dx{s} \\
& = \frac{h (-1)^{i-j}}{(i-j)!\,(m-i+j)!}
\int_{-q}^1 \prod_{\substack{k=0\\k\neq i-j}}^m (s+k) \dx{s}
\end{align*}
after substituting $k$ by $i-k$ and noting that $0 \leq i-j\leq m$,
and
\begin{align*}
&\int_{t_{i-q}}^{t_{i+1}} \prod_{\substack{k=i-m\\k\neq j}}^{i+1}
\left(\frac{t-t_k}{t_j-t_k}\right) \dx{t}
= h\int_{-q}^1 \prod_{\substack{k=0\\k\neq i-j+1}}^{m+1}
\left(\frac{t_i+sh -t_{i-k+1}}{t_{i-(i-j)}-t_{i-k+1}}\right) \dx{s} \\
&\qquad = h \int_{-q}^1 \prod_{\substack{k=0\\k\neq i-j+1}}^{m+1}
\left(\frac{s+k-1}{-(i-j+1)+k}\right) \dx{s}
= \frac{h (-1)^{i-j+1}}{(i-j+1)!\,(m-i+j)!}
\int_{-q}^1 \prod_{\substack{k=0\\k\neq i-j+1}}^{m+1} (s+k-1) \dx{s} \ .
\end{align*}
after substituting $k$ by $i-k+1$ and noting that
$0 \leq i-j+1\leq m+1$.

This approach obviously yields the same formulae than those found with
the approach that we have chosen.  However, it does not provide the
local truncation error that we have been able to find with our
approach.
\end{rmk}

\subsection{Another Approach to Multistep Methods}\label{AAMM}

There is still another approach to develop multistep methods based on
the following theorem.

\begin{theorem}
The multistep method (\ref{GFMSM}) is of order $p\geq 1$ if and only
if there exists $c \neq 0$ such that
\[
p(w) - q(w) \,\ln(w) = c(w-1)^{p+1} + O((w-1)^{p+2})
\]
for $w$ near $1$, where
\[
p(w) =  w^{m+1} - \sum_{j=0}^m\,a_j\,w^{m-j} \quad\text{and} \quad
q(w) = \sum_{j=-1}^m\,b_j\,w^{m-j} \ .
\]
The polynomial $p$ is called the
{\bfseries characteristic polynomial}\index{Multistep
Methods!Characteristic Polynomial} of the multistep method. 
\label{link_order_char}
\end{theorem}

The beauty of this approach is, as we will see in
Section~\ref{CCSyuk}, that the polynomials $p$ and $q$
in the previous theorem play in important role in the study of
``consistency'', ``stability'' and ``convergence'' of numerical
methods to approximate solutions of ordinary differential equations.

\begin{proof}
As usual, we assume that $f$ is smooth enough such that we
can express the solution $y$ of (\ref{ODE}) as a Taylor series of
radius at least $m h$ about any point $t_i \in [a,b]$.  Hence, since
$f(t_i,y(t_i)) = y'(t_i)$, we get
\begin{align*}
&h\,\tau_{i+1}(h) = y(t_i+h) - \sum_{j=0}^m\,a_j\,y(t_i-jh)
-h\,\sum_{j=-1}^m\,b_j\,f(t_i-jh,y(t_i-jh)) \\
&\quad =
\sum_{k=0}^\infty\,\frac{1}{k!}\,y^{(k)}(t_i)\,h^k - \sum_{j=0}^m\,a_j
\left( \sum_{k=0}^\infty\,\frac{1}{k!}\,y^{(k)}(t_i)\,(-j)^kh^k \right)
-h \sum_{j=-1}^m\,b_j
\left(\sum_{k=0}^\infty\,\frac{1}{k!}\,y^{(k+1)}(t_i)\,(-j)^kh^k \right)  \\
&\quad = \left( 1 - \sum_{j=0}^m\,a_j \right) y(t_i) +
\left( 1 + \sum_{j=0}^m\,a_j\, j - \sum_{j=-1}^m b_j \right) y'(t_i) h \\
&\quad\qquad
+ \sum_{k=2}^\infty\,\frac{1}{k!}\,\left(1 - (-1)^k\sum_{j=0}^m\,a_j\,j^k\right) 
y^{(k)}(t_i)\,h^k 
-h\sum_{k=1}^\infty\,\frac{1}{k!}\,\left( (-1)^k\sum_{j=-1}^m\,b_j\,
j^k\right) y^{(k+1)}(t_i)\,h^k\\
&\quad = \left( 1 - \sum_{j=0}^m\,a_j \right) y(t_i) +
\left( 1 + \sum_{j=0}^m\,a_j\, j - \sum_{j=-1}^m b_j \right) y'(t_i) h \\
&\quad\qquad
+ \sum_{k=2}^\infty\,\frac{1}{k!}\,\bigg( 1 - (-1)^k \sum_{j=0}^m\,a_j\,j^k
- (-1)^{k-1} k \sum_{j=-1}^m\,b_j\, j^{k-1}\bigg) y^{(k)}(t_i)\,h^k \ .
\end{align*}
Thus, (\ref{GFMSM}) is of order $p\geq 1$ if and only if
\begin{equation} \label{cond_order_p1}
1 - \sum_{j=0}^m\,a_j = 0 \quad \text{and} \quad
1 - (-1)^k \sum_{j=0}^m\,a_j\,j^k - (-1)^{k-1}k\,\sum_{j=-1}^m\,b_j\,j^{k-1} = 0
\end{equation}
for $1 \leq k \leq p$, and
\begin{equation} \label{cond_order_p2}
c \equiv 1 - (-1)^{p+1} \sum_{j=0}^m\,a_j\,j^{p+1}
- (-1)^p (p+1)\,\sum_{j=-1}^m\,b_j\,j^p \not= 0 \ .
\end{equation}
In this case, we have that
\[
h\,\tau_{i+1}(h) = c\, \frac{y^{(p+1)}(t_i)}{(p+1)!}\,h^{p+1} + O(h^{p+2}) \ .
\]

Moreover, if we set $w=e^x$, we get
\begin{align*}
&\frac{p(w)-q(w)\,\ln(w)}{e^m} = e^x
-\sum_{j=0}^m\,a_j\,e^{-jx} - x\,\sum_{j=-1}^m\,b_j\,e^{-jx} \\
&\quad = \sum_{k=0}^\infty\,\frac{1}{k!}\, x^k
- \sum_{j=0}^m\,a_j\left(\sum_{k=0}^\infty\,\frac{1}{k!}\,(-j)^k x^k \right) -
x\,\sum_{j=-1}^m\,b_j\left( \sum_{k=0}^\infty\,\frac{1}{k!}\,(-j)^k x^k
\right) \\
&\quad = \left( 1 - \sum_{j=0}^m\,a_j \right)
+ \left( 1 + \sum_{j=0}^m\,a_j\, j - \sum_{j=-1}^m\,b_j \right) x \\
&\quad\qquad + \sum_{k=2}^\infty\,\frac{1}{k!}\left( 1 - (-1)^k
\sum_{j=0}^m\,a_j\,j^k\right) x^k
-\sum_{k=1}^\infty \frac{1}{k!} \left( (-1)^k\sum_{j=-1}^m\,b_j\,j^k
\right) x^{k+1} \\
&\quad = \left( 1 - \sum_{j=0}^m\,a_j\right)
+ \left( 1 + \sum_{j=0}^m\,a_j\, j - \sum_{j=-1}^m\,b_j \right) x \\
&\quad\qquad
+ \sum_{k=2}^\infty\,\frac{1}{k!}\bigg(1 - (-1)^k \sum_{j=0}^m\,a_j\,j^k
- (-1)^{k-1} k\,\sum_{j=-1}^m\,b_j\,j^{k-1} \bigg) x^k \ .
\end{align*}
So,
\[
p(w)-q(w)\,\ln(w) = c\,x^{p+1} + O(x^{p+2})
= c\,(w-1)^{p+1} + O((w-1)^{p+2})
\]
if and only if (\ref{cond_order_p1}) and (\ref{cond_order_p2}) are
satisfied.  Recall that $x = \ln(w) = (w-1) + O((w-1)^2)$ for $w$ near
$1$.
\end{proof}

\begin{rmk}
We will see in Section~\ref{CCSyuk} that the condition
\[
  p(1) = 1 - \sum_{j=0}^m a_j = 0
\]
is necessary for the method to be ``consistent.''\  We will see that
consistency and the ``root condition'' imply that the method is
``convergent.''\   The ``root condition'' also implies
that the method is ``zero-stable.''\ But we are getting ahead of
ourselves and should go back to multistep methods.
\end{rmk}

\begin{egg}
The multistep method
\[
  w_{i+1} = w_i + h\left(\frac{23}{12} f(t_i,w_i) -\frac{4}{3}
    f(t_{i-1},w_{i-1}) +\frac{5}{12} f(t_{i-2},w_{i-2}) \right)
\quad, \quad 0 \leq i < N
\]
is of order $3$.  We use the previous theorem with $m=2$ to prove this
statement.  We have $p(w) = w^3-w^2$ and
\[
q(w) = \frac{23}{12} w^2 - \frac{4}{3} w + \frac{5}{12} \ .
\]
To develop $p(w) - q(w)\,\ln w$ near $w=1$, we set
$v = w-1$.  Hence
\begin{align*}
p(w) &= (v+1)^3 - (v+1)^2 = v^3 +2v^2+v \ , \\
q(w) &= \frac{23}{12} (v+1)^2 -\frac{4}{3} (v+1) +\frac{5}{12}
= \frac{23}{12}v^2+\frac{5}{2}v+1
\intertext{and}
p(w) - q(w)\,\ln w &= (v^3
+2v^2+v)-\left(\frac{23}{12}v^2+\frac{5}{2}v+1\right)
\left(\sum_{k=1}^\infty(-1)^{k+1} \frac{v^k}{k} \right) \\
&= \frac{3}{8}v^4 + O(v^5) = \frac{3}{8}(w-1)^4 + O((w-1)^5) \ .
\end{align*}
\end{egg}

We can use Theorem~\ref{link_order_char} to construct multistep
methods of any order.

To construct an explicit multistep method of order $p$, choose a
polynomial $\displaystyle p(w)= w^{m+1} - \sum_{j=0}^m\,a_j\,w^{m-j}$
such that $p(1)=0$.  The polynomial $q(w)$ is the polynomial of degree
$m$ (if such polynomial exists) given by the relation 
\[
p(w) - q(w)\,\ln(w) = O((w-1)^{p+1}) \ .
\]
To construct an implicit multistep method of order $p$, choose $p(w)$
as before but this time the polynomial $q(w)$ is the polynomial
of degree $m+1$ (if such a polynomial exists) given by the relation 
\[
p(w) - q(w)\,\ln(w) = O((w-1)^{p+1}) \ .
\]

As we will realize in the next examples, it is useful to note that
$\displaystyle \frac{v}{\ln(v+1)}$ can be defined at $v=0$ by the
extension
\[
g(v) = \begin{cases}
\displaystyle \frac{v}{\ln(v+1)} & \quad \text{if} \quad v \neq 0 \\
1 & \quad \text{if} \quad v = 0  
\end{cases}
\]
Moreover, 
\[
g(v) = 1 + \frac{v}{2} - \frac{v^2}{12} + \frac{v^3}{24} + O(v^4) \ .
\]

\begin{egg}
To construct an implicit method of order $4$ from $p(w) = w^3- w^2$,
let $v=w-1$.  Then
\begin{align*}
\frac{p(w)}{\ln(w)}
&= \frac{w^2(w-1)}{\ln(w)} = (v+1)^2\, \frac{v}{\ln(v+1)} \\
&= (v^2+2v+1)\left(1 + \frac{v}{2} - \frac{v^2}{12} + \frac{v^3}{24}
+ O(v^4) \right) \\
&= 1 + \frac{5}{2}\,v + \frac{23}{12}\,v^2 + \frac{3}{8} \, v^3
+ O(v^4) \\
&= \frac{1}{24} - \frac{5}{24}\,w + \frac{19}{24}\,w^2
+ \frac{3}{8} \, w^3 + O((w-1)^4) \ .
\end{align*}
Thus
\[
  q(w) = \frac{1}{24} - \frac{5}{24}\,w + \frac{19}{24}\,w^2
  + \frac{3}{8} \, w^3
\]
and the multistep method is
\[
w_{i+1} = w_i + h \left(
\frac{3}{8}\,f(t_{i+1},w_{i+1}) + \frac{19}{24}\,f(t_i,w_i)
-\frac{5}{24} f(t_{i-1},w_{i-1}) + \frac{1}{24}f(t_{i-2},w_{i-2}) \right)
\ , \ \leq i < N \ .
\]
This is our famous Adams-Moulton method of order four.
\end{egg}

\begin{egg}
To construct an explicit method of order $1$ from $p(w) = w^2-w$, let
$v=w-1$.  Then
\begin{align*}
\frac{p(w)}{\ln(w)} &= \frac{w(w-1)}{\ln(w)}
= (v+1)\, \frac{v}{\ln(v+1)}
= (v+1)\left(1 + \frac{v}{2} - O(v^2) \right) \\
&= 1 + \frac{3}{2}\,v + O(v^2)
= -\frac{1}{2} + \frac{3}{2} w + O(|w-1|^2) \ .
\end{align*}
Thus $\displaystyle q(w) = -\frac{1}{2} + \frac{3}{2} w$ and
the multistep method is
\[
w_{i+1} = w_i + h \left( \frac{3}{2} f(t_i,w_i)
-\frac{1}{2} f(t_{i-1},w_{i-1}) \right) \quad, \quad 1 \leq i < N \ .
\]
\end{egg}

\begin{rmk}
If we consider $p(w) = w^{m-1}(w-1)$,
we get the {\bfseries Adams methods}\index{Multistep Methods!Adams Methods}.
The implicit methods are called 
{\bfseries Adams-Moulton methods}\index{Multistep Methods!Adams-Moulton
Methods} and the explicit methods are called 
{\bfseries Adams-Bashforth methods}\index{Multistep Methods!Adams-Bashforth
Methods}.

If we consider $p(w) = w^{m-2}(w^2-1)$, the explicit methods (of order
$m$) are called
{\bfseries Nystron methods}\index{Multistep Methods!Nystron Methods}
and the implicit methods (of order $m+1$) are called
{\bfseries Milne methods}\index{Multistep Methods!Milne Methods}.
\end{rmk}

\subsection{Backward Difference Formulae}

\begin{defn}
A multistep method of the form (\ref{GFMSM}) and of order $m+1$ is
called a
{\bfseries Backward Difference Formula}\index{Multistep
Methods!Backward Difference Formula}
if $b_{-1} \neq 0$ and $b_j=0$ for $0 \leq j \leq m$.
\end{defn}

\begin{prop}
For a Backward Difference Formula, we have
$\displaystyle b_{-1} = \left(\sum_{j=1}^{m+1}\,\frac{1}{j} \right)^{-1}$
and the characteristic polynomial is
$\displaystyle p(w) = b_{-1}\sum_{j=1}^{m+1}\,\frac{1}{j}\,w^{m+1-j}(w-1)^j$.
\end{prop}

\begin{proof}
We will use Theorem~\ref{link_order_char}.  We have by hypothesis that
$q(w) = b_{-1} w^{m+1}$ for some non-zero constant $b_{-1} \in \RR$.  
Since the Backward Difference Formula is of order $m+1$, we have
\begin{equation} \label{backwardParg}
p(w) - b_{-1} w^{m+1} \ln(w) = O((w-1)^{m+2})
\end{equation}
for $w$ near $1$.  If we substitute $w=1/v$ in this equation and
multiply it by $v^{m+1}$, we get
\[
v^{m+1}p\left(\frac{1}{v}\right) = b_{-1} \ln(v) + O((v-1)^{m+2})
\]
for $v$ near $1$.  Since
\[
\ln(v) = \ln(1+(v-1)) = \sum_{j=1}^{m+1} \frac{(-1)^{j-1}}{j}\,(v-1)^j +
O((v-1)^{m+2})
\]
for $v$ near $1$, we get
\[
v^{m+1} p\left(\frac{1}{v}\right) =
b_{-1} \sum_{j=1}^{m+1}\,\frac{(-1)^j}{j}\,(v-1)^j + O((v-1)^{m+2}) \ .
\]
If we rewrite this equation in function of $w$, we get
\begin{align*}
p(w) &= b_{-1} w^{m+1} \sum_{j=1}^{m+1}\,\frac{(-1)^j}{j}\,(1-w)^j\,w^{-j} +
O((w-1)^{m+2}) \\
&= b_{-1} \sum_{j=1}^{m+1}\,\frac{1}{j}\,(w-1)^j\,w^{m+1-j} +
O((w-1)^{m+2})  \ .
\end{align*}
Since $p(w)$ is a polynomial of degree $m+1$ and any extra terms
of the form $(w-1)^k$ with $k\geq m+2$ will not affect
(\ref{backwardParg}), we may assume that
\[
p(w) = b_{-1} \sum_{j=1}^{m+1}\,\frac{1}{j}\,(w-1)^j\,w^{m+1-j} \ .
\]
To get $p$ of the form
$\displaystyle p(w) = w^{m+1} - \sum_{j=0}^m a_j w^{m-j}$, we need
$\displaystyle 1 = b_{-1} \sum_{j=1}^{m+1}\,\frac{1}{j}$.
\end{proof}

\begin{egg}
The case $m=0$ gives $b_{-1} = 1$ and $p(w) = w -1$.  We get the 
Backward Euler's method $w_{i+1} = w_i + h\,f(t_{i+1},w_{i+1})$ for
$0 \leq i < N$.

The case $m=1$ gives $b_{-1} = 2/3$ and
\[
p(w) = \frac{2}{3} \left( (w-1)w + \frac{1}{2} (w-1)^2\right)
= w^2 - \frac{4}{3}\, w + \frac{1}{3} \ .
\]
We get the backward method
\[
w_{i+1} = \frac{4}{3}\, w_i - \frac{1}{3} \, w_{i-1}
+ \frac{2}{3}\,h f(t_{i+1},w_{i+1}) \quad, \quad 1 \leq i < N \ .
\]
\end{egg}

\subsection{Predictor-Corrector Methods}

Since it is generally impossible to solve explicitly for $w_{i+1}$ the
finite difference equations of the implicit multistep methods, we do not use
implicit multistep methods to approximate $y_{i+1}$ but we use them
to improve the approximation of $y_{i+1}$ given by the explicit
methods.

The combination of an explicit and an implicit multistep method of
the same order gives a predictor-corrector method.  We illustrate this
idea with the Adams-Bashforth method of order four and the
Adams-Moulton method of order four.  Both are multistep methods of
order four.

\begin{algo}[Predictor-Corrector Method]
\begin{enumerate}
\item Use Runge-Kutta Method of order four to get approximations $w_i$,
of $y_i$ for $i=1$, $2$, and $3$.  Recall that $w_0 = y_0$.
\item (P) Suppose that we have found the approximation $w_i$ of $y_i$
for $i\geq 3$.  Use Adams-Bashforth formula to get a first approximation
\[
w_{i+1}^{[0]} = w_i + \frac{h}{24} \left( 55 \, f(t_i.w_i) - 59 \,
f(t_{i_1},w_{i-1}) + 37\, f(t_{i-2},w_{i_2}) - 9 \,f(t_{i-3},w_{i-3}) \right)
\]
of $y_{i+1}$. \label{PredictStep}
\item (C) Use Adams-Moulton formula to get a better (we hope)
approximation
\[
w_{i+1}^{[1]} = w_i + \frac{h}{24} \left( 9\, f\left(t_{i+1},w_{i+1}^{[0]}\right)
+ 19 \,f(t_i.w_i)  - 5 \,f(t_{i-1},w_{i-1}) + f(t_{i-2},w_{i-2})\right)
\]
of $y_{i+1}$.  Accept $\displaystyle w_{i+1}^{[1]}$ as the
approximation $w_{i+1}$ of $y_{i+1}$.
\item Go back to (\ref{PredictStep}) if $i<N$.
\end{enumerate}
\end{algo}

\begin{rmk}
Generally, no more than two iterations are done.  If the iterative
process does not give a ``good'' approximation after two
iterations.  The step-size is usually reduced.
\end{rmk}

We now look a little deeper into the theory to determine the order of
the predictor-corrector method resulting from combining two multistep
methods.

We consider two multistep methods to approximate the solution of
(\ref{ODE}).  The first method is an explicit method of order $p$
given by
\begin{equation} \label{Pred}
w_{i+1} = \sum_{j=0}^m\,a_j\,w_{i-j} + h \sum_{j=0}^m\,b_j\,f(t_{i-j},w_{i-j})
\end{equation}
for $m\leq i < N$ and the second method is an implicit method of order
$\tilde{p}$ given by
\begin{equation} \label{Corr}
\tilde{w}_{i+1} = \sum_{j=0}^{\tilde{m}}\,\tilde{a}_j\,\tilde{w}_{i-j}
+ h \sum_{j=-1}^{\tilde{m}}\,\tilde{b}_j\,f(t_{i-j},\tilde{w}_{i-j})
\end{equation}
for $\tilde{m} \leq i < N$.  We have that
$w_{i-j}$ is the approximation of $y(t_{i-j})$ given by
(\ref{Pred}) and $\tilde{w}_{i-j}$ is the approximation of
$y(t_{i-j})$ given by (\ref{Corr}).

We combine these two multistep methods to create a predictor-corrector
method as follows.

Let $M = \max\{m,\tilde{m}\}$.  Suppose that $w_0$, $w_1$, \ldots,
$w_M$ have been obtained from a method of order at least equal to
$\max\{p,\tilde{p}\}$.

Assuming that we have the values of $w_{i-j} = \tilde{w}_{i-j}$ and
$f_{i-j} = f(t_{i-j},w_{i-j})$ for $0\leq J \leq M$, we compute the value of
$w_{i+1}$ for $i\geq M$ as
follows.
\begin{description}
\item[P:] Prediction
\begin{equation} \label{Pred0}
w_{i+1}^{[0]} = \sum_{j=0}^m\,a_j\,w_{i-j} + h \sum_{j=0}^m\,b_j\,f_{i-j}
\end{equation}
\item[(EC)$^{\nu}$:] Evaluation and Correction for $k=0$, $1$, \ldots $\nu-1$.
\begin{align}
f_{i+1}^{[k+1]} &= f(t_{i+1},w_{i+1}^{[k]}) \nonumber \\
w_{i+1}^{[k+1]} &= \sum_{j=0}^{\tilde{m}}\,\tilde{a}_j\,w_{i-j} + h\left(
\sum_{j=0}^{\tilde{m}} \tilde{b}_j\,f_{i-j} + \tilde{b}_{i+1}\,f^{[k+1]}_{i+1}
\right)\label{Corr0}
\end{align}
We set $w_{i+1} = \tilde{w}_{i+1} = w_{i+1}^{[\nu]}$.
\item[E:] Evaluation
\[
f_{i+1} = f(t_{i+s},w_{i+s})
\]
\end{description}
This predictor-corrector method is named $P(EC)^{\nu}E$.  In general,
the number of iterations $\nu$ should be small.

In the proof of Theorem~\ref{link_order_char}, we have shown that a
multistep method of the form (\ref{GFMSM}) satisfies
\begin{equation}\label{GMSMdev}
\begin{split}
& y(t_i+h) - \sum_{j=0}^m\,a_j\,y(t_i-jh)
-h\,\sum_{j=-1}^m\,b_j\,f(t_i-jh,y(t_i-jh)) \\
&= \left( 1 - \sum_{j=0}^m\,a_j \right) y(t_i)
+ \left( 1 + \sum_{j=0}^m\,a_j\,j - \sum_{j=-1}^m\,b_j\right) y'(t_i)\,h \\
&\qquad
+ \sum_{k=2}^\infty\,\frac{1}{k!}\,\left( 1 - (-1)^k \sum_{j=0}^m\,a_j\,j^k
- (-1)^{k-1} k \sum_{j=-1}^m\,b_j\, j^{k-1}\right) y^{(k)}(t_i)\,h^k \ .
\end{split}
\end{equation}

To compute the order of the predictor-corrector method,
we make the localisation assumption that
$w_{i-j} = y_{i-j} = y(t_{i-j})$ for $0\leq j \leq m$.
Using (\ref{GMSMdev}), we find that our explicit multistep method
($b_{-1} =0$) of order $p$ satisfies
\begin{equation} \label{LE_Pred}
y(t_{i+1}) - w_{i+1}^{[0]} = C_p h^{p+1} y^{(p+1)}(t_i) + O(h^{p+2})
\ ,
\end{equation}
where
\[
C_p = \frac{1}{(p+1)!}\,\bigg( 1 + (-1)^p \sum_{j=0}^m\,a_j\,j^{p+1}
- (-1)^p (p+1) \sum_{j=0}^m\,b_j\, j^p\bigg) \ .
\]
Again, using (\ref{GMSMdev}), we find that our implicit multistep
method of order $\tilde{p}$ satisfies
\begin{align}
&y(t_{i+1}) - w_{i+1}^{[k+1]}
= h \tilde{b}_{-1} \left( f(t_{i+1},y(t_{i+1})) - f(t_{i+1},w_{i+1}^{[k]}) \right)
+ \big( y(t_{i+1}) - w_{i+1} \big) \nonumber \\
&=h \tilde{b}_{-1} \left( f(t_{i+1},y(t_{i+1})) - f(t_{i+1},w_{i+1}^{[k]}) \right)
+ \tilde{D}_{\tilde{p}} h^{\tilde{p}+1} y^{(\tilde{p}+1)}(t_i) +
O(h^{\tilde{p}+2}) \nonumber \\
&= h \tilde{b}_{-1} \,\pdydx{f}{y}(t_{i+s},\eta_i)
\left( y(t_{i+s}) - w_{i+s}^{[k]} \right)
+ \tilde{D}_{\tilde{p}} h^{\tilde{p}+1} y^{(\tilde{p}+1)}(t_i) + O(h^{\tilde{p}+2})
\label{LE_Corr} 
\end{align}
for some $\eta_{i,k}$ between $y(t_{i+1})$ and $w_{i+1}^{[k]}$, where
\[
\tilde{D}_{\tilde{p}} = \frac{1}{(\tilde{p}+1)!}\,\bigg( 1
+ (-1)^{\tilde{p}} \sum_{j=0}^{\tilde{m}}\,\tilde{a}_j\,j^{\tilde{p}+1}
- (-1)^{\tilde{p}} (\tilde{p}+1) \sum_{j=-1}^{\tilde{m}}\,\tilde{b}_j\,
j^{\tilde{p}}\bigg) \ .
\]

If $p \geq \tilde{p}$, we get from (\ref{LE_Corr}) with $k=0$ that
\begin{align*}
y(t_{i+1}) - w_{i+1}^{[1]} =
&= h \tilde{b}_{-1} \,\pdydx{f}{y}(t_{i+1},\eta_{i.0})
\underbrace{\left( y(t_{i+1}) - w_{i+1}^{[0]} \right)}_{=O(h^{p+1})}
+ \tilde{D}_p h^{\tilde{p}+1} y^{(\tilde{p}+1)}(t_i) +
O(h^{\tilde{p}+2}) \\
&= \tilde{D}_p h^{\tilde{p}+1} y^{(\tilde{p}+1)}(t_i) +
O(h^{\tilde{p}+2})
\end{align*}
because of (\ref{LE_Pred}).  If we substitute this result into
(\ref{LE_Corr}) with $k=1$, we get
\begin{align*}
y(t_{i+s}) - w_{i+s}^{[2]} =
&= h \tilde{b}_{-1} \,\pdydx{f}{y}(t_{i+1},\eta_{i,1})
\underbrace{\left( y(t_{i+1}) - w_{i+1}^{[1]} \right)}_{=O(y^{\tilde{p}+1})}
+ \tilde{D}_p h^{\tilde{p}+1} y^{(\tilde{p}+1)}(t_i) +
O(h^{\tilde{p}+2}) \\
&= \tilde{D}_p h^{\tilde{p}+1} y^{(\tilde{p}+1)}(t_i) +
O(h^{\tilde{p}+2}) \; .
\end{align*}
Proceeding this way with $k=2$, $3$, \ldots, $\nu-1$, we get
\[
y(t_{i+s}) - w_{i+s}^{[\nu]}
= \tilde{D}_p h^{\tilde{p}+1} y^{(\tilde{p}+1)}(t_i) +
O(h^{\tilde{p}+2}) \ .
\]
This shows that the principal part of the local truncation error (the
term in $h$ with the smallest exponent) for the predictor-corrector
method is given by the principal part of the corrector only.  The 
predictor-corrector method is of order $\tilde{p}$.

Proceeding as we have just done, we find that
\begin{enumerate}
\item If $p < \tilde{p}$ and $\nu \geq \tilde{p}-p$, the
predictor-corrector method has the same order as the corrector method.
However, the principal part of the local truncation error for the
predictor-corrector method is not the principal part of the local
truncation error for the corrector method.
\item If $p < \tilde{p}$ and $\nu < \tilde{p}-p$, the
predictor-corrector method is of order $p+\nu$.  Each iteration of the
implicit multistep method increases the order of the method by $1$.
\end{enumerate}

\subsection{Variable Step-Size Multistep methods}

We show how the predictor-corrector method of the previous section can
be adapted to control the step-size.

Let $\tilde{\tau}_{i+1}(h)$ be the local truncation error for the
Adams-Moulton method of order four.  Combining the
Adams-Bashforth Method of order four and the Adams-Moulton method of
order four, we can determine the step-size $h$ between $t_i$ and $t_{i+1}$
such that $\tilde{\tau}_{i+1}(h)<\epsilon$ where $\epsilon$ is given.

The following procedure outlines this variable step-size multistep
method based on the Adams-Moulton method of order four and
the Adams-Bashforth method of order four.

\begin{algo}[Variable Step-Size Multistep method]
\begin{enumerate}
\item Let $i=0$, $\tilde{t}_0 = t_0$ and $\tilde{w}_0 = y(t_0)$.
\item Use Runge-Kutta Method of order four starting with $\tilde{w}_0$
as approximation of $y(t_0)$ to get approximations $\tilde{w}_j$
of $y(t)$ at $\tilde{t}_j = \tilde{t}_0 + jh$ for $1 \leq j \leq 3$.
Let $w_{i-j} = \tilde{w}_{3-j}$ and $t_{i-j} = \tilde{t}_{3-j}$ for
$0 \leq j \leq 3$. \label{VSTMmethodI2}
\item Use Adams-Bashforth formula to get a first approximation
\[
w_{i+1}^{[0]} = w_i + \frac{h}{24} \left( 55 \, f(t_j.w_j)
- 59 \, f(t_{j-1},w_{j-1}) + 37\, f(t_{j-2},w_{j-2})
- 9 \,f(t_{j-3},w_{j-3}) \right)
\]
of $y(t)$ at $t_{j+1}=t_i+h$.  \label{VSTMmethodI3}.
\item Use Adams-Moulton formula to get a better (we hope)
approximation
\[
w_{i+1}^{[1]} = w_i + \frac{h}{24} \left( 9\, f\left(t_{i+1},w_{i+1}^{[0]}\right)
+ 19 \, f(t_i.w_i)  - 5 \,f(t_{i-1},w_{i-1}) + f(t_{i-2},w_{i-2})\right)
\]
of $y(t)$ at $t_{i+1}=t_i+h$.
\item If
\[
\frac{19}{270} \left| \frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h} \right|
< \epsilon \  ,
\]
we accept $\displaystyle w_{i+1} = w_{i+1}^{[1]}$, $w_i$, $w_{i-1}$
and $w_{i-2}$ as approximations of $y(t)$ at $t_{i+1}=t_i+h$, $t_i$,
$t_{i-1} = t_0- h$, and $t_{i-2}=t_0-2h$ respectively.
\begin{enumerate}
\item  We choose a bigger step-size if
\[
\frac{19}{270} \left| \frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h} \right| <
\frac{\epsilon}{2} \ .
\]
We replace $h$ by $qh$, where
\begin{equation}\label{Q}
q= \left| \left(\frac{270}{19}\right)
\frac{h\epsilon}{w_{i+1}^{[1]} - w_{i+1}^{[0]}} \right|^{1/4} \ .
\end{equation}
$q$ should be greater than $1$ as we will show below.
Set $\tilde{t}_0 = t_{i+1}$ and $\tilde{w}_0 = w_{i+1}$, increase $i$
by $4$, and go back to step~\ref{VSTMmethodI2}.
\item We do not change the step-size if
\[
\frac{\epsilon}{2} \leq \frac{19}{270} \left|
\frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h}
\right| < \epsilon \  .
\]
Increase $i$ by $1$ and go back to step~\ref{VSTMmethodI3}..
Changing the step-size is expensive.  So, we do not change it if there
is little or no gain to make.
\end{enumerate}
\item If
\[
\frac{19}{270} \left| \frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h} \right|
\geq \epsilon \ ,
\]
we reduce the step-size.  We replace $h$ by $qh$, where $q$ is defined
in (\ref{Q}).  $q$ should be less than $1$.  If $w_j$ for $i-3 \leq j \leq i$
have already been accepted as good approximations (i.e. $w_i$
comes from the Adams-Moulton method of order four),
set $\tilde{t}_0 = t_i$ and $\tilde{w}_0= w_i$, go back to
step~~\ref{VSTMmethodI2}.  Otherwise
(i.e.\ $w_i$ comes from the Runge-Kutta method of order four), just
go back to step~~\ref{VSTMmethodI2} with the same
$\tilde{t}_0$ and $\tilde{w}_0$ but the new $h$.
\end{enumerate}
\end{algo}

We now justify non-rigorously this variable step-size multistep
method.

We suppose that $w_{i-j}$ has been accepted as an approximation of
$y(t_{i-j})$ with the local truncation error for the
Adams-Moulton method of order four less than $\epsilon$ for
$0 \leq j \leq 3$.  Moreover, we make the localization assumption that
$w_{i-j} \approx y(t_{i-j})$ for $0 \leq j \leq 3$.  Then, from
Definition~\ref{ABFSM}, we get
\begin{equation} \label{ABFSM_tau}
\frac{y(t_{i+1}) - w_{i+1}^{[0]}}{h}
\approx \tau_{i+1}(h) = \frac{251}{720}\, y^{(5)}(\eta) h^4
\end{equation}
for some $\eta$ between $t_{i-3}$ and $t_{i+1}$, where $\tau_{i+1}(h)$
denotes the local truncation error for the Adams-Bashforth method of
order four.

If we also assume that
$\displaystyle y(t_{i+1}) \approx w_{i+1}^{[0]}$, we get
from Definition~\ref{AMTSM} that
\begin{equation} \label{AMTSM_tau}
\frac{y(t_{i+1}) -w_{i+1}^{[1]}}{h} \approx
\tilde{\tau}_{i+1}(h) = -\frac{19}{720}\, y^{(5)}(\xi) h^4
\end{equation}
for some $\xi$ between $t_{i-2}$ and $t_{i+1}$, where
$\tilde{\tau}_{i+1}(h)$ denotes the local truncation error for the
Adams-Moulton method of order four.

Finally, if we assume that $y^{(5)}(t)$ is almost constant on
$[t_{i-3},t_{i+1}]$ and subtract (\ref{AMTSM_tau}) from
(\ref{ABFSM_tau}), we get
\[
\frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h} \approx
\frac{270}{720}\;Y \,h^4 = \frac{3}{8}\; Y \,h^4 \ ,
\]
where $Y \approx y^{(5)}(\xi) \approx y^{(5)}(\eta)$.

Thus $\displaystyle Y \approx
\frac{8}{3}\left(\frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h^5}\right)$.
If we substitute $y^{(5)}(\xi)$ by $Y$ in (\ref{AMTSM_tau}), we get
\[
\left| \tilde{\tau}_{i+1}(h) \right| \approx
\frac{19}{720}\left(\frac{8}{3}
\left| \frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h^5} \right| \right) h^4 =
\frac{19}{270}\,\left|\frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h}\right| \ .
\]

If $\displaystyle \frac{19}{270}\,
\left| \frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h}\right| < \epsilon$,
we may expect that
$\left| \tilde{\tau}_{i+1}(h) \right| < \epsilon$.

If we use $qh$ instead of $h$ in the previous discussion (we keep the
same estimate for $Y$), we get
\[
\left| \tilde{\tau}_{i+1}(qh) \right| \approx
\frac{19}{720}\; Y\,(qh)^4 \approx
\frac{19}{720}\left( \frac{8}{3}
\left| \frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h^5} \right| \right) (qh)^4 =
\frac{19}{270}\,\left| \frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h}\right|\,q^4
\ . \]
To get $\left| \tilde{\tau}_{i+1}(qh) \right| < \epsilon$, we choose
$q$ such that
\[
\frac{19}{270}\,\left|\frac{w_{i+1}^{[1]} - w_{i+1}^{[0]}}{h}\right|\,q^4
< \epsilon \ ;
\]
namely,
\[
q < \left( \frac{270\epsilon}{19}\left|
\frac{h}{w_{i+1}^{[1]} - w_{i+1}^{[0]}}
\right|\right)^{1/4} \ .
\]

The following code implement the variable step-size multistep method
outlined above.

\begin{code}[Variable Step-Size Multistep Method]
To approximate the solution of the initial value problem
\[
\begin{split}
y'(t) &= f(t,y(t)) \quad, \quad t_0 \leq t \leq t_f \\
y(t_0) &= y_0
\end{split} .
\]
\subI{Input} The maximal step-size  hmax.\\
The minimal step-size  hmin.\\
The maximal tolerated error  T.\\
The initial time $t_0$ (t0 in the code below).\\
The final time $t_f$ (tf in the code below).\\
The initial condition $y_0$ (y0 in the code below).\\
The function $f(t,y)$ (funct in the code below).\\
\subI{Output} The approximations $w_i$ (gw(i+1) in the code below)
of $y(t_i)$ at $t_i$ (gt(i+1) in the code below) if all the requested
requirements can be met.
\small
\begin{verbatim}
function [gt,gw] = multistepABM(funct,t0,y0,tf,hmin,hmax,T)
  % last = 1  if we have reached  tf  or  h < hmin  at some point,
  % and  last = 0  otherwise.
  last = 0;
  h = hmin;
  gt(1) = t0;
  gw(1) = y0;
  t(1) = t0;
  w(1) = y0;

  % Given t0 and y0, we use Runge-Kutta of order four, to compute
  % an approximation w(i+1) of y(t0+i*h) for i = 1, 2 and 3 .
  % The code for the function rgkt4() was given previously.
  [t,w] = rgkt4(funct,h,3,t(1),w(1));

  % rkflag = 1  if the last stage used Runge-Kutta of order four and
  % rkflag = 0  otherwise,
  rkflag = 1;
  i=1:4;
  f(i) = funct(t(i),w(i));
  
  while (1==1)
    t(5) = t(4) + h;
    % We use the predictor-corrector method
    predict = w(4) + h*(55*f(4) - 59*f(3) + 37*f(2) - 9*f(1))/24;
    f(5) = funct(t(5),predict);
    correct = w(4) + h*(9*f(5) + 19*f(4) - 5*f(3) + f(2))/24;
    sigma = 19*abs(predict-correct)/(270*h);
    if (sigma < T)
      w(5) = correct;
      f(5) = funct(t(5),correct);
      j=1:4;
      t(j) = t(j+1);
      w(j) = w(j+1);
      f(j) = f(j+1);

      if (rkflag==1)
        %  We accept the three values obtained from Runge-Kutta of order
        %  four and the one obtained with the predictor-corrector method.
        for j=1:4
          gt = [gt,t(j)];
          gw = [gw,w(j)];
        end
      else
        % We accept the new value obtained with the predictor-corrector
        % method.  The other values have already been accepted.
        % It is at least the second time in a row that we apply
        % the predictor-corrector method.
        gt = [gt,t(4)];
        gw = [gw,w(4)];
      end

      if (last == 1)
        break;
      end
      
      % We have now executed at least one iteration of the
      % predictor-corrector method
      rkflag = 0;
      
      if ( (t(4)+h > tf) || (sigma < T/2) )
        % We now choose a bigger step-size.
        if (sigma == 0)
          q = 4.0;
        else
          q = (T/sigma)^0.25;
        end
        h = min([hmax,4*h,q*h]);

        % We check that after the next stage  t  will not exceed  tf.
        if (t(4) + h > tf)
          %  We divide by four because we are now going to use
          %  rgkt4 and one step of Adams-Moulton to
          %  complete the integration;  we must therefore
          %  have that t(4) + 4*h = tf. 
          h = (tf-t(4))/4;
          last = 1;
        end

        t(1) = t(4);
        w(1) = w(4);
        f(1) = f(4);
        [t,w] = rgkt4(funct,h,3,t(1),w(1));
        rkflag = 1;
        j=2:4;
        f(j) = funct(t(j),w(j));
      end
    else
      % We choose a smaller step-size.
      q = max([0.1, (T/sigma)^0.25]);
      h = q*h;

      if (h < hmin)
        gt = NaN;
        gw = NaN;
        break
      end

      % We start Runge-Kutta with  t(4)  and  w(4)  if
      % we have used the predictor-corrector method at the previous
      % stage.
      if (rkflag == 0)
        t(1) = t(4);
        w(1) = w(4);
        f(1) = f(4);
      end
      [t,w] = rgkt4(funct,h,3,t(1),w(1));
      rkflag = 1;
      last = 0;
      j=2:4;
      f(j) = feval(funct,t(j),w(j));
    end
  end  
end
\end{verbatim}
\end{code}

We now describe non-rigorously how to control the step-size for general
variable step-size multistep methods.

We consider two multistep methods of order $p$ to approximate the
solution of (\ref{ODE}).  The first method is an explicit method given by
\begin{equation} \label{vss1}
w_{i+1} = \sum_{j=0}^m\,a_j\,w_{i-j} + h \sum_{j=0}^m\,b_j\,f(t_{i-j},w_{i-j})
\end{equation}
for $m\leq i < N$ and the second method is an implicit method given by
\begin{equation} \label{vss2}
\tilde{w}_{i+1} = \sum_{j=0}^{\tilde{m}}\,\tilde{a}_j\,\tilde{w}_{i-j}
+ h \sum_{j=-1}^{\tilde{m}+1}\,\tilde{b}_j\,f(t_{i-j},\tilde{w}_{i-j})
\end{equation}
for $\tilde{m} \leq i < N$.  We have that
$w_{i-j}$ is the approximation of $y(t_{i-j})$ given by
(\ref{vss1}) and $\tilde{w}_{i-j}$ is the approximation of
$y(t_{i-j})$ given by (\ref{vss2}).

In the proof of Theorem~\ref{link_order_char}, we have shown that a
multistep method of the form (\ref{GFMSM}) satisfies
\begin{align}
& y(t_i+h) - \sum_{j=0}^m\,a_j\,y(t_i-jh)
-h\,\sum_{j=-1}^m\,b_j\,f(t_i-jh,y(t_i-jh)) \nonumber \\
&= \left( 1 - \sum_{j=0}^m\,a_j \right) y(t_i)
+ \left( 1 + \sum_{j=0}^m\,a_j\,j - \sum_{j=-1}^m\,b_j\right) y'(t_i)\,h
\nonumber \\
&\qquad
+ \sum_{k=2}^\infty\,\frac{1}{k!}\,\left( 1 - (-1)^k \sum_{j=0}^m\,a_j\,j^k
- (-1)^{k-1} k \sum_{j=-1}^m\,b_j\, j^{k-1}\right) y^{(k)}(t_i)\,h^k \ .
\label{GMSMmotiv}
\end{align}

Using (\ref{GMSMmotiv}) with $b_{-1}=0$ and the localization assumption 
$w_{i-j} = y_{i-j} = y(t_{i-j})$ for $0 \leq j \leq m$, we find that
the first multistep method of order $p$ satisfies
\begin{equation} \label{VSSE1}
y(t_{i+1}) - w_{i+1} = C_p h^{p+1} y^{(p+1)}(t_i) + O(h^{p+2}) \ ,
\end{equation}
where
\[
C_p = \frac{1}{(p+1)!}\,\bigg( 1 + (-1)^p \sum_{j=0}^m\,a_j\,j^{p+1}
- (-1)^p (p+1) \sum_{j=0}^m\,b_j\, j^p\bigg) \ .
\]
Similarly, $\tilde{w}_{i-j} = y_{i-j} = y(t_{i-j})$ for $0 \leq j \leq m$
and $w_{i+1} \approx y_{i+1} = y(t_{i+1})$, the second multistep method of
order $p$ satisfies
\begin{equation} \label{VSSE2}
y(t_{i+1}) - \tilde{w}_{i+1} = \tilde{D}_p h^{p+1} y^{(p+1)}(t_i) + O(h^{p+2})
\ ,
\end{equation}
\[
\tilde{C}_p = \frac{1}{(p+1)!}\,\bigg( 1
+ (-1)^p \sum_{j=0}^{\tilde{m}}\,a_j\,j^{p+1}
- (-1)^p (p+1) \sum_{j=-1}^{\tilde{m}}\,b_j\, j^p\bigg) \ .
\]

If we assume that $y^{(p+1)}(t)$ is almost constant on an interval
$[t_{i-m},t_{i+1}]$, we may choose $K$ such that
$y^{(p+1)}(t_i) \approx K$.  Hence,
\[
y(t_{i+1}) - w_{i+1} \approx C_p K h^{p+1} + O(h^{p+2})
\quad\text{and} \quad
y(t_{i+1}) - \tilde{w}_{i+1} = \tilde{C}_p K h^{p+1} + O(h^{p+2}) \ .
\]
Thus, after subtracting the second expression from the first
expression, we get
\[
\tilde{w}_{i+1} - w_{i+1} \approx
(C_p - \tilde{C}_p) K h^{p+1} + O(h^{p+2}) \ .
\]
If we ignore the small error of order $h^{p+2}$, we get
\[
K h^{p+1} \approx \frac{\displaystyle \tilde{w}_{i+1} - w_{i+1}}
{\displaystyle C_p - \tilde{C}_p}
\]
if $ C_p \neq \tilde{C}_p$.  If we substitute this expression into
\[
  y(t_{i+s}) - w_{i+s} \approx C_p K h^{p+1} + O(h^{p+2}) \ ,
\]
we get
\begin{equation} \label{MSvssLocalError}
y(t_{i+s}) - w_{i+s} \approx \frac{C_p}{C_p-\tilde{C}_p}
\left( \tilde{w}_{i+1} - w_{i+1} \right) \ .
\end{equation}

Let $\delta$ be a small number.  We may require
\[
\bigg| \frac{\displaystyle C_p}{\displaystyle C_pc-\tilde{D}_p}
\left( \tilde{w}_{_i+s} - w_{i+s} \right)  \bigg| < \delta
\]
at each step.  This is called
{\bfseries error control per step}\index{Multistep Methods!Error
Control per Step}.  If the requirement is not satisfied, we reduce the
step-size $h$.  We may instead require
\[
\bigg| \frac{\displaystyle C_p}{\displaystyle C_pc-\tilde{C}_p}
\left( \tilde{w}_{_i+s} - w_{i+s} \right)  \bigg| < \delta h
\]
at each step.  This is called
{\bfseries error control per unit step}\index{Multistep Methods!Error
Control per Unit Step}.
This takes care of the accumulation of error at each step (assuming
that the error is evenly distributed among the integration steps).
We have that $N\delta h = \delta (b-a)$ is the
{\bfseries cumulative error}\index{Multistep Methods!Cumulative Error}.

\section{Convergence, Consistency and Stability}\label{CCSyuk}

The content of this section is based in great part on \cite{I,IK}.

We consider the initial value problem (\ref{ODE}), where 
we assume that $f:[t_0,t_f]\times \RR \rightarrow \RR$ has continuous mixed
derivatives of sufficiently high order.  This
implies that the solution $y$ of (\ref{ODE}) is sufficiently
differentiable.

In this section, we consider multistep methods of the form
\begin{equation} \label{MSM_GF}
\begin{split}
w_{i+1} &= \sum_{j=0}^m\,a_jw_{i-j} + h F(h,\VEC{t},\VEC{w},f)
\quad, \quad m \leq i < N \\
w_i &= y(t_i) \quad, \quad 0 \leq i \leq m
\end{split}
\end{equation}
where $a_m \neq 0$,
$\VEC{t}
=\begin{pmatrix} t_{i-m} & \ldots & t_{i-1} & t_i & t_{i+1} \end{pmatrix}^\top$
and 
$\VEC{w}
=\begin{pmatrix} w_{i-m} & \ldots & w_{i-1} & w_i & w_{i+1} \end{pmatrix}^\top$.
We assume that
\begin{equation} \label{Fzero}
F(h,\VEC{t},\VEC{w},0) = 0
\end{equation}
and
\begin{equation} \label{FDP_Lip}
\big| F\left(h,\VEC{t}, \VEC{w}^{[1]},f\right) -
F\left(h,\VEC{t},\VEC{w}^{[2]},f\right) \big|
\leq R\,\left\|\VEC{w}^{[1]} -\VEC{w}^{[2]} \right\|_1
= R \sum_{j=-1}^m \left|w_{i-j}^{[1]}-w_{i-j}^{[2]}\right|
\end{equation}
for a constant $R$.

\begin{enumerate}
\item The Multistep methods defined in Definition~\ref{GFMSM} are of the
form (\ref{MSM_GF}), and satisfy (\ref{Fzero}) and (\ref{FDP_Lip}).

We have
\[
F(h,\VEC{t},\VEC{w},f) = \sum_{j=-1}^m b_j f(t_{i-j},w_{i-j}) \ .
\]

Obviously, $F(h,\VEC{t},\VEC{w},0) = 0$.  Suppose that $L$ is the
Lipschitz constant in the definition of a well posed differential
equation; namely, $L$ is such that
$|f(t,x)-f(t,y)| \leq L |x-y|$ for all $(t,x)$ and $(t,y)$ in
the domain of $f$.  We have that
\begin{align*}
&\left| F\left(h,\VEC{t},\VEC{w}^{[1]},f\right)
- F\left(h,\VEC{t},\VEC{w}^{[2]},f\right) \right|
\leq \sum_{j=-1}^m |b_j|\,
\left| f\left(t_{i-j},w_{i-j}^{[1]}\right)
- f\left(t_{i-j},w_{i-j}^{[2]}\right) \right| \\
& \qquad
\leq L \sum_{j=-1}^m |b_j|\, \left| w_{i-j}^{[1]} - w_{i-j}^{[2]} \right|
\leq L \max_{-1 \leq j \leq m} |b_j| 
\sum_{j=-1}^m \left| w_{i-j}^{[1]} - w_{i-j}^{[2]} \right|
= R \, \left\|\VEC{w}^{[1]}-\VEC{w}^{[2]} \right\|_1
\end{align*}
for $\displaystyle R = L \max_{-1 \leq j \leq m} |b_j|$.
\item The Runge-Kutta methods defined in Definition~\ref{GFRKM} are also
of the form (\ref{MSM_GF}), and satisfy (\ref{Fzero}) and (\ref{FDP_Lip}).

We have
\[
  F(h,\VEC{t},\VEC{w},f) = \sum_{j=1}^s \gamma_j K_j \ ,
\]
where $\VEC{t} = (t_i)$ and $\VEC{w} = (w_i)$ since Runga-Kutta methods
are one-step methods.

By definition of the $K_j$, we have that $F(h,\VEC{t},\VEC{w},0) = 0$.
As before, let $L$ be the Lipschitz constant in the definition of a well
posed differential  equation; namely, $L$ is such that
$|f(t,x)-f(t,y)| \leq L |x-y|$ for all $(t,x)$ and $(t,y)$ in
the domain of $f$.  Let
\[
K_j^{[k]} = f(t_i + \alpha_j h,
w_i^{[k]} + h \sum_{m=1}^s \beta_{j,m} K_m^{[k]})
\]
for $k=1$ and $2$.  To verify this claim, we first note that
\begin{align}
&\left| K_j^{[1]}  - K_j^{[2]}  \right|
= \left|f(t_i + \alpha_j h,
w_i^{[1]} + h \sum_{m=1}^s \beta_{j,m} K_m^{[1]})
- f(t_i + \alpha_j h,
w_i^{[2]} + h \sum_{m=1}^s \beta_{j,m} K_m^{[2]}) \right| \nonumber \\
&\qquad \leq L \left| w_i^{[1]} - w_i^{[2]} \right|
 + L h \sum_{m=1}^s \beta_{j,m} \left| K_m^{[1]} - K_m^{[2]} \right|
\quad , \quad 1 \leq j \leq s \ . \label{CCSRKcase}
\end{align}
Let
\[
B = \begin{pmatrix} \beta_{1,1} & \beta_{1,2} & \ldots & \beta_{1,s} \\
\beta_{2,1} & \beta_{2,2} & \ldots & \beta_{2,s} \\
\vdots & \vdots & \ddots & \vdots \\
\beta_{s,1} & \beta_{s,2} & \ldots & \beta_{s,s}
\end{pmatrix} \ , \ 
\VEC{K} =
\begin{pmatrix}
\left| K_1^{[1]}  - K_1^{[2]}  \right| \\[0.5em]
\left| K_2^{[1]}  - K_2^{[2]}  \right| \\
\vdots \\
\left| K_s^{[1]}  - K_s^{[2]}  \right|
\end{pmatrix}
\ \text{and} \quad
\VEC{W} =
\begin{pmatrix}
\left| w_i^{[1]}  - w_i^{[2]}  \right| \\[0.5em]
\left| w_i^{[1]}  - w_i^{[2]}  \right| \\
\vdots \\
\left| w_i^{[1]}  - w_i^{[2]}  \right|
\end{pmatrix}
\ .
\]
We can rewrite (\ref{CCSRKcase}) as
\[
  \VEC{K} \leq L\VEC{W} + L h B \VEC{K} \ ,
\]
where the inequality is component by component.
We have that
\[
  \| \VEC{K} \|_1 \leq L \|\VEC{W}\|_1 + L h \|B\|_1 \|\VEC{K}\|_1 \ .
\]
Thus
\[
\left| K_j^{[1]}  - K_j^{[2]}  \right| \leq
\|\VEC{K}\|_1 \leq \frac{L}{1-L h\|B\|_1} \|\VEC{W}\|_1
\leq 2 L \|\VEC{W}\|_1
= 2 L s \left| w_i^{[1]} - w_i^{[2]} \right|
\quad , \quad 1 \leq j \leq s \ ,
\]
if we assume that $h$ is small enough to have $L h\|B\|_1 < 1/2$
\footnote{Any value smaller than $1$ could have been used.}.
Finally, we have that
\begin{align*}
\left| F\left(h,\VEC{t},\VEC{w}^{[1]},f\right)
- F\left(h,\VEC{t},\VEC{w}^{[2]},f\right) \right|
&\leq \sum_{j=1}^s \gamma_j \left| K_j^{[1]} - K_j^{[2]}\right|
\leq \underbrace{\sum_{j=1}^s \gamma_j}_{=1}
\ 2 L s \left| w_i^{[1]} - w_i^{[2]} \right| \\
&= 2 L s \left| w_i^{[1]} - w_i^{[2]} \right|
= R \left| w_i^{[1]} - w_i^{[2]} \right|
\end{align*}
for $\displaystyle R = 2 L s$.
\end{enumerate}

To define the stability of a numerical method, we consider a perturbation
of (\ref{MSM_GF}) given by the difference equation
\begin{equation} \label{MSM_GFpertub}
\begin{split}
u_{i+1} &= \sum_{j=0}^m\,a_ju_{i-j} + h F(h,\VEC{t},\VEC{w},f)
+ \delta_{i+1}(h) \quad, \quad m\leq i < N \\
u_i &= y(t_i) + \delta_i(h) \quad, \quad 0 \leq i \leq m
\end{split}
\end{equation}

Convergence and consistency are two primordial concepts.  Convergence
obviously does not need any motivation. 

\begin{defn}
A multistep method of the form (\ref{MSM_GF}) \footnotemark\ is
{\bfseries convergent}\index{Convergent Method} if,
for all well-posed initial value problems (\ref{ODE}),
\[
\lim_{h \rightarrow 0}\,\max_{0\leq i \leq N} \,|y_i - u_i| = 0 \ ,
\]
where $u_i$ is the numerical approximation of $w_i$.
\label{methodConv}
\end{defn}

\footnotetext{From now on, this will refer to Runge-Kutta methods
(Definition~\ref{GFRKM}) and multistep methods (Definition~\ref{GFMSM}).}

\begin{rmk}
To prove convergence of a multistep method, we will require that
$\displaystyle \max_{0\leq i \leq N} |\delta_i(h)| \to 0$ as $h\to 0$.
Obviously, in practice, this is not realistic.  We do not expect
round off errors to go to $0$ as $h$ goes to $0$.  However, our
theoretical result shows that by having 
$\displaystyle \max_{0\leq i \leq N} |\delta_i(h)|$ very small,
we may hope that our numerical approximation of the solution be
very accurate.  To decrease round off errors (by choosing
the right algorithm, by efficiently programming it, ...) is one of
the big challenges in numerical analysis.
\end{rmk}

\begin{rmk}
A definition of convergence that is often given in textbooks is the
following.  \label{AltDefConv}

A multistep method is convergent if, for all
well-posed initial value problems (\ref{ODE}),
\[
\lim_{h \rightarrow 0}\,\max_{0\leq i \leq N} \,|y_i - w_i| = 0 \ .
\]
All rounding errors are assumed to be null.

For the Euler's method, if we ignore rounding errors
in Theorem~\ref{Eulererrorbound} (i.e.\ $\delta=\delta_0=0$), we get
\[
\max_{0\leq i \leq N} |y_i - w_i| \leq \frac{Mh}{2L}\left(e^{L(t_f -t_0)} -1 \right)
\rightarrow 0
\]
as $h \rightarrow 0$.  So, the Euler's method is
converging in this weak sense.  Unfortunately, this does not
prove that the Euler's method is convergent according to
Definition~\ref{methodConv}.  In fact, we will need to assume
(the unrealistic assumption) that
$\displaystyle \max_{0\leq i \leq N} |\delta_i(h)| = O(h^2)$ to be
able to show that Euler's method is converging in the sense of
Definition~\ref{methodConv}.   

Another example is given by the Trapezoidal Method.  It is convergent in
the weak sense above.  For this example, we refer to
Definition~\ref{trapmethdef} and the paragraphs following this
definition.  We prove that
\begin{equation} \label{trap_conv0}
\lim_{h\rightarrow 0}\,\max_{0\leq i \leq N}\,|w_i-y(t_i)| = 0 \;.
\end{equation}

Let $e_i = w_i - y(t_i)$.  If we subtract
\[
y(t_{i+1}) = y(t_i) + \frac{h}{2}\,\big(
f(t_i,y(t_i)) + f(t_{i+1},y(t_{i+1})) \big) + M(\xi_i,\eta_i)\,h^3
\]
from
\[
w_{i+1} = w_i + \frac{h}{2} \left(f(t_{i+1},w_{i+1}) +f(t_i,w_i)
\right) \ ,
\]
we get
\begin{equation} \label{trap_conv1}
e_{i+1} = e_i + \frac{h}{2}\bigg( f(t_i,w_i) -
f(t_i,y(t_i)) + f(t_{i+1},w_{i+1}) - f(t_{i+1},y(t_{i+1})) \bigg)
- M(\xi_i,\eta_i)\,h^3 \ .
\end{equation}
As we have seen when computing the order of the Trapezoidal Method
if we assume that $f$ is twice continuously differentiable on
$[t_0,t_f]\times\RR$, then $| M(\xi_i, \eta_i)| \leq Q$ for all $i$, where
$Q = 5K/12$ and $K$ is the maximum of $y^{(3)}(t)$ on $[t_0,t_f]$.
If we use this property and the assumption that $f$ satisfies the
Lipschitz condition (\ref{Lipschitz}), we get from (\ref{trap_conv1}) that
\begin{equation} \label{trap_conv2}
|e_{i+1}| \leq |e_i| + \frac{hL}{2}\left( |e_i| + |e_{i+1}| \right)
+ Q\,h^3 \ .
\end{equation}
Since $h\rightarrow 0$, we may assume that $hL/2 < 1$.  Hence, 
we get from (\ref{trap_conv2}) that
\begin{equation} \label{trap_conv3}
|e_{i+1}| \leq \frac{1 + hL/2}{1-hL/2}\,|e_i|
+\frac{Q\,h^3}{1-hL/2} \ .
\end{equation}

We now show by induction that
\begin{equation} \label{trap_conv4}
|e_i|  \leq \frac{Q}{L}\left( \left(
\frac{1+ hL/2}{1-hL/2} \right)^i -1 \right)h^2
\quad , \quad 0 \leq i \leq N \ .
\end{equation}
The result is true for $i=0$ because we assume that
$w_0 = y_0$.  Suppose (\ref{trap_conv4}) it is true
for $i$.  Using (\ref{trap_conv3}) and (\ref{trap_conv4}), we get
\begin{align*}
|e_{i+1}| &\leq \frac{1+ hL/2}{1-hL/2}\,|e_i|
+\frac{Q\,h^3}{1- hL/2} \leq \frac{1+ hL/2}{1- hL/2}\,
\left(\frac{Q}{L}\left(\left( \frac{1+ hL/2}{1- hL/2}\right)^i
 -1 \right)h^2\right) +\frac{Q\,h^3}{1-hL/2} \\
&= \frac{Q}{L}\left(\frac{1+ hL/2}{1- hL/2}\right)^{i+1}
h^2 + \frac{Q h^2}{L(1- hL/2)}
\left(-\left(1+ \frac{hL}{2}\right)+L h\right) \\
&= \frac{Q}{L}\left(\frac{1+ hL/2}{1- hL/2}\right)^{i+1}
h^2 - \frac{Q h^2}{L}
= \frac{Q}{L}\left( \left(\frac{1+hL/2}{1-hL/2}\right)^{i+1} -1 \right) h^2
\ .
\end{align*}
So (\ref{trap_conv4}) is true for $i$ replaced by $i+1$, completing
the proof by induction.  Since $0 < hL/2 < 1$, we have that
\[
0 < \frac{1+hL/2}{1-hL/2} =
1 + \frac{hL}{1- hL/2} \leq
\sum_{j=0}^\infty\, \frac{1}{j!}\,
\left( \frac{hL}{1 - hL/2} \right)^j =
e^{hL/(1-hL/2)} \ .
\]
Thus
\[
|e_i| \leq \frac{Q}{L}
\left(\frac{1+hL/2}{1 - hL/2}\right)^i
h^2 \leq \frac{Q}{L} e^{ihL/(1-hL/2)}\,h^2
\leq \frac{Q}{L} e^{(t_f-t_0)L/(1-hL/2)}\,h^2
\quad, \quad 0 \leq i \leq N \ .
\]
Hence
\[
\max_{0\leq i \leq N}\,|w_i-y(t_i)| = \max_{0\leq i \leq N}\,|e_i|
\leq \frac{Q}{L} e^{(t_f-t_0)L/(1-hL/2)}\,h^2 \to 0
\]
as $h \rightarrow 0$.  This proves (\ref{trap_conv0}).
\label{ConvAltern}
\end{rmk}

Consistency ensure that the numerical method approximates adequately
the differential equation.

\begin{defn}
The {\bfseries local truncation error}\index{Local Truncation Error} of
a multistep method of the form (\ref{MSM_GF}) is defined by
\[
\tau_{i+1}(h) = \frac{1}{h} \left( y_{i+1} - \sum_{j=0}^m\,a_j y_{i-j} \right)
- F(h,\VEC{t},\VEC{y}, h) \quad , \quad 0 \leq i < N \ ,
\]
where
$\VEC{y} = \begin{pmatrix} y_{i-m} & \ldots & y_{i-1} & y_i & y_{i+1}
\end{pmatrix}^\top$ and $y_i = y(t_i)$ for all $i$ as usual.

we say that a multistep method is
{\bfseries consistent}\index{Consistent Method} if, for each
well posed initial value problems (\ref{ODE}), there exists a function
$\tau:\RR\to \RR$ such that
\[
\max_{0 \leq i < N} \, |\tau_{i+1}(h)| | \leq \tau(h) \to 0
\]
as $h\to 0$.  \label{methodCons}
\end{defn}

A finite difference problem of order greater than $0$ is consistent.

For the Runga-Kutta methods given in Definition~\ref{GFRKM}, the
local truncation error is defined by
\[
\tau_{i+1}(h) = \frac{y_{i+1} - y_i}{h} - h \sum_{j=1}^s \gamma_j K_j
\quad, \quad 0 \leq i < N \ ,
\]
where
\[
K_j = f(t_i + \alpha_j h, y_i + h \sum_{k=1}^s \beta_{j,k} K_k) \ ,
\]

For the multistep methods given in Definition~\ref{GFMSM}, the
local truncation error is defined by
\[
\tau_{i+1}(h) = \frac{1}{h}\left( a_{i+1} -\sum_{j=0}^m\,a_j y(t_{i+j})\right)
- \sum_{j=-1}^m b_j f(t_{i-j},y_{i-j}) \quad, \quad m \leq i < N \ .
\]

\begin{rmk}
In the definitions of convergence and consistency, we consider the
limit when $h \to 0$.  We have to keep in mind that $h = (t_f-t_0)/N$
and that $N \to \infty$.  It would have been more appropriate to write
$h_N$ instead of $h$ in these definitions but we will stick to
the tradition of only writing $h$. \label{LinkhN}
\end{rmk}

\begin{egg}
One of the simplest multistep methods is obviously the Euler's method.
  
Assume that $f$ in the initial value problem (\ref{ODE}) satisfies a
Lipschitz condition on $[t_0,t_f]\times \RR$ with respect to the
second variable and that $L$ is the Lipschitz constant.  Moreover, assume
that $|y''|$ is bounded by $M$ on $[t_0,t_f]$ where $y$ is the
solution of (\ref{ODE}).

The Euler's method is consistent with respect to (\ref{ODE})
because
\[
|\tau_{i+1}(h)| = \left|\frac{h}{2}\,y''(\xi_i) \right| \leq
\tau(h) \equiv \frac{Mh}{2} \rightarrow 0
\]
as $h \rightarrow 0$.
\end{egg}

\begin{egg}
Consider the following Adams-Bashforth Method of order two.
\begin{align*}
w_{i+1} &= w_{i-1} + 2hf(t_i,w_i) \quad, \quad 1 \leq i < N \\
w_0 &= y_0 \\
w_1 &= y_1
\end{align*}
This method is obtained by taking $m=1$ and $q=1$ in
(\ref{explformula}).  We now show that this is a
consistent method of order two.  Since
\begin{align*}
y(t_{i+1}) &= y(t_i) + h y'(t_i) + \frac{h^2}{2} y''(t_i) +
\frac{h^3}{3!} y'''(\xi_i)
\intertext{and}
y(t_{i-1}) &= y(t_i) - h y'(t_i) + \frac{h^2}{2} y''(t_i) -
\frac{h^3}{3!} y'''(\nu_i)
\end{align*}
for some number $\xi_i$ and $\nu_i$ between $t_{i-1}$ and $t_{i+1}$, and
$f(t_i,y_i) = y'(t_i)$, we get
\begin{align*}
&\tau_{i+1}(h) = \frac{y_{i+1}-y_{i-1}}{h} - 2 f(t_i,y_i) \\
&\quad = \frac{\displaystyle \big(y(t_i) + h y'(t_i) + \frac{h^2}{2} y''(t_i) +
\frac{h^3}{3!} y'''(\xi_i)\big) - \big(y(t_i) - h y'(t_i)
+\frac{h^2}{2} y''(t_i) - \frac{h^3}{3!} y'''(\nu_i)\big)}{h} \\
& \qquad\qquad - 2y'(t_i) \\
&\quad = \left( \frac{y'''(\xi_i)}{3!} + \frac{y'''(\nu_i)}{3!} \right) h^2\ .
\end{align*}
Hence,
\[
| \tau_{i+1}(h) | \leq \tau(h) \equiv \frac{2}{3}
\max_{t_0 \leq t \leq t_f} |y'''(t)| h^2 = \frac{2M}{3}\, h^2
\quad, \quad 1 \leq i < N \ ,
\]
where
$\displaystyle M = \max_{t_0 \leq t \leq t_f} |y'''(t)|$.
Therefore, the method is of order two and
$| \tau_{i+1}(h) | \leq \tau(h) \rightarrow 0$ as $h \rightarrow 0$.
The method is therefore consistent.
\label{comp_order}
\end{egg}

The definition of stability that we adopt is given below.

\begin{defn}
A multistep method of the form (\ref{MSM_GF}) is
{\bfseries zero-stable}\index{Zero-Stable Method} if,
for any well-posed initial value problems (\ref{ODE}), there exist
$S$ and $h_0$ such that for any partition of $[t_0,t_f]$ with
$h < h_0$, any solution $\{u_i^{[j]}\}_{i=0}^N$ of (\ref{MSM_GFpertub})
with $\delta_i = \delta_i^{[j]}$ for $j=1$ and $2$, then
\[
| u_i^{[1]} - u_i^{[2]} | < S \epsilon
\]
for $i = 0$, $1$, \ldots, $N$ whenever
$| \delta_i^{[1]} - \delta_i^{[2]}  | < \epsilon$ for
$i = 0$, $1$, \ldots, $N$.
\end{defn}

\begin{rmk}
This definition is reminiscent of the definition of stability for
systems of linear equations.  A system of the form $A \VEC{x} = \VEC{b}$,
where $A$ is a \nn matrix, is stable is there exists a constant $K$ such
that $\|\VEC{x} \| \leq K \|A \VEC{x}\|$ for all $\VEC{x}$.  This ensures
that if $\tilde{\VEC{b}}$ is a slight perturbation of $\VEC{b}$ than
the solution $\VEC{x}_{\tilde{b}}$ of $A \VEC{x} = \tilde{\VEC{b}}$ is a slight
perturbation of the solution $\VEC{x}_b$ of $A \VEC{x} = \VEC{b}$ because
\[
\|\VEC{x}_{\tilde{b}} - \VEC{x}_b \|
\leq K \|A\VEC{x}_{\tilde{b}} - A \VEC{x}_b \|
= K \|\tilde{\VEC{b}}- \VEC{b} \| \ .
\]
\end{rmk}

\subsection{Consistency}

\begin{prop}
Runge-Kutta methods are consistent if and only if
\[
\sum_{j=1}^s \gamma_j = 1 \ .
\]
\end{prop}

\begin{proof}
Since $y(t_{i+1}) = y(t_i) + h y'(\xi_i)$  for some
$\xi_i$ between $t_i$ and $t_{i+1}$, we have
\[
\tau_{i+1}(h) = \frac{y(t_{i+1}) - y(t_i)}{h} - \sum_{j=1}^s \gamma_j K_j
= y'(\xi_i) - \sum_{j=1}^s \gamma_j K_j
\]
for $0 \leq i < N$.  Let $c \in [a,b]$ be a fixed value such that
$t_i \leq c \leq t_{i+1}$ for all $h$.  So $i$ will increase and
converge to $\infty$ as $h$ goes to $0$ to ensure that
$t_i \leq c \leq t_{i+1}$.  We have that
$t_{i+1} \rightarrow c$, $t_i \rightarrow c$, $y_i \to y(c)$ and 
$\xi_i \rightarrow c$ as $h \rightarrow 0$.  Thus
\[
\lim_{h\rightarrow 0}\,\tau_{i+1}(h) = y'(c) -
\sum_{j=1}^s\,\gamma_i f(c,y(c))
= y'(c) \left( 1 - \sum_{j=1}^s\,\gamma_j \right) = 0
\]
if and only if
\[
\sum_{j=1}^s \,\gamma_i = 1 \ .  \qedhere
\]
\end{proof}

So, all our Runge-Kutta methods are consistent since we require
$\displaystyle \sum_{j=1}^s \gamma_j = 1$.

\begin{prop} \label{cond_consistency}
If the multistep method given in Definition~\ref{GFMSM} is consistent,
then
\[
1 = \sum_{i=0}^m\,a_i \\
\quad\text{and} \quad
\sum_{k=-1}^m \,b_k = \sum_{k=0}^m\,a_k\,(k+1) \ .
\]
\end{prop}

\begin{proof}
From the Mean Value Theorem, we have that $y_{i-k} = y(t_{i-k}) =
y(t_{i+1}) - (k+1)h y'(\xi_{i-k})$ for some $\xi_{i-k} \in [t_{i-k},t_{i+1}]$,
where $0 \leq k \leq m$.  If we substitute these expressions
in the definition of local truncation error given in
Definition~\ref{methodCons}, we get
\begin{equation}\label{MSMconstequ}
\tau_{i+1}(h) = \left(1-\sum_{k=0}^m\,a_k \right) \frac{y_{i+1}}{h}
+ \sum_{k=0}^m\,a_k\,(k+1)y'(\xi_{i-k})
- \sum_{k=-1}^m b_k f(t_{i-k},y_{i-k}) \ .
\end{equation}
Choose an increasing sequence $\{N_j\}_{j=1}^\infty$ of positive
integers converging to $\infty$ such that there always is a value
$i_j$ of $i$ such that $t_{i_j+1} = c$, a constant value, when $N= N_j$.
We have that $i_j \to \infty$,
$\displaystyle h = h_j \equiv \frac{t_f-t_0}{N_j} \to 0$,
$t_{i_j-k} \rightarrow c$ and $\xi_{i-k} \rightarrow c$ for all
$0 \leq k \leq m$ as $N_j\to \infty$ because
$t_{i+1}-t_{i-m} = (m+1)h_j \to 0$ as $h_j \to 0$.  If we assume that
the multistep method is consistent, then $h = h_j \to 0$ in
(\ref{MSMconstequ}) yields
\[
0 = \left(1-\sum_{k=0}^m\,a_k \right) y(c) \quad
\text{and} \quad 
0 = \sum_{k=0}^m\,a_k\,(k+1)y'(c) - \sum_{k=-1}^m b_m f(c,y(c)) \ .
\]
We get $\displaystyle 1-\sum_{k=0}^m\,a_k  = 0$ from the first equation and,
because $y'(t)=f(t,y(t))$, we get
\[
0 = \sum_{k=0}^m\,a_k\,(k+1) - \sum_{k=-1}^m \,b_k
\]
from the second equation.
\end{proof}

\begin{rmk}
We can easily show that if a consistent multistep method is converging
according to the definition given Remark~\ref{ConvAltern}; namely 
$\displaystyle \max_{0\leq i \leq N} |w_i -y_i| \to 0$ as $h \to 0$, then
$\displaystyle 1 = \sum_{i=0}^m\,a_i$.
\end{rmk}

\subsection{Finite Difference Equations}

Before diving deeper into the analysis of multistep methods, we need to
introduce some notions about finite difference equations.

\begin{defn}
Consider the
{\bfseries finite difference equation}\index{Finite Difference Equations}
\begin{equation}  \label{basicFDP}
\begin{split}
\sum_{j=0}^s\,a_j\,u_{i-j} &= C_i \quad, \quad i \geq s \\
u_i &= v_i \quad, \quad 0 \leq i < s
\end{split}
\end{equation}
where the constants $a_j$ for $0\leq j \leq s$, $v_j$ for
$0 \leq j < s$, and $C_i$ for $i\geq s$ are given.
A sequence $\{ u_i \}_{i=0}^\infty$ that satisfies (\ref{basicFDP})
is called a
{\bfseries solution}\index{Finite Difference Equations!Solution} of
(\ref{basicFDP}).

If $a_s\,a_0 \neq 0$, the finite difference equation is said to be of
{\bfseries order}\index{Finite Difference Equations!Order} $s$.
\end{defn}

\begin{theorem}
If (\ref{basicFDP}) is of order $s$, then there is a unique solution
of (\ref{basicFDP}).
\end{theorem}

\begin{proof}
Since $a_0 \neq 0$, the existence of the solution follows recursively from
\begin{equation} \label{solFDP}
u_i = -\frac{1}{a_0} \, \sum_{j=1}^s\,a_j\,u_{i-j} 
+ C_i \quad, \quad i \geq s \ ,
\end{equation}
with $u_i = v_i$ for $0 \leq i < s$.

Suppose that there are two solutions
$\displaystyle \left\{ u_i^{[1]} \right\}_{i=0}^\infty$ and
$\displaystyle \left\{u_i^{[2]} \right\}_{i=0}^\infty$ of
(\ref{basicFDP}).  Then,
$\displaystyle \left\{ u_i \right\}_{i=0}^\infty$ with
$u_i = u_i^{[1]} - u_i^{[2]}$ for all $i\geq 0$ is a solution of
(\ref{basicFDP}) with $C_i = 0$ for $i\geq s$ and $u_i = 0$ for
$0 \leq i < s$.  It follows from (\ref{solFDP}) that
$u_i = u_i^{[1]} - u_i^{[2]} = 0$ for $i\geq 0$.
\end{proof}

Consider the {\bfseries homogeneous finite difference
equation}\index{Finite Difference Equations!Homogeneous}
\begin{equation} \label{homFDP}
\sum_{j=0}^s\,a_j\,u_{i-j} = 0 \quad, \quad i \geq s \ ,
\end{equation}
of order $s$.

It is clear that a linear combination of solutions of
(\ref{homFDP}) is a solution of (\ref{homFDP}).  Moreover, it follows
from (\ref{solFDP}) with $C_i = 0$ for all $i \geq s$ that the linear
independence of solutions $\{ u_i^{[j]} \}_{i=0}^\infty$ of
(\ref{homFDP}) for $1 \leq j \leq k$ is completely determined by the
linear independence of the vectors
$\displaystyle \begin{pmatrix} u_0^{[j]} & u_1^{[j]} & \ldots &
u_{s-1}^{[j]} \end{pmatrix}^\top$ in $\RR^s$ for $1 \leq j \leq k$.
It follows that (\ref{homFDP}) may have $s$ linearly independent
solutions.  For instance, given $0 \leq k < s$, let
$\displaystyle \left\{ u_i^{[k]} \right\}_{i=0}^\infty$ be the solution of
\begin{align*}
\sum_{j=0}^s\,a_j\,u_{i-j} &= 0 \quad, \quad i \geq s \\
u_i &= \delta_{k,i} \quad, \quad 0 \leq i < s
\end{align*}
where
\[
\delta_{k,i} = \begin{cases}
0 & \quad \text{if} \quad i \neq k \\
1 & \quad \text{if} \quad i = k
\end{cases}
\]
is the well known Dirac Delta function.  The solutions
$\displaystyle \left\{ u_i^{[0]} \right\}_{i=0}^\infty$,
$\displaystyle \left\{ u_i^{[1]} \right\}_{i=0}^\infty$,
\ldots, $\displaystyle \left\{ u_i^{[s-1]} \right\}_{i=0}^\infty$
form a set of $s$ linearly independent solutions of (\ref{homFDP}).

\begin{defn}
A set of $s$ linearly independent solutions of an homogeneous finite
difference equation of order $s$ is called a
{\bfseries fundamental set of solutions}\index{Finite Difference
Equations!Fundamental Set of Solutions}.
\end{defn}

\begin{theorem}
Let $\displaystyle \{ u_i^{[k]} \}_{i=0}^\infty$ for $0 \leq k < s$ be
a fundamental set of solutions of the homogeneous
finite difference equation (\ref{homFDP}).  Then, the solution of
\begin{align*}
\sum_{j=0}^s\,a_j\,u_{i-j} &= 0 \quad, \quad i \geq s \\
u_i &= v_i \quad, \quad 0 \leq i < s
\end{align*}
can be expressed uniquely as a linear combination of
$\displaystyle \left\{ u_i^{[k]} \right\}_{i=0}^\infty$ for $0 \leq k < s$.
\end{theorem}

\begin{proof}
We have to find $\alpha_0$, $\alpha_1$, \ldots, $\alpha_{s-1}$ such that
\[
\sum_{k=0}^{s-1} \,\alpha_k \, u_i^{(k)} = v_i
\]
for $0 \leq i < s$.  Namely, we have to solve
$A\VEC{\alpha} = \VEC{v}$ where
\[
A = \begin{pmatrix}
u_0^{(0)} & u_0^{(1)} & \ldots & u_0^{(s-1)} \\
u_1^{(0)} & u_1^{(1)} & \ldots & u_1^{(s-1)} \\
 \vdots & \vdots & \ddots & \vdots \\
u_{s-1}^{(0)} & u_{s-1}^{(1)} & \ldots & u_{s-1}^{(s-1)}
\end{pmatrix}
\quad , \quad 
\VEC{v} = \begin{pmatrix}
v_0 \\
v_1 \\
\vdots \\
u_{s-1}
\end{pmatrix}
\quad \text{and} \quad
\VEC{\alpha} = \begin{pmatrix}
\alpha_0 \\
\alpha_1 \\
\vdots \\
\alpha_{s-1}
\end{pmatrix} \ .
\]
Since $\{ u_i^{[k]} \}_{i=0}^\infty$ for $0\leq k < s$ are
linearly independent, the columns of $A$ must also be linearly
independent.  Thus the matrix $A$ is invertible.  Hence there is a
unique solution to $A\VEC{\alpha} = \VEC{v}$.
\end{proof}

Suppose that $u_i = z^i$ for $i \geq 0$ is a solution of
the homogeneous finite difference equation (\ref{homFDP}).  If we
substitute this formula for $u_i$ in (\ref{homFDP}) and factor out
$z^{i-s}$, we get the
{\bfseries characteristic polynomial}\index{Multistep
Methods!Characteristic Polynomial} 
\begin{equation}\label{charpolFDE}
\sum_{j=0}^s \,a_j \, z^{s-j} = 0 \ .
\end{equation}
If $r$ is a real root of the characteristic polynomial, then
$\{ u_i \}_{i=0}^\infty$ with $u_i = r^i$ is a solution for
(\ref{homFDP}).  We note that $r \neq 0$ because we assume that
$a_s \neq 0$.  If we could find $s$ distinct roots of the
characteristic polynomial, then we will have $s$ solutions that
provide a fundamental set of solutions for (\ref{homFDP}).
Unfortunately, not all polynomials of degree $s$ have $s$ distinct
real roots.  If we work in $\CC$, we can describe all the solutions
of (\ref{homFDP}).

\begin{prop}
If $r \in \CC$ is a root of algebraic multiplicity $m$ of the
characteristic polynomial (\ref{charpolFDE}) with $a_s \neq 0$, then
$\displaystyle \left\{ u_i^{[k]} \right\}_{i=0}^\infty$
with $u_i = i^kr^i$ and $0\leq k <m$
are $m$ linearly independent solutions for (\ref{homFDP}).
\label{rootHO}
\end{prop}

\begin{proof}
Let
\[
p(z) = \sum_{j=0}^s \,a_j \, z^{s-j}
\]
If $r$ is a root of $p$ of algebraic multiplicity $m$ (so a zero of
order $m$ of $p$), we have that
$p(r) = p'(r) = \ldots = p^{(m-1)}(r) = 0$ and $p^{(m)}(r) \neq 0$.
Let $q(z) = z^{i-s}p(z)$ for some $i \geq s$.  We have
\[
  q^{(k)}(z) = \sum_{j=0}^k \binom{k}{j}\left(\pdfdxn{z^{i-s}}{z}{j}\right)
  p^{(k-j)}(z) \ .
\]
Hence, $q(r) = q'(r) = \ldots = q^{(m-1)}(r) = 0$
and $q^{(m)}(r) = r^{i-s} p^{(m)}(r) \neq 0$.  Therefore,
\[
  q^{(k)}(r) = \sum_{j=0}^s \,a_j \, (i-j)(i-j-1)\ldots(i-j-k+1)r^{i-j-k}
  = 0
\]
for $0<k<m$.  Hence
\[
 z^k q^{(k)}(r) = \sum_{j=0}^s \,a_j \, (i-j)(i-j-1)\ldots(i-j-k+1)r^{i-j}
  = 0
\]
for $0<k<m$.  We have shown that
$\{ v_i^{[k]} \}_{i=0}^\infty$ with $v_i = i(i-1)\ldots(i-k+1)r^i$ and
$0 < k <m$ are solutions for (\ref{homFDP}).  Since
\[
i \prod_{n=1}^N (i - a_n)
= i^{N+1} + \underbrace{\left(-\sum_{n_1=1}^N a_{n_1}\right)}_{=A_1}  i^N
+ \underbrace{\left( (-1)^2 \sum_{\substack{n_1,n_2=1 \\ n_1 \neq n_2}}^N
a_{n_1}a_{n_2}\right)}_{=A_2} i^{N-1}  + \ldots
+ \underbrace{\left( (-1)^N\prod_{n_1=1}^N a_{n_1}\right)}_{=A_N} i \ ,
\]
we have that
\[
i(i-1)\ldots(i-k+1) = i^k + \sum_{j=1}^{k-1}A_j i^{k-j}
\]
for $0<k<m$ and the appropriate definitions of the $A_j$; for
instance, $\displaystyle A_1 = - \sum_{n=1}^{k-1} n$.
Using the fact that a linear combination of solutions of
(\ref{homFDP}) is a solution of (\ref{homFDP}), we have that
$\{ u_i^{[0]} \}_{i=0}^\infty$ with $u_i^{(0)} = r^i$,
$\{ u_i^{[1]} \}_{i=0}^\infty$ with $u_i^{[1]} = v_i^{[1]} = i r^i$,
and in general
$\{ u_i^{[k]} \}_{i=0}^\infty$ with $1<k<m$ and
\[
u_i^{[k]} = v_i^{[k]} - \sum_{j=1}^{k-1} A_j u_i^{[j]} = i^k r^i
\]
are linearly independent solutions of (\ref{homFDP}).
\end{proof}

Suppose that $z_1$, $z_2$, \ldots, $z_q$ are the
roots of the characteristic polynomial of multiplicities $k_1$, $k_2$,
\ldots, $k_q$ respectively.  It follows from the previous proposition
that the general solution
$\{ u_i \}_{i=0}^\infty$ of (\ref{homFDP}) is given by
\[
u_i = \sum_{j=1}^q \left( \sum_{m=0}^{k_j-1}\,c_{j,m}\,i^m \right) z_j^i 
\]
for $i \geq 0$, where the constants $c_{j,m}$ are determined by the
initial conditions $u_0$, $u_1$, \ldots, $u_{s-1}$.  We note that
$\displaystyle s = \sum_{j=1}^q k_j$.

\begin{egg}
Consider the finite difference equation
\[
u_{i+3} -9\,u_{i+2} +24\,u_{i+1} -20\,u_i = 0
\]
for $i\geq 0$ with initial conditions $u_0 = u_1 = 0$ and $u_2 = 1$.
The characteristic polynomial is $z^3 -9z^2 +24z -20 = (z-2)^2(z-5)$.
There are two distinct roots: $2$ of multiplicity $2$ and $5$ of
multiplicity $1$.  The general solution is
\[
u_i = (c_{1.0} + c_{1,1}\,i)2^i + c_{2,0}\,5^i
\]
for $i \geq 1$.  From the initial conditions, we get
\begin{align*}
u_0 &= 0 \Rightarrow c_{1,0}+c_{2,0} = 0 \\
u_1 &= 0 \Rightarrow 2(c_{1,0} +c_{1,1}) + 5 c_{2,0} = 0 \\
u_2 &= 1 \Rightarrow (c_{1,0}+ 2 c_{1,1})\,2^2 + c_{2,0}\,5^2 = 1
\end{align*}
Solving, we find $c_{1,0} = -1/9$, $c_{2,0} = 1/9$ and
$c_{1,1} = -1/6$.

The solution $\{u_i\}_{i=0}^\infty$ is given by
$\displaystyle u_i = -\left(\frac{1}{9}+\frac{1}{6}\,i\right)2^i +
\frac{1}{9}\,5^i$ for $i=0$, $1$, $2$, \ldots
\end{egg}

\begin{theorem}
Suppose that (\ref{basicFDP}) is of order $s$ and let
$\{ u_i^{[k]} \}_{i=0}^\infty$ be the solution of the homogeneous finite
difference equation 
\begin{align*}
\sum_{j=0}^s\,a_j\,u_{i-j} &= 0 \quad, \quad i \geq s \\
u_i &= \delta_{k,i} \quad, \quad 0 \leq i < s
\end{align*}
for $0 \leq k < s$.  Then, the solution $\{ u_i \}_{i=0}^\infty$ of
(\ref{basicFDP}) is given by
\begin{equation} \label{non_hom_solA}
u_i = \sum_{k=0}^{s-1}\,v_i\,u_i^{[k]} + w_i \quad, \quad i \geq 0 \ ,
\end{equation}
where
\begin{equation} \label{non_hom_solB}
w_i =  \begin{cases}
\displaystyle \frac{1}{a_0}\,\sum_{k=0}^{i-s}\,C_{s+k}\,u_{i-k-1}^{[s-1]} &
\quad \text{if} \quad i\geq s \\
0 & \quad \text{if} \quad 0 \leq i < s
\end{cases}
\end{equation}
\label{solNonHomFDP}
\end{theorem}

\begin{proof}
It is easy to see that the first sum in (\ref{non_hom_solA}) satisfies
the homogeneous finite difference problem
\begin{align*}
\sum_{j=0}^s\,a_j\,u_{i-j} &= 0 \quad, \quad i \geq s \\
u_i &= v_i \quad, \quad 0 \leq i < s
\end{align*}

We now proof that (\ref{non_hom_solB}) satisfies
\begin{align*}
\sum_{j=0}^s\,a_j\,w_{i-j} &= C_i \quad, \quad i\geq s \\
w_i &= 0 \quad, \quad 0 \leq i < s
\end{align*}
We obviously have $w_i =0$ for $0 \leq i <s$ by definition of the
$w_i$.  We have that
\[
\sum_{j=0}^s\,a_j\,w_{i-j} = \frac{1}{a_0}\,
\sum_{j=0}^s\,a_j\left(\sum_{k=0}^{i-j-s}\,C_{s+k}\,u_{i-j-k-1}^{[s-1]}\right)
= \frac{1}{a_0}\,
\sum_{j=0}^s\,a_j\left(\sum_{k=0}^{i-s}\,C_{s+k}\,u_{i-j-k-1}^{[s-1]}\right)
\]
because $i-j-k-1 < s-1$ for $k>i-j-s$ implies that
$u_{i-j-k-1}^{[s-1]} = 0$.  To simplify the notation, we assume that
$u_i^{[s-1]} = 0$ for $i<0$.  Hence,
\[
\sum_{j=0}^s\,a_j\,w_{i-j} =
\frac{1}{a_0}\,\sum_{k=0}^{i-s}\,C_{s+k}
\left(\sum_{j=0}^s\,a_j\,u_{i-j-k-1}^{[s-1]}\right)
= \frac{1}{a_0}\,C_i\left(\sum_{j=0}^s\,a_j\,u_{s-j-1}^{[s-1]}\right) = C_i
\]
for $i\geq s$.   We note that
$s-1 \leq i-k-1 \leq i-1$ for $0\leq k \leq i-s$.  Hence, the second
equality comes from 
$\displaystyle \sum_{j=0}^s\,a_j\,u_{(i-k-1)-j}^{(s-1)} = 0$ 
for $i-k-1 \geq s$ because
$\{ u_i^{(s-1)} \}_{i=0}^\infty$ is a solution of the homogeneous
difference equation.  The last equality, for $i-k-1 = s-1$, comes from
\[
  u_{(i-k-1)-j}^{(s-1)} = u_{s-j-1}^{(s-1)}
= \begin{cases}
  1 & \quad \text{if} \quad j = 0 \\
  0 & \quad \text{if} \quad j > 0
\end{cases}  \qedhere
\]
\end{proof}

\subsection{Convergence}

Our study of the convergence of multistep methods will use the notion of
"root condition" that we first define.

If $F \equiv 0$ in (\ref{MSM_GF}), we get
$\displaystyle w_{i+1} = \sum_{k=0}^m a_k w_{i-k}$.
If we substitute $\lambda^i$ for $w_i$ in this expression, we get
$\displaystyle \lambda^{i+1} = \sum_{k=0}^m a_k \lambda^{i-k}$. 
If we multiply both sides of this equation by $\lambda^{m-i}$, we get 
$p(\lambda) = 0$ for
$\displaystyle p(\lambda) = - \lambda^{m+1} + \sum_{k=0}^m a_k \lambda^{m-k}$.

\begin{defn}
The {\bfseries characteristic polynomial}\index{Characteristic Polynomial}
of the multistep method (\ref{MSM_GF}) is the polynomial
$\displaystyle p(\lambda) = - \lambda^{m+1} + \sum_{k=0}^m a_k \lambda^{m-k}$.
\end{defn}

\begin{defn}
\begin{enumerate}
\item A multistep method satisfies the
{\bfseries root condition}\index{Root Conditon} if all the roots of its
characteristic polynomial have absolute values less 
than or equal to one and those equal to one are simple roots.
\item A multistep method is {\bfseries strongly stable}\index{Strongly Stable}
if all the roots of its characteristic polynomial have absolute values less
than one except for one root which is equal to one.
\item A multistep method is {\bfseries weakly stable}\index{Weakly Stable}
if it satisfies the root condition and has more than one root of absolute
value one.
\item A multistep method is {\bfseries unstable}\index{Unstable} if it does
not satisfy the root condition.
\end{enumerate}
\end{defn}

\begin{eggList}\label{comp_stab}
\begin{enumerate}
\item The characteristic polynomial of
 Adams-Bashforth method of order four is
 $p(\lambda) = - \lambda^4 + \lambda^3$.  $1$ is a root of 
 multiplicity one and $0$ is a root of multiplicity three.  The method
 is strongly stable.
\item The characteristic polynomial of the
 Adams-Bashforth method of order two from Example~\ref{comp_order} is
 $p(\lambda) = -\lambda^2 + 1$.  $1$ and $-1$  are the two roots of
 this polynomial.  The method is weakly stable.
\end{enumerate}
\end{eggList}

\begin{prop}
If the finite difference method in (\ref{MSM_GF}) satisfies (\ref{Fzero})
and is convergent, then (\ref{MSM_GF}) satisfies the root condition.
\label{SimpliesRC}
\end{prop}

\begin{proof}
We give a special initial value problem with specific values for the
$\delta_i$ in (\ref{MSM_GFpertub}) such that the multistep method is
not convergent if the root condition is not satisfied.

Consider the initial value problem
\begin{equation} \label{zzIVP}
\begin{split}
y'(t) &= 0 \quad , \quad  t_0 \leq t \leq t_f\\
y(a) &= 0
\end{split}
\end{equation}
Thus $F \equiv 0$ in (\ref{MSM_GF}).
If we assume that $\delta_{i+1} = 0$ for $m \leq i < N$, the
finite difference problem (\ref{MSM_GFpertub}) becomes 
\begin{equation} \label{zzFDP}
\begin{split}
u_{i+1} - \sum_{j=0}^m\,a_j\,u_{i-j} &= 0 \quad, \quad m \leq i < N \\
u_i &= \delta_i \quad, \quad 0 \leq i \leq m
\end{split}
\end{equation}

The solution of (\ref{zzIVP}) is $y(t) = 0$ for all $t$.  We show that
the finite difference problem is not convergent for our initial value
problem; namely, we do not have that
\begin{equation} \label{test_C_RC}
\lim_{h \rightarrow 0}\,\max_{0\leq i \leq N} \, | u_i | = 0 \ .
\end{equation}

Suppose that $c$ is a root of the characteristic polynomial $p(z)$
such that $|c|>1$.  Let
\[
u_i = \begin{cases}
h c^i & \quad \text{if}\quad c \in \RR \\
h \left( c^i + \bar{c}^i \right) & \quad \text{if}\quad c \in \CC \setminus \RR
\end{cases}
\]
for $0\leq i \leq N$, where $h = (t_f-t_0)/N$ as usual.
We have that $\displaystyle \{u_i\}_{i=0}^N$ is a solution of
(\ref{zzFDP}) if we set $\delta_i = u_i$ for $0\leq i \leq m$.
For $c \in \CC \setminus \RR$,
$\displaystyle \{u_i\}_{i=0}^N$ is linear combination of the two solutions,
$\displaystyle \{c^i\}_{i=0}^N$ and
$\displaystyle \{\overline{c}^i\}_{i=0}^N$.
However,
\[
|u_N| =
\begin{cases}
\displaystyle
\frac{t_f-t_0}{N} |c^N| & \quad \text{if} \quad c \in \RR \\[0.8em]
\displaystyle \frac{t_f-t_0}{N} |c^N + \bar{c}^N| &
\quad \text{if} \quad c \in \CC \setminus \RR
\end{cases}
\]
does not converge to $0$ as $N \to \infty$ (Remark~\ref{cjdiverges} below).
Thus (\ref{test_C_RC}) is not satisfied.

Suppose that $c$ is a root of the characteristic polynomial such that
$|c|=1$ and $c$ is not simple.  Let
\[
u_i = \begin{cases}
h i c^i & \quad \text{if} \quad c \in \RR \\
h i \left( c^i + \bar{c}^i \right) & \quad \text{if}\quad  c \in \CC
\setminus \RR
\end{cases}
\]
for $0 \leq i \leq N$, where $h = (t_f-t_0)/N$.
Again, $\{u_i\}_{i=0}^N$ is a solution of (\ref{zzFDP}) if we set
$\delta_i = u_i$ for $0\leq i \leq m$.
For $c \in \CC \setminus \RR$, $\displaystyle \{u_i\}_{i=0}^N$ is linear
combination of the two solutions: 
$\displaystyle \{i c^i\}_{i=0}^N$ and
$\displaystyle \{i \overline{c}^i\}_{i=0}^N$ (Proposition~\ref{rootHO}).
However,
\[
|u_N| =
\begin{cases}
\displaystyle
(t_f-t_0) |c^N| & \quad \text{if} \quad c \in \RR \\
\displaystyle
(t_f-t_0) |c^N + \bar{c}^N| & \quad \text{if} \quad c \in \CC \setminus \RR
\end{cases}
\]
does not converge to $0$ as $N \to \infty$ (Remark~\ref{cjdiverges} below).
Thus (\ref{test_C_RC}) is not satisfied.
\end{proof}

\begin{rmkList}
\begin{enumerate}
\item In the proof of the previous proposition, we could have used a
fixed value of $t \in [t_0,t_f]$ associated to another index than $N$.
Suppose that $t = t_{i(N)}$; namely, we have
\[
  \frac{t-t_0}{i(N)} = \frac{t_f-t_0}{N} = h
\]
or, stated differently,
\[
  i(N) = \frac{t-t_0}{t_f-t_0}\, N \ .
\]
We have that $i(N) = C N$ with $C = (t-t_0)/(t_f-t_0)$.
If we select an increasing sequence $\{N_j\}_{j=0}^\infty$ of positive
integers (e.g.\ $N_{j+1} = 2 N_j$) such that $C N_j$ reminds an
integer, then $t = t_{i(N_j)}$ is always one of the nodes of the
partition of $[t_0,t_f]$ and $i(N_j)$ increase proportionally to $N_j$
according to $i(N_j) = C N_j$.  It is then easy to modify the
reasoning in the proof of the previous proposition to show that
$u_{i(N_j)}$, the approximation of $y(t) = y(t_{i(N_j)})$, does not
converge to $0$ as $j \to \infty$.
\item In the proof of the previous proposition, we have used the
following claims:
\begin{enumerate}  
\item $\{|c^j|/j\}_{j=1}^\infty$ does not converge to $0$ if
$c\in \RR$ satisfies $|c|>1$.
\item $\{|c^j+\bar{c}^j|/j\}_{j=1}^\infty$ does not converge to $0$ if
$c\in \CC \setminus \RR$ satisfies $|c|>1$.
\item $\{|c^j|\}_{j=1}^\infty$ does not converge to $0$ if $c\in \RR$
satisfies $|c| = 1$.
\item $\{|c^j+\bar{c}^j|\}_{j=1}^\infty$ does not converge to $0$ if
$c\in \CC \setminus \RR$ satisfies $|c|=1$.
\end{enumerate}

The two cases where $c \in \RR$ are easy to prove because
$ |c|^j/j \to \infty$ as $j \to \infty$ when $|c| >1$, and
$|c|^j = 1$ for all $j$ when $|c|=1$.

If $c \in \CC \setminus \RR$, we have $c = |c| e^{i\theta}$ for some
$\theta \neq n\pi$ for $n \in \ZZ$.  Thus,
\[
 c^j+\bar{c}^j = |c|^j e^{j \theta\;i} + |c|^j e^{-j \theta\;i}
 = 2|c|^j \cos(j \theta) \ .
\]
We now show that there exists a strictly increasing sequence
$\{j_k\}_{k=1}^\infty$ of positive integers and a constant $C>0$
depending on $\theta$ such that $|\cos(j_k \theta)|\geq C$ for all $k$.

If $\displaystyle \theta = \frac{m \pi}{n}$ for two positive integer
$m$ and $n$ such that $m/n$ is in its reduced form and $n\neq 2$, then
we can take $j_k = 2 k n +1$ and $C = |\cos(\theta)|$.  We have
\[
  |\cos(j_k \theta)| = |\cos( (2kn+1) \theta)|
  = |\cos( 2km\pi + \theta)| = |\cos(\theta)| = C
\]
for all $k$.

If $\displaystyle \theta = \frac{m \pi}{2}$ for $m=1$ or $3$, then
we can take $j_k = 2k$ and $C = 1$.  We have
\[
  |\cos(j_k \theta)| = |\cos( 2k\theta)| = |\cos(k m \pi)| = 1
\]
for all $k$.

If $\theta/\pi \in \RR \setminus \QQ$ with $0 < \theta < 2\pi$, we
need to use the fact that $\{ e^{j\theta} \}_{j=0}^\infty$ is dense on
the unit circle.  Thus, there exist an infinite strictly increasing sequence
$\{j_k\}_{k=0}^\infty$ such that $j_k \theta$ is between $\pi/6$ and
$\pi/3$ modulo $2\pi$.  We then have
\[
  |\cos(j_k \theta)|
  \geq \left|\cos\left(\frac{\pi}{3}\right)\right| = \frac{1}{2}
\]
for all $k$.  We can take these $j_k$ and $C = 1/2$.

Hence,
\[
\frac{|c^{j_k}+\bar{c}^{j_k}|}{j_k}
= \frac{2|c|^{j_k} |\cos(j_k \theta)|}{j_k}
\geq 2C\, \frac{|c|^{j_k}}{j_k} \ .
\]
Since $\displaystyle \lim_{k\to \infty} \frac{|c|^{j_k}}{j_k} = \infty$
because $|c|>1$, we get that
$\displaystyle \lim_{k\to \infty} \frac{|c^j_k+\bar{c}^j_k|}{j_k} = \infty$.

Similarly,
\[
  |c^{j_k}+\bar{c}^{j_k}| = 2|c|^{j_k} |\cos(j_k \theta)|
  \geq 2C
\]
for $|c|=1$ and all $k$.  So
$\displaystyle \{|c^{j_k}+\bar{c}^{j_k}|\}_{k=0}^\infty$ does not
converge to $0$.
\end{enumerate}
\label{cjdiverges}
\end{rmkList}

\begin{prop}
If the finite difference method in (\ref{MSM_GF}) satisfies (\ref{Fzero})
and is zero-stable, then (\ref{MSM_GF}) satisfies the root
condition. \label{ZeroStabRootCond}
\end{prop}

\begin{proof}
We proceed as in the proof of Proposition~\ref{SimpliesRC}.
We give a special initial value problem with specific values for the
$\delta_i$ in (\ref{MSM_GFpertub}) such that the multistep method is
not zero-stable if the root condition is not satisfied.

Consider the initial value problem
\begin{align*}
y'(t) &= 0 \quad , \quad  t_0 \leq t \leq t_f\\
y(a) &= 0
\end{align*}
Thus $F \equiv 0$ in (\ref{MSM_GF}).
If we assume that $\delta_{i+1} = 0$ for $m \leq i < N$, the finite
difference problem (\ref{MSM_GFpertub}) becomes
\begin{equation} \label{zzFDPtwo}
\begin{split}
u_{i+1} - \sum_{j=0}^m\,a_j\,u_{i-j} &= 0 \quad, \quad m \leq i < N \\
u_i &= \delta_i \quad, \quad 0 \leq i \leq m
\end{split}
\end{equation}
Moreover, we consider the perturbed finite difference problem given by
(\ref{MSM_GFpertub}) with $\delta_i = 0$ for $0\leq i \leq N$; namely,
\[
\begin{split}
\tilde{u}_{i+1} - \sum_{j=0}^m\,a_j\,\tilde{u}_{i-j} &= 0 \quad,
\quad m \leq i < N \\
\tilde{u}_i &= \tilde{\delta}_i = 0 \quad, \quad 0 \leq i \leq m
\end{split}
\]
The solution of this perturbed finite difference problem is obviously
$\displaystyle \{ \tilde{u}_i \}_{i=0}^N$, where $\tilde{u}_i = 0$ for
$0 \leq i \leq N$.

We show that the multistep method is not zero-stable for our
initial value problem; namely, given $\epsilon>0$, there does not
exist $S$ and $h_0$ such that
\begin{equation} \label{test_S_RC}
\max_{0\leq i \leq N} |u_i - \tilde{u}_i | =
\max_{0\leq i \leq N} | u_i | < S\epsilon
\end{equation}
if $|\delta_i - \tilde{\delta}_i| = |\delta_i| < \epsilon$ for
$0 \leq i \leq N$ and $h<h_0$.

Suppose that $c$ is a root of the characteristic polynomial $p(z)$
such that $|c|>1$.  Let
\[
u_i = \begin{cases}
\delta\ c^i & \quad \text{if} \quad c \in \RR \\
\delta \left( c^i + \bar{c}^i \right) & \quad \text{if} \quad
c \in \CC \setminus \RR
\end{cases}
\]
for $0 \leq i \leq N$.  We have that $\{u_i\}_{i=0}^N$ is a solution of
(\ref{zzFDPtwo}) if we set $\delta_i = u_i$ for $0\leq i \leq m$.
We select $\delta$ small enough to get
$|\delta_i - \tilde{\delta}_i| = |u_i| < \epsilon$ for $0\leq i \leq m$.
However,
\[
|u_N| =
\begin{cases}
\displaystyle \delta\,|c|^N  & \quad \text{if}\quad c \in \RR \\
\displaystyle \delta\,|c^N + \bar{c}^N| &
\quad \text{if}\quad c \in \CC\setminus \RR
\end{cases}
\]
does not converge to $0$ as $N \to \infty$.  There are strictly
increasing sequences $\{N_j\}_{j=0}^\infty$ of positive integers
such that $\{|u_{N_j}|\}_{j=0}^\infty$ converges to  $\infty$
(Remark~\ref{cjdiverges}).
Thus, we can take $N_j$ large enough such that 
$h = (t_f-t_0)/N_j < h_0$ and $|u_{N_j}| > S\epsilon$ for whatever $S$
and $h_0$ that we choose.  Therefore,
contradicting (\ref{test_S_RC}).

Suppose that $c$ is a root of the characteristic polynomial $p(z)$
such that $|c|=1$ and $c$ is not simple.  Let
\[
u_i = \begin{cases}
\delta\,i c^i & \quad \text{if} \quad c \in \RR \\
\delta\, i \left( c^i + \bar{c}^i \right) & \quad \text{if}\quad  c \in \CC
\setminus \RR
\end{cases}
\]
for $0 \leq i \leq N$.
Again, $\{u_i\}_{i=0}^N$ is a solution of (\ref{zzFDP}) if we set
$\delta_i = u_i$ for $0\leq i \leq m$.  We also can select $\delta$
small enough to get
$|\delta_i - \tilde{\delta}_i| = |u_i| < \epsilon$ for $0\leq i \leq m$.
However,
\[
|u_N| =
\begin{cases}
\displaystyle
\delta\,N |c|^N & \quad \text{if}\quad c \in \RR \\
\displaystyle \delta\,N |c^N + \bar{c}^N| &
\quad \text{if}\quad c \in \CC \setminus \RR
\end{cases}
\]
does not converge to $0$ as $N \to \infty$.   There are strictly
increasing sequences $\{N_j\}_{j=0}^\infty$ of positive integers
such that $\{|u_{N_j}|\}_{j=0}^\infty$ converges to  $\infty$
(Remark~\ref{cjdiverges}).
Again, we can take $N_j$ large enough such that
$h = (t_f-t_0)/N_j < h_0$ and $|u_{N_j}| > S\epsilon$ for whatever $S$
and $h_0$ that we choose.  Therefore, contradicting (\ref{test_S_RC}).
\end{proof}

\begin{theorem}[Dahlquist]
Suppose that the finite difference method in (\ref{MSM_GF}) satisfies
(\ref{Fzero}) and (\ref{FDP_Lip}), and that
$\displaystyle \max_{0 \leq i \leq N} | \delta_i(h) | \leq
\delta(h) = O(h^2)$ in (\ref{MSM_GFpertub}).
If the finite difference problem (\ref{MSM_GF}) is consistent, then
(\ref{MSM_GF}) is convergent if and only if it satisfies the root
condition.  \label{Dahl_conv}
\end{theorem}

\begin{rmk}
It is not too outrageous to require
$\delta(h) = O(h^2)$ in the statement of Theorem~\ref{Dahl_conv}.

Suppose that $\displaystyle \left\{u_i\right\}_{i=0}^N$ is a
solution of
\[
\frac{1}{h} \left( u_{i+1} - \sum_{j=0}^m\,a_ju_{i-j}\right)
+ F(h,\VEC{t},\VEC{u},f) + \sigma_{i+1}(h) \quad , \quad m\leq i < N \ ,
\]
where $\sigma_{i+1}(h)$ represents the perturbation.  Then,
\[
u_{i+1} - \sum_{j=0}^m\,a_ju_{i-j} + h  F(h,\VEC{t},\VEC{w},f)
+ h\sigma_{i+1}(h) \quad , \quad m\leq i < N \ .
\]
We may set $\delta_i(h) = h \sigma_i(h)$.  So, the real assumption that we
make is that $\displaystyle \max_{0 \leq i \leq N} | \sigma_i(h) | \leq
\sigma(h) = O(h)$ near the origin.
\end{rmk}

\begin{proof}[Proof (of Dahlquist's theorem)]
That convergence implies that the root condition is satisfied is a
consequence of Proposition~\ref{SimpliesRC}.  We need only prove the
converse.  Suppose that the root condition is satisfied.

As mentioned in the previous remark, we may assume that
$\delta_i(h) = h \sigma_i(h)$ and $\delta(h) = h \sigma(h)$, where
both $\sigma_i(h)$ and $\sigma(h)$ are $O(h)$ near the origin.

If we subtract
\[
y(t_{i+1}) - \sum_{j=0}^m\,a_jy(t_{i-j})
- h F(h,\VEC{t},\VEC{y},f) = h \tau_{i+1}(h)
\]
from
\[
u_{i+1} - \sum_{j=0}^m\,a_j u_{i-j} - h F(h,\VEC{t},\VEC{u},f)
= h \sigma_{i+1}(h)
\]
for $m \leq i < N$, we get
\begin{equation} \label{Tzero}
\sum_{j=-1}^m\,a_j\,e_{i-j} = C_i \quad , \quad  m \leq i < N \ ,
\end{equation}
where $a_{-1} = -1$, $e_j = u_j - y(t_j)$ for $0 \leq j \leq N$ and
\[
C_i = -h \left( F(h,\VEC{t},\VEC{u},f) -
F(h,\VEC{t},\VEC{y},f) \right) - h ( \sigma_{i+1}(h) - \tau_{i+1}(h) )
\quad , \quad m \leq i < N \ .
\]

If we replace $j$ by $j-1$ and $i$ by $i-1$ in (\ref{Tzero}), we get
the finite difference equation
\begin{equation} \label{Tone}
\begin{split}
\sum_{j=0}^{m+1}\,a_{j-1}\,e_{i-j} = C_{i-1} &\quad \text{for} \quad
m < i \leq N \\
e_i = h\sigma_i(h) & \quad \text{for} \quad 0 \leq i \leq m
\end{split}
\end{equation}

Let $\displaystyle \left\{ u_i^{[k]} \right\}_{i=0}^\infty$ be the solution of
\begin{align*}
\sum_{j=0}^{m+1}\,a_{j-1}\,u_{i-j} &= 0 \quad , \quad i > m \\
u_i &= \delta_{k,i} \quad , \quad 0 \leq i \leq m
\end{align*}
for $0 \leq k \leq m$.
From Theorem~\ref{solNonHomFDP} (with $v_i= e_i$, $a_j$ replaced by
$a_{j-1}$, $C_i$ replaced by $C_{i-1}$ and $s=m+1$ in
(\ref{basicFDP})\,), the solution of (\ref{Tone}) is
\begin{align}
e_i &= \sum_{k=0}^m\,e_i\,u_i^{[k]}
+ \begin{cases}
\displaystyle \frac{1}{a_{-1}}\,
\sum_{k=0}^{i-m-1}\,C_{k+m}\,u_{i-k-1}^{[m]} & \quad \text{if} \quad
m<i \leq N \\
0  & \quad \text{if} \quad 0\leq i \leq m 
\end{cases} \nonumber  \\
&= \sum_{k=0}^m\,e_i\,u_i^{[k]}
- \begin{cases}
\displaystyle
\sum_{k=0}^{i-m-1}\,C_{k+m}\,u_{i-k-1}^{[m]} & \quad \text{if} \quad
m<i \leq N \\
0  & \quad \text{if} \quad 0\leq i \leq m 
\end{cases} \label{Ttwo}
\end{align}

The root condition implies that there exist a constant $Q \geq 1$ such
that $| u_i^{[k]} | \leq Q$ for all $i$ and $k$.  Recall that all
solutions of the homogeneous difference equation
\begin{equation} \label{Tthree}
\sum_{j=0}^{m+1}\,a_{j-1}\,u_{i-j} = 0 \quad , \quad i > m \ ,
\end{equation}
are linear combinations of solutions with terms of the
form $e_i = i^n c^i$, where $c$ is a root of the characteristic polynomial
\[
  \sum_{j=0}^{m+1}\, a_{j-1}\,z^{m+1-j} =
  \sum_{j=-1}^m\, a_j\,z^{m-j} = 0
\]
and $n$ is a non-negative integer smaller than the multiplicity of
$c$.

It follows from the definition of $C_i$ and (\ref{FDP_Lip}) that
\begin{align*}
|C_{k+m}| &\leq h\left(R\,\sum_{j=k}^{k+m+1}\,|e_j| +\sigma(h) + \tau(h)
\right)
\leq h\left(R\,(m+2)\,\max_{k\leq j \leq k+m+1} |e_j|
+ \sigma(h) + \tau(h) \right) \\
& \leq h\left(R\,(m+2)\,\max_{0\leq j \leq i} |e_j| +\sigma(h) + \tau(h)
\right)
\end{align*}
for  $0\leq k \leq i-m -1$.  Hence, from (\ref{Ttwo}), we get
\begin{align}
|e_i| & \leq
\begin{cases}
\displaystyle Q(m+1)\,\max_{0\leq j \leq m} |e_j| & \\
\displaystyle \qquad + Q(i-m+1)\,h \bigg( R\,(m+2)\,\max_{0\leq j \leq i} |e_j|
+ \sigma(h) + \tau(h) \bigg) & \quad \text{if}\quad  m< i \leq N\\[1em]
\displaystyle Q(m+1)\,\max_{0\leq j \leq m} |e_j| & \quad \text{if}
\quad 0\leq i \leq m
\end{cases}  \nonumber \\
& \leq
\begin{cases}
\displaystyle Q(m+1)\,\max_{0\leq j \leq m} |e_j| & \\
\displaystyle \qquad + Qih \bigg( R\,(m+2)\,\max_{0\leq j \leq i} |e_j|
+ \sigma(h) + \tau(h) \bigg) & \quad \text{if}\quad  m< i \leq N\\[1em]
\displaystyle Q(m+1)\,\max_{0\leq j \leq m} |e_j| & \quad \text{if}
\quad 0\leq i \leq m
\end{cases}  \label{Tfour}
\end{align}

We get from (\ref{Tfour}) that
\begin{equation}\label{Tsix}
\max_{0\leq j \leq i} |e_j| \leq
Q(m+1)\,\max_{0\leq j \leq m} |e_j|
+ M i h\,\max_{0\leq j \leq i} |e_j|
+ Q i h \left(\sigma(h) + \tau(h) \right) \quad , \quad m < i \leq N \, ,
\end{equation}
where $M= RQ(m+2)$.

We choose $h$ small enough (i.e.\ $N$ large enough) such
that $1/(2Mh) > m+1$ and consider $m < i \leq 1/(2Mh)$.  We get from
(\ref{Tsix}) that
\[
\max_{0\leq j \leq i} |e_j| \leq
Q(m+1)\,\max_{0\leq j \leq m} |e_j| + \frac{1}{2}\,\max_{0\leq j \leq i} |e_j|
+ Q i h\left(\sigma(h) + \tau(h) \right)
\]
for $m < i \leq 1/(2 M h)$. If we isolate 
$\displaystyle \max_{0\leq j \leq i} |e_j|$, we get
\begin{equation}\label{Tfive}
\max_{0\leq j \leq i} |e_j| \leq
2\,Q\left( (m+1)\,\max_{0\leq j \leq m} |e_j|
+\frac{1}{2M}\left(\sigma(h) + \tau(h) \right) \right)
\end{equation}
for $m < i \leq 1/(2 M h)$.

Let $i_k = \intpt{k/(2M h)}$ for $1 \leq k \leq K$, where
$K=\intpt{2M(t_f-t_0)}$.  Recall that $\intpt{a}$
is the largest integer smaller than or equal to $a$.  Let
$i_{-1} = 0$, $i_0=m$ and $i_{K+1} = N$.

If we repeat the same argument on the interval
$\displaystyle I_k = [t_0+k/(2M), t_0+(k+1)/(2M)]$ for $1 \leq k \leq K$
with the initial conditions at $t_j$ given by $e_j$ for
$\displaystyle i_k-m \leq j \leq i_k$, we get
\begin{equation}\label{Tseven}
\max_{i_k-m \leq j \leq i} |e_j| \leq 2 Q \left( (m+1)
\max_{i_1-m\leq j \leq i_1} |e_j|
+\frac{1}{2 M}\left(\sigma(h) + \tau(h) \right) \right)
\end{equation}
for $i_k < i \leq i_{k+1}$ and $1 \leq k \leq K$.

Let $\displaystyle E_k = \max_{i_k \leq i \leq i_{k+1}} |e_i|$ for
$-1 \leq k \leq K$.  We deduce from (\ref{Tfive}) that
\[
E_0 \leq 2\,Q\left( (m+1)\,E_{-1}
+\frac{1}{2M}\left(\sigma(h) + \tau(h) \right) \right)
\]
and from (\ref{Tseven}) that
\[
E_k \leq 2 Q \left( (m+1) E_{k-1}
+\frac{1}{2 M}\left(\sigma(h) + \tau(h) \right) \right)
\]
for $1 \leq k \leq K$.  By induction, we find
\begin{align*}
E_k & \leq
(2Q(m+1))^{k+1}\,E_{-1} + \left(\frac{1-(2Q(m+1))^{k+1}}{1-2Q(m+1)}\right)
\frac{Q}{M }\,\left(\sigma(h) +\tau(h)\right)  \\
& \leq
(2Q(m+1))^{K+1}\,E_{-1} + \left(\frac{1-(2Q(m+1))^{K+1}}{1-2Q(m+1)}\right)
\frac{Q}{M}\,\left(\sigma(h) +\tau(h)\right)
\end{align*}
for $0\leq k \leq K$ because $Q(m+1) \geq 1$ by assumption.
Therefore,
\[
\max_{0\leq i \leq N} | u_i - y(y_i) |
= \max_{-1\leq k \leq K} E_k \rightarrow 0
\]
as $h \rightarrow 0$ independently of $f$.  Recall that
$E_{-1} \rightarrow 0$ as $h \rightarrow 0$ because
$\displaystyle E_{-1} = \max_{0\leq j \leq m} |e_j|
= \max_{0\leq j \leq m} |\delta_j(h)| = O(h^2)$.
\end{proof}

\begin{rmk}
If we use the definition of convergence given in
remark~\ref{AltDefConv} which is equivalent to assuming that
$\delta_i(h) \equiv 0$ for all $i$ in (\ref{MSM_GFpertub}), the
previous theorem can be stated as follows.

Suppose that the finite difference method in (\ref{MSM_GF}) satisfies
(\ref{Fzero}) and (\ref{FDP_Lip}).  If the finite difference
problem (\ref{MSM_GF}) is consistent, then (\ref{MSM_GF}) is convergent
if and only if it satisfies the root condition.

There is no reference to perturbations $\delta_i$ in this statement.
\label{DahlquistWOdelta}
\end{rmk}

\begin{theorem}
Suppose that the finite difference method in (\ref{MSM_GF}) satisfies
(\ref{Fzero}) and (\ref{FDP_Lip}), and that
$\displaystyle \max_{0 \leq i \leq N} | \delta_i(h) | \leq
\delta(h) = O(h^2)$ in (\ref{MSM_GFpertub}).
Then (\ref{MSM_GF}) is zero-stable if and only if it satisfies the
root condition.
\end{theorem}

\begin{proof}
We have from Proposition~\ref{ZeroStabRootCond} that zero-stable implies
root condition.  We only need to prove the converse.

The proof is similar to the proof of the previous theorem.
If we subtract
\[
u_{i+1}^{[1]} - \sum_{j=0}^m\,a_j u_{i-j}^{[1]} - h
F(h,\VEC{t},\VEC{u}^{[1]},f) = \delta_{i+1}^{[1]}(h)
\]
from
\[
u_{i+1}^{[2]} - \sum_{j=0}^m\,a_j u_{i-j}^{[2]} - h
F(h,\VEC{t},\VEC{u}^{[2]},f) = \delta_{i+1}^{[2]}(h)
\]
for $m \leq i < N$, we get
\[
\sum_{j=-1}^m\,a_j\,e_{i-j} = C_i \quad , \quad  m \leq i < N \ ,
\]
where $a_{-1} = -1$, $\displaystyle e_j = u_j^{[2]} - u_j^{[1]}$ for
$0 \leq j \leq N$ and
\[
C_i = -h \left( F(h,\VEC{t},\VEC{u}^{[2]},f)
- F(h,\VEC{t},\VEC{u}^{[1]},f) \right) -
\left( \delta_{i+1}^{[2]}- \delta_{i+1}^{[1]} \right)
\quad , \quad m \leq i < N \ .
\]

Let $\delta_i^{[j]}(h) = h \sigma_i^{[j]}(h)$ for $j=1$ and $2$.
Moreover, let
\[
\sigma(h) = \max_{0\leq i \leq N} \left|\sigma_i^{[2]}(h)
- \sigma_i^{[1]}(h)\right| \ .
\]
We then have that\footnote{Instead of requiring
$\displaystyle \max_{0 \leq i \leq N} | \delta_i(h) | \leq
\delta(h) = O(h^2)$ in the statement of the theorem, we could have only
required
$\displaystyle \max_{0 \leq i \leq N}
\left| \delta_i^{[2]}(h) - \delta_i^{[1]}(h) \right| \leq
\delta(h) = O(h^2)$.}
\[
  \left| \delta_i^{[2]}(h) - \delta_i^{[1]}(h) \right| \leq h \sigma(h)
\quad , \quad 0 \leq i \leq N \ .
\]
Proceeding as we did in the proof of Dahlquist's theorem, we show that
\[
E_k \leq
(2Q(m+1))^{K+1}\,E_{-1} + \left(\frac{1-(2Q(m+1))^{K+1}}{1-2Q(m+1)}\right)
\frac{Q}{M}\,\sigma(h)
\]
for $0\leq k \leq K$, where
\[
E_{-1} = \max_{0 \leq i \leq m} \left| e_i\right|
= \max_{0 \leq i \leq m} \left| u_i^{[2]}- u_i^{[1]} \right|
= \max_{0 \leq i \leq m} \left| h\sigma_i^{[2]} - h \sigma_i^{[1[} \right|
\leq h \sigma(h)
\]
because $\displaystyle u_i^{[1]} = y(t_i) + \delta_i^{[1]}(h)$ and
$\displaystyle u_i^{[2]} = y(t_i) + \delta_i^{[2]}(h)$ for
$0\leq i \leq m$.  Thus
\[
E_k \leq \left(
(2Q(m+1))^{K+1}h + \left(\frac{1-(2Q(m+1))^{K+1}}{1-2Q(m+1)}\right)
\frac{Q}{M}\right) \sigma(h)
\]
for $-1 \leq k \leq M$.

Let
\[
  K = (2Q(m+1))^{K+1} + \left(\frac{1-(2Q(m+1))^{K+1}}{1-2Q(m+1)}\right)
  \frac{Q}{M} \ .
\]
Given $\epsilon >0$, choose $h_0<1$ such that
$\sigma(h) < \epsilon$ for $|h|<h_0$.  This is possible because
$\sigma(h) \to 0$ as $h \to 0$.  Then
\[
\max_{0\leq i \leq N} | u_i^{[1]} - u_i^{[2]} |
= \max_{-1\leq k \leq K} E_k < K \epsilon
\]
namely, the definition of zero-stability is satisfied with these values
of $K$ and $h_0$. 
\end{proof}

We end this section by stating (without proofs) a couple of results
providing some constraints on the maximal order of some
numerical methods if stability and convergence have to be preserved.

\begin{theorem}[Dahlquist First Barrier]
The maximum order of a zero-stable implicit multistep method
of the form (\ref{MULTISTEP}) is $m+3$ when $m$ is even and $m+1$ when $m$
is odd.  For a zero-stable explicit multistep method of the form
(\ref{MULTISTEP}), the maximum order is $m+1$.
\end{theorem}

\begin{prop}
The Backward Difference Formulae satisfy the root condition
if and only if $0\leq m \leq 5$ (i.e. they are method of order $1$ to
$6$ inclusively).  Since these methods are consistent by
construction, they are convergent if and only if $0\leq m \leq 5$. 
\end{prop}

\subsection{Absolute Stability and A-Stability}

Stability is a delicate concepts.  There are several ways to define
it.  The goal is always to ensure (as much as possible) that errors do not
increase as we iterate; namely, that $|u_i - y(t_i)|$ does not increase as
$i$ increases, where $u_i$ is the numerical approximation of $w_i$.

To define the new notion of stability, we consider the simple linear
initial value problem
\begin{equation} \label{RKThe_equ}
\begin{split}
y'(t) & = \mu\, y(t) \quad , \quad t_0 \leq t \leq t_f \\
y(t_0) & = y_0
\end{split}
\end{equation}
where $\RE \mu < 0$.

We will start with Runge-Kutta methods before turning our attention to
multistep methods (with $m>0$).

\subsubsection{Runge-Kutta Methods}

If we apply the general Runge-Kutta method given
in Definition~\ref{GFRKM} to (\ref{RKThe_equ}), we get
\begin{align*}
w_{i+1} &= w_i + h \sum_{j=1}^s \gamma_j K_j \\
w_0 &= y_0
\end{align*}
for $0 \leq i < N$, where
\[
K_j = \mu \left( w_i + h \sum_{m=1}^s\,\beta_{j,m}K_m \right)
\]
for $1 \leq j \leq s$.

We can rewrite these two formulae in a more compact way using vectors and
matrices.  Let
\[
B = \begin{pmatrix} \beta_{1,1} & \beta_{1,2} & \ldots & \beta_{1,s} \\
\beta_{2,1} & \beta_{2,2} & \ldots & \beta_{2,s} \\
\vdots & \vdots & \ddots & \vdots \\
\beta_{s,1} & \beta_{s,2} & \ldots & \beta_{s,s}
\end{pmatrix}
\ , \quad
\VEC{c} = \begin{pmatrix} \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_s
\end{pmatrix}
\ , \quad
\VEC{K} = \begin{pmatrix} K_1 \\ K_2 \\ \vdots \\ K_s \end{pmatrix}
\ , \quad
\VEC{u} = \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix}
\quad \text{and} \quad
\VEC{w}_i = w_i \VEC{u} \ .
\]
We can rewrite the formulae above as
\begin{align*}
w_{i+1} &= w_i + h \VEC{c}^{\top} \VEC{K} \\
w_0 &= y_0
\end{align*}
for $0\leq i < N$, where
\[
\VEC{K} = \mu \left( \VEC{w}_i + h B \VEC{K}\right) \ .
\]
If we solve this last equation for $\VEC{K}$, we get
\[
  \VEC{K} = \mu\left( \Id - h\mu B\right)^{-1} \VEC{w}_i
\]
Thus
\begin{equation}\label{RK_stabA1}
  w_{i+1} = w_i + h\mu \VEC{c}^{\top}(\Id-h\mu B)^{-1} \VEC{w}_i
= w_i\left(1 + h\mu \VEC{c}^{\top}(\Id-h\mu B)^{-1}\VEC{u} \right)
\end{equation}
for $0\leq i <N$.

\begin{defn}
The {\bfseries region of absolute stability}\index{Runge-Kutta!Region of
Absolute Stability} of a Runge-Kutta method 
is the set of all values $h\mu \in \CC$ such that
$\displaystyle \lim_{i\rightarrow +\infty} w_i = 0$ for all solutions
$\{w_i\}_{i=0}^\infty$ of the difference equation associated to the
Runge-Kutta method given in Definition~\ref{GFRKM} applied to the initial
value problem (\ref{RKThe_equ}).  \label{DefnABSAS}

A Runge-Kutta method is
{\bfseries A-stable}\index{Runge-Kutta!A-Stable} if the region of absolute
stability contains the half-plane to the left of the imaginary axis
(complex numbers with a negative real part.)
\end{defn}

\begin{rmk}
In the previous definition, we have to remember that we assume that
$\RE \mu <0$.  Hence, all solutions $y(t) = e^{\mu (t-t_0)}y_0$ of the
differential equation (\ref{RKThe_equ}) satisfy
$\displaystyle \lim_{t\to \infty} y(t) = 0$.
\end{rmk}

\begin{egg}
We find the region of absolute stability for the Runge-Kutta
method of order two given in Definition~\ref{RK2}.  The computations
for the other explicit Runge-Kutta methods are similar but more convoluted.

The recursive formula for the Runge-Kutta method of order two is
\[
w_{i+1} = w_i + h \left( \gamma_1 f(t_i,w_i) + \gamma_2 f(t_i +
\alpha_2 h,w_i + \beta_{2,1} h f(t_i,w_i)) \right) \  .
\]
If we use the Runge-Kutta method of order two to solve the
non-trivial initial value problem (\ref{RKThe_equ}), the iterative
formula becomes
\begin{equation}\label{RKAstabequ}
\begin{split}
w_{i+1} &= w_i + h\left( \gamma_1 \mu\, w_i
+ \gamma_2 \mu\, (w_i + \beta_{2,1} h \mu\, w_i) \right) \\
 &= \left( 1 + (\gamma_1 + \gamma_2)\mu\, h + \gamma_2 \beta_{2,1}
(\mu\,h)^2 \right) w_i \ .
\end{split}
\end{equation}
If we substitute $\lambda^i$ for $w_i$, we get
\[
\lambda^{i+1} = \left( 1 + (\gamma_1 + \gamma_2)\mu\, h + \gamma_2
\beta_{2,1} (\mu\, h)^2 \right) \lambda^i
\]
and, after dividing by $\lambda^i$, we get the only non-null root
\[
\lambda = \left( 1 + (\gamma_1 + \gamma_2)\mu\, h + \gamma_2
\beta_{2,1} (\mu\, h)^2 \right) \ .
\]
Hence, the general solution of (\ref{RKAstabequ}) is
$w_i = \lambda^i w_0$ for $i=0$, $1$, $2$, \ldots\ From the condition
$\gamma_1 + \gamma_2 = 1$ and $\beta_{2,1} \gamma_2 = 1/2$
(Definition~\ref{RK2}), we get
\begin{equation} \label{abs_stab_cond_RK2}
\lambda = \left( 1 + \mu\, h + \frac{1}{2} (\mu\, h)^2 \right) \ .
\end{equation}
We need
\begin{equation} \label{stab_cond_RK2}
\left| \lambda \right| =
\left| 1 + \mu\, h + \frac{1}{2} (\mu\, h)^2 \right| < 1
\end{equation}
to get $\displaystyle \lim_{i\to \infty} w_i = 0$.

The region of absolute stability of the Runge-Kutta methods of order
two is the set of all $h\mu \in \CC$ such that
(\ref{stab_cond_RK2}) is satisfied.  The set of values
$z \in \CC$ such that $1 + z + z^2/2$ is on the unit circle
is the black curve shown in Figure~\ref{reg_abs_stab_RK2}.
To draw this black curve, we may use the fact that
$1 + z + z^2/2 = e^{i\theta}$ is a quadratic equation whose
solutions are given by $z = -1 \pm \sqrt{-1+2 e^{i \theta}}$.
The number $i$ in the previous sentence is the complex number such
that $i^2 = -1$ and not the index $i$ in the Runge-Kutta method.
As $\theta$ goes from $0$ to $2\pi$, we move along the (upper and
lower branches of the) black curve.

The region of absolute stability is inside the black curve.
To determine if the region of absolute stability is inside or outside
the continuous curve, we have drawn the curve generated
by the set of points $z$ such that
$\left| 1 + z + z^2/2 \right| = 1.2$, the red curve in
Figure~\ref{reg_abs_stab_RK2}, and the curve generated by the set of
points $z$ such that $\left| 1 + z + z^2/2 \right| = 0.8$, the blue
curve in Figure~\ref{reg_abs_stab_RK2}.  In the first case, the points
$z$ correspond to values of $h\mu$ for which
$| \lambda | > 1$, so they are outside the region of absolute
stability, while in the second case they correspond to values 
of $h\mu$ for which $| \lambda | < 1$, so they are inside the
region of absolute stability.

One can show that all the Runge-Kutta methods of order $p$ fixed
have the same region of absolute stability.  It is certainly true for
$p=2$ as we have just shown.
\end{egg}

\mathF{init_value_probl/stability_ex5}{10cm}{Absolute stability region for the
Runge-Kutta method of order two}{Boundaries of the region
of absolute stability for the Runge-Kutta method of order two (black
curve) and the curve generated by the set of points $z \in \CC$ such that
$\left|1 + z + z^2/2 \right|=1.2$ and $0.8$ (red and blue curves
respectively).  The region of absolute stability is inside the black
curve.}{reg_abs_stab_RK2}

\begin{prop}
Consider the general Runge-Kutta method from Definition~\ref{GFRKM}.
There exists a rational function $r:\CC \to \CC$ such that
$\displaystyle w_i = (r(h\mu))^i w_0$ for $0 \leq i \leq N$.
If the Runge-Kutta method is explicit, $r$ is a polynomial.
\label{rat_funct}
\end{prop}

\begin{proof}
We get by induction from (\ref{RK_stabA1}) that
\[
w_i = w_0\left(1 + h\mu \VEC{c}^\top(\Id-h\mu B)^{-1}\VEC{u} \right)^i
\]
for $0\leq i \leq N$.  Thus, we have to show that
\[
r(z) = 1 + z\VEC{c}^\top (\Id - zB)^{-1} \VEC{u}
\]
is a rational function.
It is enough to show that $\VEC{c}^\top(\Id-zB)^{-1}\VEC{u}$ is a
rational function in $z$.  This comes from
\[
(\Id-zB)^{-1} = \frac{1}{\det(\Id-zB)}\,(\text{adj}(\Id-zB))^\top \ ,
\]
where $\det(\Id-zB)$ is a polynomial in $z$ of degree at most $s$, and
$\text{adj}(\Id-zB)$ is an \nm{s}{s} matrix whose $(i,j)$ entry is
the {\bfseries cofactor} $(-1)^{i+j} \det A_{i,j}$, where $A_{i,j}$ is
obtained from $\Id - zB$ by removing the $i^{th}$ row and $j^{th}$ column.
The cofactors are polynomials in $z$ of degree at most $s-1$.
Hence, $\VEC{c}^\top(\Id-zA)^{-1}\VEC{u}$ is the quotient
of a polynomial of degree at most $s-1$ by a polynomial of degree at
most $s$.

If the Runge-Kutta method is explicit, $\Id-zB$ is a lower-triangular
matrix with only $1$ on the diagonal.  Thus $\det(\Id-zB) =1$.
\end{proof}

\begin{cor}
The stability domain of a general Runge-Kutta method is
$\{ z \in \CC : |r(z)| < 1 \}$.
\label{RKstabReg}
\end{cor}

\begin{proof}
The result follows from
\[
\lim_{i\rightarrow +\infty} w_i = 0
\Leftrightarrow \lim_{i\rightarrow +\infty}(r(h\mu))^i = 0
\Leftrightarrow |r(h\mu)| < 1 \ .   \qedhere
\]
\end{proof}

\begin{rmk}
To motivate the definition of stability, suppose that $u_i$ is the
numerical approximation of $w_i$.  Let
$r(h\mu) = 1 + h\mu \VEC{c}^{\top}(\Id-h\mu B)^{-1}\VEC{u}$.  We have from
(\ref{RK_stabA1}) that
\[
 w_{i+1} = r(h\mu) w_i \quad , \quad 0\leq i < N \ .
\]
We have by definition of the local truncation error that
\begin{equation}\label{RKstabMotivA}
y_{i+1}- w_{i+1} = r(h\mu) (y_i- w_i) + h \tau_{i+1}(h) \quad ,
\quad 0\leq i < N \ ,
\end{equation}
where $y_i = y(t_i)$ for $0\leq i \leq N$.

Moreover, we may assume that $u_i$ is the exact solution of
\[
  u_{i+1} = r(h\mu) u_i + \delta_i \quad , \quad 0\leq i < N \ ,
\]
where $\delta_i$ represents the error for each computation.  Hence,
\begin{equation}\label{RKstabMotivB}
  u_{i+1} - w_{i+1} = r(h\mu) (u_i -w_i) + \delta_i \quad , \quad 0\leq i < N \ .
\end{equation}

If we subtract (\ref{RKstabMotivB}) from (\ref{RKstabMotivA}), we get
\begin{equation}\label{RKstabMotivC}
  (y_{i+1}-u_{i+1}) = r(h\mu) (y_i-u_i) + h\tau_{i+1}(h) - \delta_i \quad ,
\quad 0\leq i < N \ .
\end{equation}
We now prove by induction that
\begin{equation}\label{RKstabMotivD}
|y_i-u_i| \leq |r(h\mu)|^i\, |y_0-u_0| + \sum_{j=0}^{i-1}
|r(h\mu)|^j \big( |h\tau_{i-j}(h)| + |\delta_{i-1-j}|\big) \quad ,
\quad 0 < i \leq N \ .
\end{equation}
It follows from (\ref{RKstabMotivC}) with $i=0$ that
(\ref{RKstabMotivD}) is true for $i=1$.  Suppose that (\ref{RKstabMotivD})
is true. then (\ref{RKstabMotivC}) and the induction hypothesis imply that
\begin{align*}
|y_{i+1}-u_{i+1}| &\leq |r(h\mu)|\,|y_i-u_i| + |h\tau_{i+1}(h)| + |\delta_i| \\
&\leq |r(h\mu)| \left(|r(h\,u)|^i |y_0-u_0| + \sum_{j=0}^{i-1}
|r(h\mu)|^j \big( |h\tau_{i-j}(h)| + |\delta_{i-1-j}|\big) \right)
+ |h \tau_{i+1}(h)| + |\delta_i| \\
&= |r(h\,u)|^{i+1} |y_0-u_0| + \sum_{j=0}^{i-1}
|r(h\mu)|^{j+1} \big(|h\tau_{i-j}(h)| + |\delta_{i-1-j}|\big)
+ |h\tau_{i+1}(h)| + |\delta_i| \\
&= |r(h\,u)|^{i+1} |y_0-u_0| + \sum_{j=1}^i
|r(h\mu)|^j \big(|h\tau_{i+1-j}(h)| + |\delta_{i-j}|\big)
+ |h\tau_{i+1}(h)| + |\delta_i| \\
&= |r(h\,u)|^{i+1} |y_0-u_0| + \sum_{j=0}^i
|r(h\mu)|^j \big(|h\tau_{i+1-j}(h)| + |\delta_{i-j}|\big)
\end{align*}
This is (\ref{RKstabMotivD}) with $i$ replaced by $i+1$.  Thus completing
the proof by induction.

Suppose that the Runge-Kutta method is consistent; namely,
$\displaystyle \max_{0\leq i <N} |\tau_{i+1}(h)| \leq \tau(h) \to 0$
as $h\to 0$.  Moreover, suppose that $|\delta_i| < \delta$ for all $i$.

If $h\mu$ is in the region of absolute stability, then $|r(h\mu)|<1$.
Hence, (\ref{RKstabMotivD}) yields
\begin{align*}
|y_i-u_i| &\leq |r(h\,u)|^i\, |y_0-u_0| +
\sum_{j=0}^{i-1} |r(h\mu)|^j \big( h\tau(h) + \delta \big) \\
&= |r(h\,u)|^i\, |u_0-w_0| + \frac{1 - |r(h\mu)|^i}{1-|r(h\mu)|}\,
\big( h\tau)h) + \delta \big) \\
&\leq |u_0-w_0| + \frac{h\tau(h)}{1-|r(h\mu)|} + \frac{\delta}{1-|r(h\mu)|}
\quad , \quad 1<i \leq N \ .
\end{align*}
Thus, the error of the approximation $u_i$ does not increase as $i$
increases.

If we consider the previous example for the Runge-Kutta method of
order two, we have $r(h\mu) = 1 + h\mu + (h\mu)^2/2$.  Then
\begin{align*}
1 - |r(h\mu)|
&= 1 - \left( 1 - r(h\mu)\,\overline{r(h\mu)} \right)^{1/2}
= 1 - \left( 1 - (\mu + \overline{\mu})h + O(h^2)\right)^{1/2} \\
&= 1 - \left( 1 - \frac{(\mu + \overline{\mu})h}{2} + O(h^2)\right)
= \frac{(\mu + \overline{\mu})h}{2} + O(h^2) \ .
\end{align*}
Thus,
\[
\frac{h\tau(h)}{1-|r(h\mu)|}
= \frac{\tau(h)}{(\mu + \overline{\mu})/2 + O(h)} \to 0
\]
as $h\to 0$.  However, since $r(h\mu) \to 1$ as $h \to 0$, 
$\delta/(1-|r(h\mu)|)$ increases as $h\to 0$.  So, the numerical
approximations $u_i$ may not get better as $h \to 0$.  As for the
Euler's method (see the remark after Theorem~\ref{Eulererrorbound}),
we may suspect that there is an optimal value of $h$ to reduce
round off errors.  This will require a thorough analysis of
$\displaystyle \frac{h\tau(h)}{1-|r(h\mu)|} + \frac{\delta}{1-|r(h\mu)|}$.
\end{rmk}

\begin{cor}
No explicit Runge-Kutta method is A-stable
\end{cor}

\begin{proof}
For an explicit Runge-Kutta method, the function $r$ in
Proposition~\ref{rat_funct} is a polynomial.  There is no polynomial $r$ of
degree greater than zero that is bounded on
$\{ z : \RE z < 0 \}$.  If $r$ is constant, then
$r(z) = 1$ for all $z$ because $r(0)=1$.  So, there is no polynomial
$r$ such that $|r(z)| < 1$ for all $z$ in $\{ z : \RE z < 0 \}$.
\end{proof}

\begin{egg}
Consider the Runge-Kutta method given by the Butcher array
\[
\begin{array}{c|cc}
0 & 1/4 & -1/4 \\
2/3 & 1/4 & 5/12 \\
\hline
 & 1/4 & 3/4
\end{array}
\]
So
$B= \begin{pmatrix} 1/4 & -1/4 \\ 1/4 & 5/12 \end{pmatrix}$
and
$\VEC{c}= \begin{pmatrix} 1/4 \\ 3/4 \end{pmatrix}$.  Thus,
$\Id-z B = \begin{pmatrix} 1-z/4 & z/4 \\ -z/4 & 1-5z/12 \end{pmatrix}$
and 
\begin{align*}
r(z) &= 1 +z\VEC{c}^\top(\Id-z B)^{-1}\VEC{u}
= 1+ \frac{z}{(1-z/4)(1-5z/12) + z^2/16}
\,\VEC{c}^\top
\begin{pmatrix}
1-5z/12 & -z/4 \\
z/4 & 1 - z/4
\end{pmatrix}
\VEC{u} \\
&= \frac{1+z/3}{1-2z/3+ z^2/6} \ .
\end{align*}
To show that this method is A-stable, we show that
$|r(z)| < 1$ for $z=\rho e^{i\theta}$ with $\rho>0$ and
$\pi/2 < \theta < 3\pi/2$.  We have
\begin{align*}
|r(z)| < 1 &\Leftrightarrow
\bigg| 1 + \frac{\rho e^{i\theta}}{3} \bigg|^2 <
\bigg| 1-\frac{2}{3}\rho e^{i\theta} +\frac{1}{6}\rho^2 e^{2i\theta}
\bigg|^2 \\
&\Leftrightarrow
2\rho (1+\frac{1}{9}\rho^2) \cos(\theta) <
\frac{2}{3} \rho^2 \cos^2(\theta) +\frac{1}{36} \rho^4 \; .
\end{align*}
Since the last inequality is always true for $\rho>0$ and
$\pi/2 < \theta < 3\pi/2$ (because
$2\rho (1+\rho^2/9) >0$, $\cos(\theta) <0$ and
$2\rho^2 \cos^2(\theta)/3 + \rho^4/36 > 0$), the
method is A-stable.  We will see shortly that this is typical for
a large class of implicit Runge-Kutta methods.
\label{AS_IRK}
\end{egg}

\begin{lemma}
Let $r$ be a rational non-constant function.  $|r(z)| <1$ in
$\{ z\in\CC : \RE z<0\}$ if and only if $r$ has no pole
in $\{ z\in\CC : \RE z\leq 0\}$ and $|r(z)| \leq 1$ for all $z$
on the imaginary axis.
\label{RKAstabPole}
\end{lemma}

\begin{proof}
Let $D = \{ z\in\CC : \RE z<0\}$.  So,
$\overline{D} =  \{ z\in\CC : \RE z \leq 0\}$.

Suppose that $|r(z)| <1$ in $D$.  Then $|r(z)| \leq 1$ in
$\overline{D}$ by continuity.  Thus, $r$ has no pole in
$\overline{D}$ and $|r(z)| \leq 1$ for all $z$ on the imaginary axis.

Conversely, suppose that $r$ has no pole in $\overline{D}$ and
$|r(z)| \leq 1$ for all $z$ on the imaginary axis.  The function $r$
cannot reach its absolute
maximum in $D$ because it is an analytic and non constant function on
the open set $D$\footnote{We use the Maximum Modulus Theorem from
complex analysis.}.  However, $r$ must reach its absolute maximum at one
point of $\overline{D}$ because it is continuous on
$\overline{D}$ \footnote{Since $r$ has no pole at infinity, We may
consider that $r$ is a continuous function on the compactification 
of $\overline{D}$.}.  Since $r$ is not constant, $r$
reaches its absolute maximum on the imaginary axis only; the
boundary of $\overline{D}$.  Since $r(z) \leq 1$ for all $z$ on the
imaginary axis, then $r(z) < 1$ in $D$.
\end{proof}

\begin{egg}[Example~\ref{AS_IRK} continued]
The poles of
\[
r(z) = \frac{1+ z/3}{1-2z/3+z^2/6}
\]
are the roots of $1-2z/3+z^2/6$; namely,
$z_{\pm} = 2\pm i\sqrt{2}$.  The poles of $r(z)$ are not in
the half-plane $\{ z\in\CC : \RE z\leq 0\}$.

Moreover, on the imaginary axis, $z=ti$ with $t\in \RR$. Thus,
\[
r(ti) = \frac{1+ti/3}{1-2it/3-t^2/6}
\]
and
\[
|r(ti)|\leq 1 \Leftrightarrow
\bigg| 1+\frac{ti}{3} \bigg|^2 \leq
\bigg| 1-\frac{2it}{3}-\frac{t^2}{6} \bigg|^2
\Leftrightarrow
0 \leq \frac{t^4}{36} \ .
\]
Since the last inequality is true for all $t \in \RR$, $|r(z)|\leq 1$
on the imaginary axis.  Thus, from the previous lemma, $|r(z)| <1$ for
all $z$ in $\{z \in\CC : \RE z<0\}$.  Namely, the method is
A-stable as we have already shown in example~\ref{AS_IRK}.
\end{egg}

\begin{lemma}
Suppose that $r$ is the rational function associated to a Runge-Kutta
method as in Proposition~\ref{rat_funct} and that the Runge-Kutta method is
of order $p$, then $r(z) = e^z +O(z^{p+1})$ as $z \rightarrow 0$.
\label{exp_approx}
\end{lemma}

\begin{proof}
By definition of order,
$y(t_{i+1}) = y(t_i) + h\,\phi(t_i,y(t_i)) + O(h^{p+1})$ where
$\phi(t_i,w_i)$ represents the right hand side summation in the
Runge-Kutta method.  Thus, for $i=0$, we get
\[
y(t_1) = y(t_0) + h\,\phi(t_0,y(t_0)) + O(h^{p+1})
= w_0 + h\,\phi(t_0,w_0) + O(h^{p+1}) = w_1+O(h^{p+1})
\]
because $w_0 = y_0 = y(t_0)$.
But $w_{i+1} = r(h\mu)w_i$ for $i\geq 0$ and the solution
of (\ref{RKThe_equ}) is $y(t) = e^{t\mu}w_0$.  Thus,
\[
  e^{h\mu}w_0 = r(h\mu)w_0 + O(h^{p+1}) \ .
\]
We get the conclusion of the lemma after a division by $w_0$ on both
sides of the previous equality.
\end{proof}

\begin{theorem}
A Runge-Kutta method given by a collocation method satisfying
Corollary~\ref{best_IRK} with $k>0$ is A-stable.
\label{RKcolAstable}
\end{theorem}

\begin{proof}
From Corollary~\ref{best_IRK}, the Runge-Kutta method is of order
$2k$.  For the initial value problem (\ref{RKThe_equ}), we have from
Proposition~\ref{rat_funct} that the approximation $w_j$ of $y(t_j)$ given
by the Runge-Kutta method is $w_i = (r(h\lambda))^iw_0$ for $i\geq 0$,
where $r(z)$ is the quotient of two polynomials of degree at most $k$.
From Lemma~\ref{exp_approx}, $r(z)$ is a ``Pad\'e approximation'' of
$e^z$ of order $2k$.  From Wanner-Hairer-Norsett
theorem\footnote{Roughly, this theorem states that a $r(z) = p(z)/q(z)$,
a ``Pad\'e approximante to the exponential function'', is
A-acceptable if and only if the $p$ and $q$ have a specific form and
$\text{deg}\,p \leq \text{deg}\,q \leq 2 + \text{deg}\,p$.}, $r$ is
associated to a A-stable method.
\end{proof}

\subsubsection{Multistep Methods}

The finite difference formula (\ref{MULTISTEP})
applied to (\ref{RKThe_equ}) becomes
\[
w_{i+1} = \sum_{k=0}^m a_k w_{i-k} + h \mu \sum_{k=-1}^m b_k w_{i-k}\   .
\]
If we substitute $\lambda^i$ for $w_i$, we get
\[
  \lambda^{i+1} = \sum_{k=0}^m a_k \lambda^{i-k} + h \mu
  \sum_{k=-1}^m b_k \lambda^{i-k} \ .
\]
If we subtract $\lambda^{i+1}$ from both sides of this equality
and multiply them by $\lambda^{m-i}$, we get
\[
p(\lambda) +h\mu\, q(\lambda) = 0 \ ,
\]
where
\[
p(\lambda) = - \lambda^{m+1} + \sum_{k=0}^m a_k \lambda^{m-k}
\quad \text{and} \quad
q(\lambda) = \sum_{k=-1}^m  b_k \lambda^{m-k} \ .
\]

We have already defined $p(\lambda)$ as the characteristic polynomial
\index{Characteristic Polynomial} of the multistep method given in
Definition~\ref{GFMSM}.  We add another definition.

\begin{defn}
The {\bfseries stability polynomial}\index{Stability Polynomial} of the
multistep method given in Definition~\ref{GFMSM}
is the polynomial $p(\lambda) + h\mu\, q(\lambda)$.
\end{defn}

\begin{rmkList}
\begin{enumerate}
\item We have from Proposition~\ref{cond_consistency} that
$\displaystyle 1 = \sum_{i=0}^m\,a_i$ (i.e.\ $p(1) = 0$) if the
multistep method is consistent.  Thus, a necessary condition for the
consistency of a multistep method is the existence of $1$ has a root
of its characteristic polynomial.
\item For the multistep method given in Definition~\ref{GFMSM}, we have
seen that the finite difference formula (\ref{MULTISTEP}) was derived
from a formula of the form
\[
y_{i+1} = \sum_{k=0}^m a_k y_{i-k} + h \sum_{k=-1}^m b_k
f(t_{i-k},y_{i-k}) + h \tau_{i+1}(h)
\]
for $m \leq i < N$.  Since $y(t) = A e^{\mu\, t}$ is the general solution of
$y' = \mu\,y$,
\[
y_i = A e^{\mu (t_0+ih)} = A e^{\mu t_0} \left( e^{\mu h}\right)^i
\]
is a solution of
\[
y_{i+1} = \sum_{k=0}^m a_k y_{i-k} + h \mu \sum_{k=-1}^m b_k
y_{i-k} + h \tau_{i+1}(h) \ .
\]
Thus
\[
\left( e^{\mu h}\right)^{i+1} = \sum_{k=0}^m a_k \left( e^{\mu h}\right)^{i-k}
+ h \mu \sum_{k=-1}^m b_k \left( e^{\mu h}\right)^{i-k}
+ h \left(A e^{\mu t_0}\right)^{-1} \tau_{i+1}(h) \ .
\]
If the multistep method is consistent for (\ref{RKThe_equ}), one of
the roots of the stability polynomial must approximate $e^{\mu h}$ for
$h$ small.  This root is called the
{\bfseries principal root}\index{Multistep Methods!Principal Root} of
the stability polynomial. 
\item Because the roots of the stability polynomial are continuous
functions of $h$, the roots of the characteristic polynomial can be
use to approximate the roots of the stability polynomial for $h$
small.
\end{enumerate}
\end{rmkList}

We can now restate the definition of absolute stability for the
multistep methods defined in Definition~\ref{GFMSM}.

\begin{defn}
The {\bfseries region of absolute stability}\index{Multistep Method!Region of
Absolute Stability} of a multistep method as defined in
definition~\ref{GFMSM} is the set of all values $h\mu \in \CC$
such that $\displaystyle \lim_{i\rightarrow +\infty} w_i = 0$ for all
solutions $\{w_i\}_{i=0}^\infty$ of the difference
equation (\ref{MULTISTEP}) applied to the initial value problem
(\ref{RKThe_equ}).

We say that a multistep method is
{\bfseries absolutely stable}\index{Multistep Method!Absolutely Stable} for
the value $h\mu$ if $h\mu$ is in the region of absolute stability.

A multistep method is {\bfseries A-stable}\index{Multistep Method!A-Stable}
if the region of absolute stability contains the half-plane to the left of
the imaginary axis (complex numbers with a negative real part.)
\end{defn}

\begin{rmk}
As for the Runge-Kutta methods, we have to remember that $\RE \mu < 0$
in the previous definition.  Hence, all solutions
$y(t) = e^{\mu(t-t_0)}y_0$ of the differential equation (\ref{RKThe_equ}) satisfy
$\displaystyle \lim_{t\to \infty} y(t) = 0$.
\end{rmk}

The following example illustrates the crucial role played by
absolute stability.

\begin{egg}
Consider the initial value problem
\begin{equation} \label{stable_ex1}
\begin{split}
y'(t) &= 1 -2y(t) \quad , \quad 0\leq t \leq 4 \\
y(0) &= 1
\end{split}
\end{equation}

The exact general solution of $y'= 1-2y$ is $y(t) = c e^{-2t} +1/2$.
The initial condition $y(0)=1$ gives $c = 1/2$.

\begin{table}
\begin{center}
{\small
\begin{tabular}{crrrrr}
$i$ & $t_i$ & $w_i$ & $y_i$ & $w_i-y_i$ & $|y_i-w_i|/|y_i|$ \\
\hline
$0$ & $0$ & $1$ & $1$ & $0$ & $0$ \\
$8$ & $0.25$ & $0.79835974$ & $0.80326533$ & $-0.00490559$ & $0.00610706$\\
$16$ & $0.5$ & $0.67803707$ & $0.68393972$ & $-0.00590266$ & $0.00863038$ \\
$24$ & $0.75$ & $0.60623818$ & $0.61156508$ & $-0.00532690$ & $0.00871027$ \\
$32$ & $1$ & $0.56339439$ & $0.56766764$ & $-0.00427325$ & $0.00752773$ \\
$40$ & $1.25$ & $0.53782867$ & $0.54104250$ & $-0.00321383$ & $0.00594007$ \\
$48$ & $1.5$ & $0.52257310$ & $0.52489353$ & $-0.00232043$ & $0.00442076$ \\
$56$ & $1.75$ & $0.51346981$ & $0.51509869$ & $-0.00162888$ & $0.00316227$ \\
$64$ & $2$ & $0.50803770$ & $0.50915782$ & $-0.00112012$ & $0.00219995$ \\
$72$ & $2.25$ & $0.50479625$ & $0.50555450$ & $-0.00075825$ & $0.00149983$ \\
$80$ & $2.5$ & $0.50286202$ & $0.50336897$ & $-0.00050696$ & $0.00100713$ \\
$88$ & $2.75$ & $0.50170782$ & $0.50204339$ & $-0.00033556$ & $0.00066840$ \\
$96$ & $3$ & $0.50101909$ & $0.50123938$ & $-0.00022029$ & $0.00043948$ \\
$104$ & $3.25$ & $0.50060811$ & $0.50075172$ & $-0.00014361$ & $0.00028679$ \\
$112$ & $3.5$ & $0.50036287$ & $0.50045594$ & $-0.9307\times 10^{-4}$ &
$0.00018596$ \\
$120$ & $3.75$ & $0.50021653$ & $0.50027654$ & $-0.6001 \times 10^{-4}$ &
$0.00011995$ \\
$128$ & $4$ & $0.50012921$ & $0.50016773$ & $-0.3852 \times 10^{-4}$ &
$0.7702 \times 10^{-4}$ \\
\hline
\end{tabular}
}
\end{center}
\caption[Example for the Euler's method]{Some results from
the Euler's method used in Example~\ref{Roundoff_effect} to approximate
the solution of (\ref{stable_ex1}).\label{STAB_EM}}
\end{table}

If we use the Euler's Method with $N=128$, then $h=(t_f-t_0)/N = 1/32$,
$t_i = t_0 + ih = i/32$ for $i=0$, $1$, \ldots, $128$ and the
approximations $w_i$ of $y_i$ are given by the difference equation
\begin{align*}
w_0 &= 1 \\
w_{i+1} &= w_i + h(1-2w_i) \\
\end{align*}
for $i=0$, $1$, \ldots, $127$.  The values of some of the $w_i$
are given in Table~\ref{STAB_EM} and the graph of the
approximation of $y$ given by the $w_i$ can be found in
Figure~\ref{Euler_AB}.

If we use the Adams-Bashforth method of order two from
Example~\ref{comp_order} with $N=128$, then $h=(t_f-t_0)/N = 1/32$,
$t_i = t_0 + ih = i/32$ for $i=0$, $1$, \ldots, $128$ and the
approximations $w_i$ of $y_i$ are given by the difference equation
\begin{align*}
w_0 &= 1 \\
w_1 &= y(1/32) = (e^{-1/16}+1)/2 = 9.69706531\ldots \times 10^{-1} \\
w_{i+1} &= w_{i-1} + 2hf(t_i,w_i) = w_{i-1} + 2h(1-2w_i)
\end{align*}
for $i=1$, $2$, \ldots, $127$.  The values of some of the $w_i$
are given in Table~\ref{STAB_ADBF_twoS} and the graph of the
approximation of $y$ given by the $w_i$ can be found in
Figure~\ref{Euler_AB}.

\begin{table}[t]
\begin{center}
{\small
\begin{tabular}{crrrrr}
$i$ & $t_i$ & $w_i$ & $y_i$ & $w_i-y_i$ & $|y_i-w_i|/|y_i|$ \\
\hline
$0$ & $0$ & $1$ & $1$ & $0$ \\
$1$ & $0.03125$ & $0.96970653$ & $0.96970653$ & $0$ & $0$ \\
$8$ & $0.25$ & $0.80337381$ & $0.80326533$ & $0.00010848$ & $0.00013505$ \\
$16$ & $0.5$ & $0.68408166$ & $0.68393972$ & $0.00014194$ & $0.00020754$ \\
$24$ & $0.75$ & $0.61171439$ & $0.61156508$ & $0.00014931$ & $0.00024415$ \\
$32$ & $1$ & $0.56782462$ & $0.56766764$ & $0.00015698$ & $0.00027654$ \\
$40$ & $1.25$ & $0.54122426$ & $0.54104250$ & $0.00018176$ & $0.00033594$ \\
$48$ & $1.5$ & $0.52513250$ & $0.52489353$ & $0.00023897$ & $0.00045527$ \\
$56$ & $1.75$ & $0.51544736$ & $0.51509869$ & $0.00034867$ & $0.00067691$ \\
$64$ & $2$ & $0.50969996$ & $0.50915781$ & $0.00054215$ & $0.00106479$ \\
$72$ & $2.25$ & $0.50642522$ & $0.50555450$ & $0.00087072$ & $0.00172230$ \\
$80$ & $2.5$ & $0.50478834$ & $0.50336897$ & $0.00141937$ & $0.00281974$ \\
$81$ & $2.53125$ & $0.50167598$ & $0.50316486$ & $-0.00148887$ & $0.00295902$ \\
$82$ & $2.5625$ & $0.50457884$ & $0.50297311$ & $0.00160573$ & $0.00319249$ \\
$83$ & $2.59375$ & $0.50110363$ & $0.50279298$ & $-0.00168935$ & $0.00335993$ \\
$88$ & $2.75$ & $0.50437208$ & $0.50204339$ & $0.00232869$ & $0.00463843$ \\
$89$ & $2.78125$ & $0.49945547$ & $0.50191958$ & $-0.00246412$ & $0.00490939$ \\
$90$ & $2.8125$ & $0.50444014$ & $0.50180328$ & $0.00263686$ & $0.00525477$ \\
$91$ & $2.84375$ & $0.49890045$ & $0.50169403$ & $-0.00279358$ & $0.00556829$ \\
$96$ & $3$ & $0.50507032$ & $0.50123938$ & $0.00383094$ & $0.00764293$ \\
$104$ & $3.25$ & $0.50706105$ & $0.500751720$ & $0.00630933$ & $0.01259971$ \\
$112$ & $3.5$ & $0.51085173$ & $0.50045594$ & $0.01039579$ & $0.02077264$ \\
$113$ & $3.53125$ & $0.48936667$ & $0.50042832$ & $-0.01106164$ &
$0.02210435$ \\
$114$ & $3.5625$ & $0.51218090$ & $0.50040237$ & $0.01177853$ & $0.02353812$ \\
$115$ & $3.59375$ & $0.48784406$ & $0.50037799$ & $-0.01253393$ &
$0.02504892$ \\
$120$ & $3.75$ & $0.51740867$ & $0.50027654$ & $0.01713212$ & $0.03424531$ \\
$121$ & $3.78125$ & $0.48202618$ & $0.50025979$ & $-0.01823360$ &
$0.03644827$ \\
$128$ & $4$ & $0.52840330$ & $0.50016773$ & $0.02823557$ & $0.05645221$ \\
\hline
\end{tabular}
}
\end{center}
\caption[Example for the Adams-Bashforth method of order two]{Some
results from the Adams-Bashforth method of order two used in
Example~\ref{Roundoff_effect} to approximate the solution of
(\ref{stable_ex1}). \label{STAB_ADBF_twoS}}
\end{table}

For the first steps, the magnitude of the absolute error for the Euler's
method is bigger than the magnitude of the absolute error for the
Adams-Bashforth method of order two.  However, the magnitude of the
absolute error for the Euler's method decreases as the index $i$
increases while the magnitude of the absolute error for the
Adams-Bashforth method of order two increases as the index $i$ increases.
As $i$ approaches $128$, the values of $w_i$ start to oscillate
between values above and values below the exact value of $y$ at $t_i$.
For $i$ near $128$ the approximation $w_i$ given by the
Adams-Bashforth method of order two has only one significant digit.

\mathF{init_value_probl/stability_ex1}{12cm}{Solution of $y'(t)=1-2y(t)$ where
$0 \leq t \leq 4$ and $y(0)=1$}{Approximation of the solution of
$y'(t)=1-2y(t)$ where $0 \leq t \leq 4$ and $y(0)=1$ given by three
different numerical methods.}{Euler_AB}

To find out why the Adams-Bashforth method of order two gives a poor
approximation of the solution of (\ref{stable_ex1}), we
rewrite the equation $w_{i+1} = w_{i-1} + 2h(1-2w_i)$ as
\begin{equation}\label{finitdiffSOAB}
  w_{i+1} + 4hw_i -w_{i-1} = 2h \ .
\end{equation}
The general solution of (\ref{finitdiffSOAB}) is
\begin{equation} \label{sol_of_DE}
w_i = c_1\lambda_1^i + c_2\lambda_2^i + \frac{1}{2} \;  ,
\end{equation}
where
\[
\lambda_1 =-2h+\sqrt{1+4h^2} = 1-2h+O(h^2)
\quad \text{and} \quad 
\lambda_2 =-2h-\sqrt{1+4h^2} = -1-2h+O(h^2)
\]
are the roots of $\lambda^2 + 4h\lambda -1 =0$, and $c_1$ and $c_2$
are arbitrary constants.  Note that $w_i= 1/2$ for all $i$ is a
solution of the non-homogeneous equation (\ref{finitdiffSOAB}) and
$w_i =c_1\lambda^i + c_2\lambda_2^i$ is the general solution of
the homogeneous equation
\[
  w_{i+1} + 4hw_i -w_{i-1} = 0 \ .
\]
The initial conditions $w_0=y(0)=1$ and $w_1=y(1/32) = 0.96970\ldots$
gives $c_1 = 0.50793\ldots \cong 1/2$ and $c_2 = 0.0079\ldots \cong 0$
because of round off errors.  The exact values are $c_1=1/2$
and $c_2=0$.  Even if the initial conditions were such that $c_2 =0$,
the effect of rounding error is equivalent to having $c_2 \neq 0$.
Because $|\lambda_2| >1$, the magnitude of the second term of
(\ref{sol_of_DE}) increases as $i$ increases.  This explains the
increase of the errors as $i$ increases.

If we substitute $\lambda_1$ and $\lambda_2$ in (\ref{sol_of_DE}),
we get
\begin{align}
w_i &= c_1 \left(-2h+\sqrt{1+4h^2} \right)^i
 + c_2 \left(-2h-\sqrt{1+4h^2} \right)^i + \frac{1}{2} \nonumber \\
&= c_1 \left(1-2h+O(h^2) \right)^i
 + c_2 (-1)^i \left(1+2h+O(h^2) \right)^i + \frac{1}{2} \nonumber \\
&\approx c_1 \left(1-2h \right)^i
 + c_2 (-1)^i \left(1+2h\right)^i + \frac{1}{2}
\approx c_1 e^{-2t_i} + c_2 (-1)^i e^{2t_i} + \frac{1}{2} \label{diff_diff}
\end{align}
for $h$ very small.  For the last approximation above, we note
that
\[
  (1-2h)^i = \left( (1-2h)^{-1/(2h)}\right)^{-2 i h} \ ,
\]
where $\displaystyle \lim_{h\to 0} (1-2h)^{-1/(2h)} = e$.  Thus, for
$h$ very small, we may assume that
$\displaystyle (1-2h)^i \approx e^{-2 i h} = e^{-2t_i}$.
Similarly, $(1+2h)^i \approx e^{2 t_i}$ for $h$ very small.

From (\ref{diff_diff}) we may conclude that the first term in
(\ref{sol_of_DE}) is associated to the solution of $y'=1-2y$ but the
second term in (\ref{sol_of_DE}) exists only because we have
transformed the first order differential equation $y'=1-2y$ into a
second order difference equation $w_{i+1} = w_{i-1} +2h(1-2w_i)$.

This example shows that it is important to study the magnitude of the
roots of the stability polynomial associated to a multistep method.
\label{Roundoff_effect}
\end{egg}

\begin{rmk}
The Adams-Bashforth method of order two from Example~\ref{comp_order} is
consistent and, as we saw in Example~\ref{comp_stab}, satisfies the
root condition.   Thus, this Adams-Bashforth method of order two is
convergent according to Theorem~\ref{Dahl_conv}.  However, we saw in the
numerical experiment of Example~\ref{Roundoff_effect} that this
Adams-Bashforth method of order two does not converge.  Is there
anything wrong with Theorem~\ref{Dahl_conv}?  No, there is nothing wrong
mathematically but the theory assumes that $\delta_i(h) = O(h^2)$
which is rarely satisfied by round off errors (round off errors
do not generally go to $0$ as $h$ decreases).  Moreover, the theory
does not take into account the information provided by the stability
polynomial that may have many roots.  This illustrate the limits
of the theory presented so far where we ignore the information
provided by the stability polynomial and the full effect of round off
errors.   We therefore need a stability criteria which is stronger
than the root condition.  Absolute stability is this criteria.
\end{rmk}

\begin{prop}
The region of absolute stability is the set of all value
$h\mu \in \CC$ with $\RE \mu < 0$ such that
all the roots of the stability polynomial have absolute
values less than one.
\end{prop}

\begin{proof}
The multistep method (\ref{MULTISTEP}) applied to this initial value
problem (\ref{RKThe_equ}) is the finite difference equation
\[
\sum_{k=-1}^m (a_k + h\mu\,b_k) w_{i-k} = 0 \quad , \quad i \geq m \ ,
\]
where $a_{-1} = -1$.  A solution
$\{w_i\}_{i=0}^\infty$ of this finite difference equation is a linear
combination of solutions of the form  $\{i^n\lambda^i\}_{i=0}^\infty$,
where $\lambda \in \CC$ is a root of multiplicity $s$ of the stability
polynomial
\begin{equation}\label{LinStable}
p(\lambda) + h\mu q(\lambda)
= \sum_{k=-1}^m (a_k + h\mu\, b_k) \lambda^{m-k}
\end{equation}
and $0 \leq n <s$.  Hence, $|\lambda| <1$ for all roots of (\ref{LinStable})
if and only if any non-trivial solution $\{w_i\}_{i=0}^\infty$ of
the finite difference equation above 
satisfies $\displaystyle \lim_{i\rightarrow +\infty} w_i = 0$ .  Namely,
if and only if $h\mu$ is in the region of absolute stability.
\end{proof}

\begin{egg}
We illustrate with the Euler's method why we should choose $h\mu$ in
the region of absolute stability.

Consider the initial value problem (\ref{RKThe_equ}).  The exact
solution is $y(t) = y_0\,e^{\mu t}$.  The approximation $w_i$ of
$y_i$ given by the Euler's method (Definition~\ref{EulerMethod}) is
the solution of
\begin{align*}
w_0 &= y_0 \\
w_{i+1} &= w_i + h\mu\, w_i = (1+h\mu)w_i
\end{align*}
Thus, $w_i = (1+h\mu)^iy_0$ for $i\geq 0$.

Since $\RE \mu<0$, we have that $y(t) \rightarrow 0$ as $t \rightarrow 0$.
To get the same behaviour for $w_i$
(i.e.\ $w_i \rightarrow 0$ as $i \rightarrow \infty$), we
need  $|1+h\mu| < 1$.

Suppose that an error $\delta_0$ is introduced in the initial
condition; namely, $w_0 = y_0 + \delta_0$.  The new value of $w_i$ is
$(1+h\mu)^i y_0 + (1+h\mu)^i \delta_0$ which differ from the
unperturbed value of $w_i$ by $(1+h\mu)^i \delta_0$.  To have this
difference decreases as $i \rightarrow \infty$, we need
$|1+h\mu| < 1$.

Let us show that $|1+h\mu| < 1$ is the condition for $h\mu$ to be in the
region of absolute stability.  The stability polynomial of the Euler's
method is $-\lambda + (1+h\mu)$.
Obviously, the only root is $\lambda = 1 + h\mu$.  The region of
absolute stability is
$\left\{ h\mu : |1 + h\mu| < 1 \right\}$.  This is the open disk of
radius $1$ centred at $(-1,0)$ in the complex plane.
\end{egg}

\begin{rmk}
Suppose that $p(\lambda) + h\mu\, q(\lambda)$ is the stability
polynomial of a multistep method.  If we draw the graph of
$z = - p(\lambda)/q(\lambda)$ for $\lambda$ on the unit circle in the
complex plane, we get the boundary of the region of absolute stability
of the multistep method; namely, the values of $h\mu$ for which
$|\lambda|=1$.
\end{rmk}

\begin{egg}
The stability polynomial for the Adams-Bashforth method of order two
from Example~\ref{comp_order} is
\[
  p(\lambda) + h\mu q(\lambda) = -\lambda^2 + 1 + 2h\mu \lambda \ .
\]
If $\lambda$ is a root of this polynomial such that $|\lambda| = 1$,
we may assume that $\lambda = e^{\theta i}$ for some $\theta \in [0,2\pi[$.
We get
\[
-e^{2\theta i} + 1 + 2h\mu e^{\theta i} = 0
\Rightarrow h \mu = \frac{e^{2\theta i} -1}{2e^{\theta i}}
= \frac{e^{\theta i} -e^{\theta i}}{2} = i\sin(\theta) \ .
\]
Thus, the boundary of the region of absolute stability is the segment
$\{ r i : -1 \leq r \leq 1\}$ on the imaginary axis.  All points
outside this segment are on curves given by $\lambda = r e^{\theta i}$
for both $r>1$ and $r<1$.  The region of absolute stability has no
interior (Figure~\ref{stab_region6}).  Thus, the region 
of absolute stability is empty.  This explains why this method
fails to converge in Example~\ref{Roundoff_effect}.
\end{egg}

\mathF{init_value_probl/stability_ex6}{8cm}{Absolute stability regions for
the Adams-Bashforth method of order two}{Regions of absolute stability
for the Adams-Bashforth method of order two of Example~\ref{comp_order}.
We have drawn the curve $z = - p(\lambda)/q(\lambda)$ for
$\lambda = e^{i\theta}$ (black curve), for $\lambda = 1.1 e^{i\theta}$
and $\lambda = 1.5 e^{i\theta}$ (red curves) and
$\lambda = 0.8 e^{i\theta}$ (blue curve).  The region of absolute
stability is empty.}{stab_region6}

\begin{egg}
The boundaries for the region of absolute stability for the
Adams-Bashforth method of order four is drawn in
Figure~\ref{stab_region1}.
To produce this boundary, we drew the graph of
$z = - p(\lambda)/q(\lambda)$ for $\lambda = e^{i\theta}$ with
$0\leq \theta < 2\pi$.   For the Adams-Bashforth method
of order four $p(\lambda) = -\lambda^4 + \lambda^3$ and
$q(\lambda) = (55\lambda^3-59\lambda^2+37\lambda - 9)/24$.

The boundaries for the region of absolute stability for the
Adams-Moulton method of order four is drawn in
Figure~\ref{stab_region2}.  To produce this boundary, 
we drew the graph of $z = - p(\lambda)/q(\lambda)$ for
$\lambda = e^{i\theta}$ with $0\leq \theta < 2\pi$.
For the Adams-Moulton method of order four
$p(\lambda) = -\lambda^3 + \lambda^2$ and
$q(\lambda) = (9\lambda^3+19\lambda^2-5\lambda + 1)/24$.

The regions inside the boundary curves (the black curve) and to the
left of the imaginary axis are the regions of absolute stability.
The two lobes for the Adams-Bashforth method of order four do not
represent regions of absolute stability.  To justify this, we have
also drawn the curves $z = - p(\lambda)/q(\lambda)$ for
$\lambda = 1.2 e^{i\theta}$ and $\lambda = 0.8 e^{i\theta}$ with
$0\leq \theta < 2\pi$ for the two methods.  The points $z$ on the
curves in red associated to $\lambda = 1.2 e^{i\theta}$ correspond
to values of $h\mu$ for which the stability polynomial has a root
$\lambda$ of absolute value $1.2$, these points are therefore outside
the region of absolute stability, whereas the points $z$ on the curves
in blue associated to  $\lambda = 0.8 e^{i\theta}$ correspond to
values of $h\mu$ for which the stability polynomial has a root
$\lambda$ of absolute value $0.8$, these points are inside the region
of absolute stability.
\end{egg}

\mathF{init_value_probl/stability_ex2}{8cm}{Absolute stability regions for
the Adams-Bashforth method of order four}{Regions of absolute stability
for the Adams-Bashforth method of order four.  We have drawn the curve
$z = - p(\lambda)/q(\lambda)$ for $\lambda = e^{i\theta}$ (black
curve), for $\lambda = 1.2 e^{i\theta}$ (red curve) and
$\lambda = 0.8 e^{i\theta}$ (blue curve).  The region to the left of
the imaginary axis and inside the black curve is the region of absolute
stability.}{stab_region1}

\mathF{init_value_probl/stability_ex3}{8cm}{Absolute stability regions for
the Adams-Moulton method of order four}{Regions of absolute stability
for the Adams-Moulton method of order four.  We have drawn the curve
$z = - p(\lambda)/q(\lambda)$ for $\lambda = e^{i\theta}$ (black
curve), for $\lambda = 1.2 e^{i\theta}$ (red curve) and
$\lambda = 0.8 e^{i\theta}$ (blue curve).  The region to the left of
the imaginary axis and inside the black curve is the region of absolute
stability.}{stab_region2}

\begin{egg}
The region of absolute stability of the Trapezoidal method is the
half-plane to the left of the imaginary axis.  Hence, the trapezoidal
method is A-stable.

The stability polynomial of the trapezoidal method is
$p(\lambda) + h\mu\, q(\lambda)$ where
$p(\lambda) = -\lambda^2 + \lambda$ and
$q(\lambda) = (\lambda^2 + \lambda)/2$.
We draw the graph of $z = - p(\lambda)/q(\lambda)$ for $\lambda$ on
the unit circle in Figure~\ref{trap_stable}.

We now prove rigorously that the region of absolute stability of the
Trapezoidal Method is the half-plane to the left of the imaginary
axis.

The Trapezoidal Method applied to the initial value problem
(\ref{RKThe_equ}) gives
\[
w_{i+1} = w_i + \frac{h}{2}\left( \mu\, w_{i+1} + \mu\, w_i \right)
\quad , \quad 0 \leq i < N \ .
\]
If we solve for $w_{i+1}$, we get
\[
w_{i+1} = \left(\frac{1+h\mu/2}{1 - h\mu/2}\right) w_i \ .
\]
Hence, by induction,
\[
w_{i+1} = \left( \frac{1+h\mu/2}{1-h\mu/2} \right)^{i+1} w_0 \quad ,
\quad i \geq 0 \ .
\]
The region of absolute stability is
\[
\left\{ h\mu :
\bigg| \frac{1+h\mu/2}{1-h\mu/2} \bigg| <1 \right\} =
\left\{ z : \RE z < 0 \right\}
\]
because
\[
\bigg| \frac{1+h\mu/2}{1-h\mu/2} \bigg| <1 \Leftrightarrow
\bigg| 1+\frac{h\mu}{2} \bigg|^2 < \bigg| 1-\frac{h\mu}{2} \bigg|^2
\Leftrightarrow \RE h\mu < 0 \ .
\]
\end{egg}

\mathF{init_value_probl/stability_ex4}{8cm}{Absolute stability region for the
trapezoidal method}{Regions of absolute stability for the trapezoidal
method.  We have drawn the curve $z = - p(\lambda)/q(\lambda)$ for
$\lambda = e^{i\theta}$ (black curve), for $\lambda = 1.2 e^{i\theta}$
(red curve) and $\lambda = 0.8 e^{i\theta}$ (blue curve).  The region
to the left of the imaginary axis is the region of absolute
stability.}{trap_stable}

\begin{rmk}
The fact that the trapezoidal method is A-stable does not mean
that all values of $h$ can be used.  Consider the differential
equation 
\[
\begin{split}
y'(t) &= \mu(t) y(t) \quad , \quad t\geq 0 \\
y(0) &= y_0
\end{split}
\]
where $\mu$ is a differentiable function such that
$\mu(t) < 0$ and $\mu'(t) > 0$ for all $t>0$.  All solutions $y$ of 
this differential equation satisfy
$\displaystyle \lim_{t\to \infty} y(t) =0$.

The trapezoidal method gives
\[
w_{i+1} = \frac{1+h\mu(t_i)/2}{1-h\mu(t_{i+1})/2}\, w_i
\]
for $i \geq 0$.  Since $\mu(t) < 0$ for all $t>0$, we still need
\[
\left| \frac{1+h\mu(t_i)/2}{1-h\mu(t_{i+1})/2} \right| < 1
\]
to ensure that $w_i \to 0$ as $i \to \infty$.  However, if
$\mu(t) \to 0$ as $t \to \infty$, we will have that
$\displaystyle   \left| \frac{1+h\mu(t_i)/2}{1-h\mu(t_{i+1})/2}
\right| \to 1$ as $i \to \infty$.  Thus, the convergence of $w_i$ to
$0$ will be really slow and taking $h$ smaller will further slow the
convergence.
\end{rmk}

\begin{egg}
Consider the initial value problem
\begin{equation}\label{trapezoid_ex}
\begin{split}
y'(t) &= 100 y(t) + 100 t^2  -2t -100 \quad , \quad 0\leq t \leq 1 \\
y(0) &= 1
\end{split}
\end{equation}

If we use the modified Euler's method, the Runge-Kutta method of
order four and the Adams-Bashfort method of order four to approximate
$y(1)$, we get the following results:
\[
\begin{array}{c|c|c|c}
 & \multicolumn{3}{|c}{\text{Approximation of $y(1)$}} \\
\text{number $N$} & \text{Modified} & \text{Runge-Kutta} &
\text{Adams-Bashforth} \\
\text{of steps} & \text{Euler's method} & \text{Method of order $4$} &
\text{Method of order $4$}\\
\hline
10 & 5.9445\ldots \times 10^{14} & 3.9941\ldots \times 10^{24} &
2.9627\ldots \times 10^{14} \\
20 & 7.8754\ldots \times 10^{21} & 2.0564\ldots \times 10^{32} &
2.9976\ldots \times 10^{19} \\
30 & 1.4899\ldots \times 10^{26} & 2.6381\ldots \times 10^{35} &
3.2001\ldots \times 10^{23} \\
40 & 9.7746\ldots \times 10^{28} & 5.5303\ldots \times 10^{36} &
4.2923\ldots \times 10^{26}
\end{array}
\]

The exact solution of (\ref{trapezoid_ex}) is
$y(t) = 1-t^2+C e^{100t}$.  The initial condition $y(0)=1$ implies
that $C=0$.  However, because of round off error, the solution that we
compute is one with $C\not=0$ small.

We now use the trapezoidal method to approximate $y(1)$.  First, we
have to explain how to implement the trapezoidal method.

Given $w_i$, we have to solve
$\displaystyle w_{i+1} =
w_i + \frac{h}{2} \left( f(t_{i+1}.w_{i+1}) + f(t_i,w_i) \right)$
for $w_{i+1}$.  This is an implicit equation for $w_{i+1}$.  In
general, this equation cannot be solved explicitly for $w_{i+1}$.
To compute $w_{i+1}$, we use Euler's method to get a first approximation
of $w_{i+1}$ and then use Newton's Method to approximate a root
of
$\displaystyle
 0 = z - w_i - \frac{h}{2} \left( f(t_{i+1}.z) + f(t_i,w_i) \right)$.
The root of this equation is the value of $w_{i+1}$.

To get a first approximation of $w_{i+1}$, we apply the Euler's method
with $N=1$ to
\[
\begin{split}
y'(t) &= f(t,y(t)) \quad , \quad t_i \leq t \leq t_{i+1} \\
y(t_i) &= w_i
\end{split}
\]
to get the first approximation of $w_{i+1}$.  The Euler's method gives
an approximation of $y(t_{i+1})$ if $w_i = y_i$.

The following code is an implementation of the trapezoidal method.
Using this code with $t_0=0$, $t_f=1$, $y_0=1$, the
number of subinterval $N=10$, the tolerance $T=10^{-5}$ and the
maximum number of iterations for the Newton's Method $M=10$, we get
\[
\begin{array}{c|ccccccccccc}
i & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline
t_i & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9
& 1.0\\
\hline
w_i & 1.00 & 0.99 & 0.96 & 0.91 & 0.84 & 0.75 & 0.64 & 0.51 & 0.36 & 0.19
& 0.0
\end{array}
\]
rounded to two decimal places.  The approximations $w_i$ of $y_i$ are exact.
\end{egg}

\begin{code}[Trapezoidal Method]
To approximate the solution of the initial value problem
\[
\begin{split}
y'(t) &= f(t,y(t)) \quad , \quad t_0 \leq t \leq t_f \\
y(0) &= y_0
\end{split}
\]
\subI{Input} The initial time $t_0$ (t0 in the code below).\\
The final time $t_f$ (tf in the code below).\\
The number $N$ of subintervals of $[t_0,t_f]$.\\
The initial condition $y_0$ (y0 in the code below).\\
The tolerance $T$ for the Newton's Method.\\
The maximum number of iterations $M$ for the Newton's Method.\\
The function $f(t,y)$ (funct in the code below).\\
The derivative of $f(t,y)$ with respect to $y$ (functprime in the code
below).\\
\subI{Output} The approximations $w_i$ (w(i+1) in the code below)
of $y(t_i)$ for $i=0$, $1$, \ldots, $N$, where $t_i=t_0+ih$
(t(i+1) in the code below) with $h=(t_f-t_0)/N$.
\small
\begin{verbatim}
function [t,w] = trapez(funct,functprime,t0,y0,tf,N,M,T)
  h = (tf-t0)/N;
  half = h/2;
  t(1) = t0;
  w(1) = y0;

  for i = 1:1:n
    % We start the iteration with the approximation of y(t_0 + h)
    % given by the Euler method.
    k = feval(funct,t(i),w(i));
    w0 = w(i) + h*k;
    t(i+1) = t(1) + i*h;

    % Newton-Raphson iterations
    for j = 1:M
      numer = w0 - w(i)- half*(funct(t(i+1),w0) + k);
      denum = 1-half*functprime(t(i+1),w0);

      if (denum == 0)
        % Newton-Raphson iterative method does not converge (fast enough).
        t = NaN;
        w = NaN;
        return;
      end

      w1 = w0 - numer/denum;
      if (abs(w1-w0) < T)
        w(i+1) = w1;
        break;
      else
        w0 = w1;
        if (j == max)
          % The maximum number of iterations has been reached
          % before getting an approximation of w(i+1) within the
          % required tolerance.
          t = NaN;
          w = NaN;
          return;
        end
      end
    end
  end
end
\end{verbatim}
\end{code}

We conclude this section with a couple more results.  This can be used
as a starting point for further reading on the subject of this chapter.

\begin{prop}
The multistep method (\ref{MULTISTEP}) is A-stable if and only if
$b_{-1}>0$ and, for each $h\mu$ on the imaginary axis, the roots
of the stability polynomial are less than or equal to $1$ in absolute value.
\label{Cond_A-stab}
\end{prop}

\begin{lemma}[Cohn-Schur Criterion]
Consider the quadratic equation $p(z) = az^2+bz+c$, where $a,b,c \in
\CC$ and $a\not=0$.  Then, the roots of $p$ are inside the closed disk
of radius $1$ centred at the origin if and only if
$|a| \geq|c|$ and
$\big| |a|^2-|c|^2\big| \geq |a\overline{b}-b\overline{c}|$
\label{CSCrit}
\end{lemma}

The proofs of these two results is sketched in \cite{I}.  We show in the next
example how these results can be used.

\begin{egg}
We prove that the backward divided difference
\begin{equation} \label{BDD}
w_{i+1} - \frac{4}{3}\,w_i +\frac{1}{3}\,w_{i-1} =
\frac{2}{3}\,hf(t_{i+1},w_{i+1}) \quad , \quad 1 \leq i < N
\end{equation}
is A-stable.

If we apply this multistep method to the initial value problem
(\ref{RKThe_equ}), we get
\[
w_{i+1} - \frac{4}{3}\,w_i +\frac{1}{3}\,w_{i-1} =
\frac{2}{3}\,h\mu\,w_{i+1}
\]
for $1 \leq i \leq N-1$.  Thus, the stability polynomial is
\[
p(\lambda) + h\mu q(\lambda) = \left(-1+\frac{2}{3}\,h\mu \right) \lambda^2 +
\frac{4}{3}\,\lambda - \frac{1}{3}= 0 \ .
\]
\begin{enumerate}
\item We have that $b_{-1} = 2/3 > 0$.
\item We show that the roots of the characteristic equation with
$h\mu$ on the imaginary axis are less than or equal to $1$.

If we substitute $h\mu = ti$ with $t \in \RR$ in the
stability polynomial, we get
\begin{equation} \label{Char_im_axis}
\left(-1+\frac{2}{3}\,ti \right) \lambda^2 + \frac{4}{3}\,\lambda
- \frac{1}{3}= 0 \ .
\end{equation}
We now apply Lemma~\ref{CSCrit} to this polynomial equation.   We
have $a = -1+2 ti/3$, $b=4/3$ and $c=-1/3$.  Thus, for all $t \in \RR$,
we have that
\begin{enumerate}
\item $a \not= 0$.
\item $|a|^2 - |c|^2 = |1-2 ti/3|^2 - |1/3|^2 =
4(2+t^2)/9 > 0 $.  Thus $|a| > |c|$.
\item $\left( |a|^2 - |c|^2 \right)^2
-|a\overline{b}-b\overline{c}|^2 = 16(2+t^2)^2/81
-64(1+t^2)/81 = 16 t^4/81 \geq 0$.
\end{enumerate}
Hence, the conditions of Lemma~\ref{CSCrit} are satisfied and we can
conclude that, for all $t\in\RR$, the roots of (\ref{Char_im_axis}) are
smaller than or equal to $1$ in absolute value.
\end{enumerate}
1 and 2 imply that the conditions of Proposition~\ref{Cond_A-stab}
are satisfied and so the method (\ref{BDD}) is A-stable.
\end{egg}

The next theorem tells us that the Trapezoidal Method is basically the
best multistep method that we can hope for if A-stability is required.

\begin{theorem}[Dahlquist Second Barrier]
The highest order of an A-stable multistep method is $2$.
\label{Dahl2ndB}
\end{theorem}

\begin{rmk}
We should not completely reject the higher order multistep methods.
There are multistep methods of order higher than $2$, though not
A-stable, that are
{\bfseries A($\alpha$)-stable}\index{Multistep Methods!A($\alpha$)-Stable};
namely, the stability region contain the cone
$\{ z \in \CC : z = \rho\,e^\theta \ \text{with $\rho>0$ and
$|\theta-\pi|<\alpha$} \}$.  For some multistep methods, $\alpha$ may
be closed to $\pi/2$.
\end{rmk}

\subsection{Conclusion}

Theorem~\ref{Dahl_conv} is a beautiful and simple theoretical result
to ensure convergence of a numerical method to solve initial value
problems.  However, it can only be use as a necessary criteria to
ensure convergence because of the strong hypothesis on the size of
round off error.  That was the motivation to introduce a stronger
stability criteria; namely, absolute stability.  However, even
absolute stability is not ideal.  It may impose strict conditions on
the step-size $h$ depending on the region of absolute stability.  We
will see in the next section that for some initial value problems
(particularly in higher dimension), there may be conditions on the
step-size that are almost impossible (if not impossible) to satisfy.

This demonstrates that solving initial value problems is still a
subject of research since we are now intensively using initial value
problems to model physical phenomena.

Though solving numerically initial value problems is not simple, it is
still a lot simpler than solving partial differential equation as we
will see in Chapter~\ref{FiniteDiffMeth}.

\section{Stiff Systems and Stability}

We consider the initial value problem
\begin{equation} \label{SODE}
\begin{split}
\dydx{y}{t}(t) &= f(t,\VEC{y}(t)) \quad , \quad t_0 \leq t \leq t_f \\
\VEC{y}(t_0) & = \VEC{y}_0
\end{split}
\end{equation}
where $f:[t_0,t_f]\times\RR^n \rightarrow \RR^n$.

As we will see, there are additional constraints on the step-size $h$
than those needed to get a converging and stable Runge-Kutta or multistep
method to numerically solve (\ref{SODE}).  We already know that
the requirements on the step size $h$ to get a stable method
may be stronger than what is necessary for the required
accuracy.  If an implicit multistep method is used with an iterative algorithm,
$h$ may have to be small enough for the iterations to converge as we have seen
in Remark~\ref{bm1hL}).  In that remark, we showed that $h$ had to be small
enough to get $|b_{-1}hL| < 1$, where $L$ is the Lipschitz constant
associated to the second variable of the function $f$; namely,
$|f|t,x)-f(t,y)| \leq L |x - y|$ for all $(t,x)$ and $(t,y)$ in the
domain of $f$.  We will add to this list the case where some "components"
of the solution vary much faster than others.  This will be one of the
major characteristics of Stiff differential equations.

The differential equation (\ref{SODE}) is
{\bfseries stiff}\index{Stiff Differential Equations} when
basically no reasonable choice of $h$ can address all the constraints.
A mistake often made is to say that a system is stiff when in fact the
system is unstable.

\begin{egg}
Consider the initial value problem
\begin{equation}\label{stiff_syst}
\begin{split}
\VEC{y}' &= A \VEC{y} \quad , \quad 0 \leq t \leq t_f \\
\VEC{y}(0) &= \VEC{y}_0
\end{split}
\end{equation}
where $\VEC{y}:\RR\rightarrow \RR^2$,
$\VEC{y}_0 =  \begin{pmatrix}
y_{0,1} \\ y_{0,2}
\end{pmatrix} \in \RR^2$, and
$A = \begin{pmatrix}
-\lambda & 1 \\
0 & -0.1
\end{pmatrix}$ for a large positive number $\lambda$.

Since $A = Q B Q^{-1}$, where
$Q=\begin{pmatrix}
1 & 1 \\
0 & \lambda -0.1
\end{pmatrix}$ and
$B=\begin{pmatrix}
-\lambda & 0 \\
0 & -0.1
\end{pmatrix}$, the solution of (\ref{stiff_syst}) is
\begin{equation}\label{sol_stiff_syst}
\VEC{y} = e^{tA}\VEC{y}_0 = Q e^{tB} Q^{-1} \VEC{y}_0
= \begin{pmatrix}
e^{-\lambda t} &
\displaystyle (e^{-0.1 t}-e^{-\lambda t})/(\lambda-0.1) \\
0 & e^{-0.1 t}
\end{pmatrix}
\begin{pmatrix}
y_{0,1} \\ y_{0,2}
\end{pmatrix} \ .
\end{equation}

Choose $N \in \NN$.  Let $h=t_f/N$ and $t_j = jh$ for $0 \leq j \leq N$.
With the Euler's method for systems of ordinary
differential equations, approximations $\VEC{w}_j$ of the exact
values $\VEC{y}(t_j)$ of the solution of (\ref{stiff_syst}) are given
by
\begin{equation}
\label{stiff_Euler}
\begin{split}
\VEC{w}_{j+1} &= (I_2 + hA)\VEC{w}_j \quad , \quad 0 \leq j < N \\
\VEC{w}_0 &= \VEC{y}_0
\end{split}
\end{equation}
The solution
$\displaystyle \{\VEC{w}_j\}_{j=0}^\infty$ of (\ref{stiff_Euler}) is
given by
\begin{equation} \label{sol_stiff_Euler}
\begin{split}
\VEC{w}_j &= (I_2+hA)^j \VEC{w}_0 = Q(I_2+hB)^j Q^{-1}\VEC{w}_0 \\
&= Q \begin{pmatrix}
(1-\lambda h)^j & 0 \\
0 & (1-0.1 h)^j
\end{pmatrix}
Q^{-1} \VEC{w}_0 \ .
\end{split}
\end{equation}

Suppose that
$\VEC{y}_0 = \begin{pmatrix}
1 \\ \lambda-0.1
\end{pmatrix}$.  This is an eigenvector of $A$ associated to the
eigenvalue $-0.1$.   The solution of (\ref{stiff_syst}) for
$0\leq t \leq t_f$ is then given by $\VEC{y}(t) = e^{-0.1 t} \VEC{y}_0$.

If $h$ is small enough such that
\begin{align}
(1-0.1h)^j &\approx e^{-0.1 jh} \label{first_restrict}\\
\intertext{and}
| 1 - \lambda h | &< 1  \; , \label{second_restrict}
\end{align}
then (\ref{sol_stiff_Euler}) will give a good approximation of
$\VEC{y}(t_j)$.

Unfortunately, because $\lambda$ is a large positive number, the
restriction (\ref{second_restrict}) is much more severe than the
restriction (\ref{first_restrict}).  So (\ref{second_restrict}) may force
$h$ to be smaller than the computer accuracy.  Therefore, the Euler's method
will not give a good approximation of the solution of
(\ref{stiff_syst}).  This type of problems occurs in system that have
two quite different "time-scales" as we have for the present system with
the two requirements on $h$.
\end{egg}

\begin{rmk}
Lambert \cite{La} defines stiffness as follows.
The differential equation (\ref{SODE}) is
{\bfseries stiff}\index{Stiff Differential Equation}
for $t_0 < t < t_f$ if, for all $t$ between $t_0$ and $t_f$,
$\RE \lambda_i(t) < 0$ for all eigenvalues $\lambda_i(t)$
of $\displaystyle \diff_{\VEC{y}} f(t,\VEC{y}(t))$ and
$\displaystyle \max_i \{ \RE \lambda_i(t) \} \gg
\min_i \{ \RE \lambda_i(t) \}$.  The ratio
\[
\max_i \{ \text{Re }\lambda_i(t) \}/\min_i \{\text{Re }\lambda_i(t)\}
\]
is called the {\bfseries stiffness ratio}\index{Stiffness Ratio}.

We do not use this definition because it does not cover all the cases
of stiffness as we have defined it.  Note that if the stiffness ratio is
large then the Lipschitz constant $L$ will be large.
\end{rmk}

\begin{egg}
We use the Trapezoidal Method to approximate the solution of
the initial value problem (\ref{stiff_syst}) considered in the previous
example.  Namely, we consider
\[
\VEC{w}_{i+1} = \VEC{w}_i + \frac{h}{2}\left( A \VEC{w}_{i+1}
+ A \VEC{w}_i \right) \quad , \quad 0 \leq i < N \ .
\]
If we solve for $\VEC{w}_{i+1}$, we get
\[
\VEC{w}_{i+1} = \left( \Id -\frac{h}{2}\,A \right)^{-1}
\left( \Id +\frac{h}{2}\,A\right) \VEC{w}_i \ .
\]
Note that $\Id - (h/2)A$ is invertible for all $h$.  By
induction, we get that
\[
\VEC{w}_{i+1} = \left( \Id -\frac{h}{2}\,A \right)^{-i-i}
\left( \Id +\frac{h}{2}\,A\right)^{i+1} \VEC{w}_0 \ .
\]
Since $A = Q B Q^{-1}$ for
$Q=\begin{pmatrix}
1 & 1 \\
0 & \lambda -0.1
\end{pmatrix}$ and
$B=\begin{pmatrix}
-\lambda & 0 \\
0 & -0.1
\end{pmatrix}$, we get
\[
\VEC{w}_{i+1} = Q\left( \Id -\frac{h}{2}\,B \right)^{-i-i}
\left( \Id +\frac{h}{2}\,B\right)^{i+1}Q^{-1} \VEC{w}_0 \ .
\]
Since
\[
\Id - \frac{h}{2} B =
\begin{pmatrix}
1 + \lambda h/2 & 0 \\
0 & 1 + h/20  
\end{pmatrix}
\quad \text{and} \quad
\Id + \frac{h}{2} B =
\begin{pmatrix}
1 - \lambda h/2 & 0 \\
0 & 1 - h/20  
\end{pmatrix}
\]
commute, we get
\begin{align*}
\VEC{w}_{i+1} &= Q \bigg( \left( \Id -\frac{h}{2}\,B \right)^{-1}
\left( \Id +\frac{h}{2}\,B\right)\bigg)^{i+1}Q^{-1} \VEC{w}_0 \\
&= Q
\begin{pmatrix}
\left(\displaystyle \frac{1-\lambda h/2}{1+\lambda h/2}\right)^{i+1}
& 0 \\
0 &                                                                     
\left(\displaystyle \frac{1- h/20}{1+ h/20}\right)^{i+1}
\end{pmatrix}
Q^{-1} \VEC{w}_0 \ .
\end{align*}
We finally get
\[
\VEC{w}_{i+1} =
\begin{pmatrix}
\left(\displaystyle \frac{1-\lambda h/2}{1+\lambda h/2}\right)^{i+1}
& \displaystyle
\frac{1}{\lambda - 0.1} \left( \left(\frac{1- h/20}{1+ h/20}\right)^{i+1}
- \left(\frac{1-\lambda h/2}{1+\lambda h/2}\right)^{i+1}\right)
\\
0 &                                                                     
\left(\displaystyle \frac{1- h/20}{1+ h/20}\right)^{i+1}
\end{pmatrix} \VEC{w}_0
\quad , \quad 0 \leq i < N \ .
\]
Independently of the choice of $h$, we have that
$\VEC{w}_i \rightarrow \VEC{0}$ as $i \rightarrow \infty$ because
$\displaystyle \bigg| \frac{1-\lambda h/2}{1+\lambda h/2} \bigg| < 1$
and
$\displaystyle \bigg| \frac{1-h/20}{1+h/20} \bigg| < 1$
for all $h$.

So there is no constraints on $h$ other than the one that we may
impose for the accuracy.

In general, we should use A-stable methods, like the Trapezoidal Method,
to solve stiff differential equations.
\end{egg}

\section{Exercises}

\begin{question}
Show that the following initial value problems are well posed.
\begin{center}
\begin{tabular}{*{1}{l@{\hspace{0.4em}}l@{\hspace{2.7em}}}l@{\hspace{0.4em}}l}
\subQ{a} &
$\displaystyle \begin{array}{rl}
y'(t) &= 2y(t)+2 \quad , \quad 0 \leq t \leq 1 \\
y(0) &= 1
\end{array}$ &
\subQ{b} &
$\displaystyle \begin{array}{rl}
y'(t) &= t^2 y(t) + 1   \quad , \quad 0 \leq t \leq 1 \\
y(0) &= 1
\end{array}$  \\[1em]
\subQ{c} &
$\displaystyle \begin{array}{rl}
y'(t) &= t^2 \sin(y(t)) + y(t)   \quad , \quad  0 \leq t \leq 1 \\
y(0) &= 1
\end{array}$ & &
\end{tabular}
\end{center}
\label{initQ1}
\end{question}

\begin{question}
Consider the initial value problem
\begin{align*}
y' &= 1 - y \quad , \quad 0 \leq t \leq 1 \\
y(0) &= 0
\end{align*}
\subQ{a} Estimate the value of $h$ (the step size) that minimize the
error bound for the Euler's method.  Assume that all rounding errors
have magnitude less than $10^{-8}$. \\
\subQ{b} With the value of $h$ found in (a), compute the error bound
on the interval $[0,1]$.
\label{initQ2}
\end{question}

\begin{question}
Consider the initial value problem
\begin{align*}
y' &= \frac{y + t}{t} \quad , \quad 1 \leq t \leq 2 \\
y(1) &= 0
\end{align*}
\subQ{a} Show that this initial value problem is well posed.\\
\subQ{b} Estimate the value of the step size $h$ that minimizes the error
bound for the Euler's method.  Assume that all rounding errors have
magnitude less than $10^{-8}$.\\
\subQ{c} Use the Euler's method with the value of $h$ found in (b)
(after a slight adjustment if needed) to find an approximation of the
solution to the initial value problem above.\\
\subQ{d} With the value of $h$ found in (b), compute the predicted
error bound at $t=2$ with the actual error.  What can you conclude?
\label{initQ3}
\end{question}

\begin{question}
Use Runge-Kutta method of order four to approximate the solution of
the initial value problem
\begin{equation}\label{rgktquest1}
\begin{split}
y' &= 1 + (t - y)^2    \quad , \quad  2 \leq t \leq 3 \\
y(2) &= 1
\end{split}
\end{equation}
Use a step size of $0.1$ and compute the absolute and relative error
at each step.  You obviously need to find the analytic solution of the
initial value problem (\ref{rgktquest1}) to compute the errors.
\label{initQ4}
\end{question}

\begin{question}
Use Runge-Kutta method of order four to approximate the solution of
\begin{align*}
y' &= \sin(t - y) \quad , \quad  2 \leq t \leq 3 \\
y(2) &= 1
\end{align*}
Use different step sizes.
\label{initQ5}
\end{question}

\begin{question}
Consider the Runge-Kutta Method
\[
\VEC{w}_{i+1} = \VEC{w}_i + h\left( \left(\frac{1}{2}+\beta\right)K_1
+ \left(\frac{1}{2}-\beta\right)K_2\right) \ ,
\]
where
\[
K_1 = f(\VEC{w}_i + \beta h K_1) \quad \text{and} \quad
K_2 = f(\VEC{w}_i + h K_1 + \beta h K_2) \ .
\]
\subQ{a} Give the Butcher array associated to this Runge-Kutta
Method.\\
\subQ{b} Use Theorem~\ref{OrdRel} to determine the order of the
method?  Show that it does not depend on $\beta$.\\
\subQ{c} Find the first term of the local truncation error.\\
\subQ{d} If we use the Runge-Kutta Method above to find an
approximation of the solution of the initial value problem
\begin{equation}\label{GradLSODE}
\begin{split}
\VEC{y}'(t) &= A \VEC{y}(t) \quad , \quad a \leq t \leq b\\
\VEC{y}(a) &= \VEC{y}_0
\end{split}
\end{equation}
where $A$ is a \nn matrix, can we chose $\beta$ to get a method of
order greater than the ordre found in (b)? \\
\subQ{e} Show that the Runge-Kutta Method above applied to
(\ref{GradLSODE}) yields a finite difference equation of the form
\[
\VEC{w}_{i+1} = R(h A,\beta) \VEC{w}_i \ ,
\]
where $R$ is a rational function.
\label{initQ6}
\end{question}

\begin{question}
Compute the local truncation error of the trapezoidal method
\[
w_{i+1} = w_i + \frac{h}{2}\left( f(t_i,w_i) + f(t_{i+1},w_{i+1}) \right)
\quad , \quad 0 < i < N \ .
\]
What is the order of this method?  Is the method consistent?
\label{initQ7}
\end{question}

\begin{question}
Show that two successive steps of the trapezoidal method
\[
w_{i+1} = w_i + \frac{h}{2}\left( f(t_i,w_i) + f(t_{i+1},w_{i+1}) \right)
\]
($t_i$ to $t_{i+1}$ followed by $t_{i+1}$ to $t_{i+2}$) yields one
step of a $3$-stage Runge-Kutta Method ($t_i$ to $t_{i+2}$).  Give the
Butcher array of the Runge-Kutta Method.  What is the order of this
method?
\label{initQ8}
\end{question}

\begin{question}
Find the $2$-stage Runge-Kutta Method given by the collocation method
associated to the nodes $\alpha_1 = 1/3$ and $\alpha_2 = 2/3$.  Find the order
of this method?  What is the maximal order of a $2$-stage Runge-Kutta
Method that we can get with the collocation method?
\label{initQ9}
\end{question}

\begin{question}
Consider an implicit Runge-Kutta Method given by the Butcher array
\[
\begin{array}{c|cccc}
\alpha_1 & \beta_{1,1} & \beta_{1,2} & \ldots & \beta_{1,k} \\
\alpha_2 & \beta_{2,1} & \beta_{2,2} & \ldots & \beta_{2,k} \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
\alpha_k & \beta_{k,1} & \beta_{k,2} & \ldots & \beta_{k,k} \\
\hline
& \gamma_1 & \gamma_2 & \ldots & \gamma_k \\
\end{array}
\]
Assume that the order of the method is greater or equal to $p$ and
$\alpha_i \not= \alpha_j$ for $i\neq j$.  Show that this method is
given by the collocation method if and only if
\begin{equation}\label{GradQ7}
\sum_{j=1}^k \, \beta_{i,j} \alpha_j^{n-1} = \frac{\alpha_i^n}{n}
\quad \text{and} \quad
\sum_{j=1}^k \, \gamma_j \alpha_j^{n-1} = \frac{1}{n}
\end{equation}
for $1 \leq i,n \leq k$.
\label{initQ10}
\end{question}

\begin{question}
Consider the initial value problem
\begin{align*}
y'(t) &= \mu y(t) \quad , \quad  t \geq 0 \\
y(0) &= y_0
\end{align*}
Show that all semi-implicit Runge-Kutta Method applied to this initial
value problem is of the form $w_i = (r(h\mu))^iw_0$, where $r(z)$
is a rational function whose denominator is a product of factor of
degree one.
\label{initQ11}
\end{question}

\begin{question}
Prove without using Theorem~\ref{RKcolAstable} that the Runge-Kutta
Method given by the Butcher array 
\[
\begin{array}{c|cc}
(3-\sqrt{3})/6 & 1/4 & (3-2\sqrt{3})/12 \\
(3+\sqrt{3})/6 & (3+2\sqrt{3})/12 & 1/4 \\
\hline
 & 1/2 & 1/2
\end{array}
\]
is A-stable?
\label{initQ12}
\end{question}

\begin{question}
Consider the initial value problem
\begin{equation} \label{IVPQuest2}
\begin{split}
y'(t) &= f(t,y(t)) \quad , \quad t_0 \leq t \leq t_f \\
y(0) &= y_0
\end{split}
\end{equation}
An explicit method to approximate the solution of (\ref{IVPQuest2}) is
defined as follows.  The approximation $w_i$ of $y(t_i)$ is given by
the solution of
\[
w_{i+1} = w_i + \frac{h}{2}\left( 3 f(t_i,w_i) - f(t_{i-1},w_{i-1})
\right) \ , \quad
w_1 = y_1 \quad \text{and} \quad w_0 = y_0 \ .
\]
\subQ{a} Show that this method is of order $2$ and that it is
consistent.\\
\subQ{b} Is this method strongly stable?\\
\subQ{c} Is this method convergent?
\label{initQ13}
\end{question}

\begin{question}
Consider the initial value problem
\begin{equation} \label{IVPQuest1}
\begin{split}
y'(t) &= f(t,y(t)) \quad , \quad t_0 \leq t \leq t_f \\
y(0) &= y_0
\end{split}
\end{equation}
An implicit method to approximate the solution of (\ref{IVPQuest1}) is
defined as follows.  The approximation $w_i$ of $y(t_i)$ is given by 
the solution of
\[
w_{i+1} = w_{i-1} + \frac{2}{3} \,h\left( f(t_{i+1},w_{i+1}) +
 f(t_{i},w_{i}) +  f(t_{i-1},w_{i-1}) \right) \ , \quad
w_1 = y_1 \quad \text{and} \quad  w_0 = y_0  \ .
\]
\subQ{a} Show that this method is of order $2$ and that it is
consistent. \\
\subQ{b} Does this method satisfy the root condition?\\
\subQ{c} Is this method convergent?
\label{initQ14}
\end{question}

\begin{question}
Consider the initial value problem
\begin{equation}
\begin{split}
y'(t) &= f(t,y(t)) \quad , \quad t_0 \leq t \leq t_f \\
y(0) &= y_0
\end{split} \label{IVPQuest3}
\end{equation}
The Simpson's method is an implicit method to approximate the solution
of (\ref{IVPQuest3}) defined as follows.  The approximation $w_i$ of
$y(t_i)$ is given by the solution of
\[
w_{i+1} = w_{i-1} + \frac{h}{3}\left( f(t_{i+1},w_{i+1}) + 4 
f(t_i,w_i) + f(t_{i-1},w_{i-1}) \right) \ , \quad
w_1 = y_1 \quad \text{and} \quad   w_0 = y_0 \ .
\]
\subQ{a} Apply the Simpson rule of integration to
\[
\int_{t_{i-1}}^{t_{i+1}}\, f(t,y(t))\,\dx{t}
\]
to derive the Simpson method above and its local truncation error.\\
\subQ{b} Show that Simpson's method is consistent. \\
\subQ{c} Does the Simpson's method satisfy the root condition?\\
\subQ{d} Is the Simpson's method convergent?
\label{initQ15}
\end{question}

% The local truncation error of this method is
% $\tau_{i+1}(h) = \frac{1}{90}\,y^{5}(\xi_i) h^4$ for some $\xi_i$.

\begin{question}
To approximate the solution of the initial value problem
\begin{equation}\label{GradQ2}
\begin{split}
y'(t) &= t \quad , \quad 0 \leq t \leq 5 \\
y(0) &= 0
\end{split}
\end{equation}
we use the multistep method
\begin{align}
w_{i+1} &= w_i + \frac{h}{12}\left( 4 f(t_{i+1},w_{i+1}) +
  9 f(t_i,w_i) - f(t_{i-1},w_{i-1}) \right) \ ,  \label{GradQ1} \\
w_1 & = y_1 \quad \text{and} \quad w_0 = y_0 \ .  \nonumber
\end{align}
for $1\leq i <N$ with $t_i = i h$ and $h = 5/N$

\subQ{a} Write the difference equation associated to (\ref{GradQ2}).\\
\subQ{b} Find the general solution of the difference equation
in (a).  A particular solution of this equation is of the form
$w_i = A i^2 + B i$ for some constants $A$ and $B$.\\
\subQ{c} Find the solution of the difference equation in (a)
with $w_0 = 0$.\\
\subQ{d} Does the solution that you have found in (c) converge to the
solution of (\ref{GradQ2}) as $h\to 0$?\\
\subQ{e} Does this multistep method satisfy the root condition?\\
\subQ{f} Is this multistep method consistent?
\label{initQ16}
\end{question}

\begin{question}
Use the technique presented in Section~\ref{AAMM} to answer the
following questions.

\subQ{a} Construct a multistep method of order at least $2$
of the form
\begin{align*}
w_{i+1} &= w_{i-2} + \sum_{j=-1}^m b_j f(t_{i-j},w_{i-j}) \quad ,
\quad  m \leq i < N \\
w_i &= y_i \quad , \quad 0 \leq i \leq m
\end{align*}
\subQ{b} Construct a multistep method of order at least $3$
of the form
\begin{align*}
w_{i+1} &= w_{i-2} + \sum_{j=-1}^m b_j f(t_{i-j},w_{i-j}) \quad ,
\quad m \leq i < N \\
w_i &= y_i \quad , \quad 0 \leq i \leq m
\end{align*}
\label{initQ17}
\end{question}

\begin{question}
Consider the multistep method
\begin{align*}
w_{i+1} &= \sum_{j=0}^m a_j w_{i-j} + h \sum_{j=-1}^m b_j f(t_{i-j},w_{i-j})
\quad , \quad m \leq i < N \\
w_i&= y_i \quad , \quad 0 \leq i \leq m
\end{align*}
Let
\[
p(w) = 1 - \sum_{j=0}^m a_j w^j \quad \text{and} \quad
q(w) = \sum_{j=-1}^m  b_j w^j \ .
\]
Moreover, let $p_1(w) = p(w)$, $p_{k+1}(w) = 1 -w p_k'(w)$ for $k>0$,
$q_1(w) = q(w)$ and $q_{k+1}(w) = -w q_k'(w)$ for $k>0$.
Show that the method is of order $r$ if and only if
$p_1(1)=0$, $p_{k+1}(1)= k q_k(1)$ for $1 \leq k \leq r$, and
$p_{r+2}(1)\neq (p+1) q_{r+1}(1)$.
\label{initQ18}
\end{question}

\begin{question}
\subQ{a} Find the multistep method of the form
\[
w_{i+1} = a_0 w_i + a_1 w_{i-1} + h \left( b_0
  f(t_i,w_i) + b_1 f(t_{i-1},w_{i-1}) \right)
\]
of highest order.\\
\subQ{b} What is the order of the method?\\
\subQ{c} Is this method A-stable?
\label{initQ19}
\end{question}

\begin{question}
If the multistep method
\[
w_{i+1} = \sum_{j=0}^m a_j w_{i-j} + \sum_{j=-1}^m b_j f(t_{i-j},w_{i-j})
\]
is convergent, shows that $0$ is on the boundary of the region of
absolute stability for this method.
\label{initQ20}
\end{question}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "notes"
%%% End: 
