\chapter[Boundary Value Problems]
{Boundary Value Problems for Ordinary Differential Equations}
\label{chapBoundValProbl}

The content of this chapter is based in great part on \cite{K}.

\section{Introduction}

\begin{egg}
A simple example of a
{\bfseries boundary value problem}\index{Boundary Value Problem} is given by
the second order differential equation
\begin{align}
&y''(t) + y(t) = 0 \quad , \quad  0 \leq t \leq \frac{\pi}{2}
\label{second_order} \\
&y(0) = 0 \quad \text{and} \quad y(\pi/2) = 1 \nonumber
\end{align}
The conditions that $y$ must satisfy at $0$ and $\pi/2$ are the
{\bfseries boundary conditions}\index{Boundary Conditions}.  The
general solution of (\ref{second_order}) is
$y(t) = a\cos(t) + b\sin(t)$ where $a$ and $b$ are constants.
$y(0)=0$ implies that $a=0$ and $y(\pi/2) = 1$ implies that
$b=1$.  The solution of the boundary value problem is therefore
$y(t) = \sin(t)$.
\label{egg_BVP}
\end{egg}

We have to be prudent when solving boundary value problems
because there may not exist a solution.

\begin{egg}
The boundary value problem
\begin{align*}
& y''(t) + y(t) = 0 \quad , \quad  0 \leq t \leq \pi \\
& y(0) = 0 \quad \text{and} \quad y(\pi) = 1 \nonumber
\end{align*}
does not have a solution as can be seen by trying to satisfy the
boundary conditions with $y(t) = a\cos(t) + b\sin(t)$.
\end{egg}

\section{Shooting Methods}

This section is based on Keller's lectures \cite{K}.

Consider the boundary value problem
\begin{equation} \label{IntroGen2ndOrder}
\begin{split}
& y'' = f(t,y,y')  \quad , \quad a \leq t \leq b \\
& y(a) = \alpha \quad \text{and} \quad y(b) = \beta      
\end{split}
\end{equation}
Assuming that this problem has a solution (which is not always true
even for nice boundary value problems), a possible approach to solve
this problem is to use our knowledge of initial value problems.
We have seen several analytical and numerical methods to solve initial
value problems.   We solve the initial value problem
\begin{equation}\label{IntroShoot}
\begin{split}
& y'' = f(t,y,y')  \quad , \quad a \leq t \leq b \\
& y(a) = \alpha \quad \text{and} \quad y'(a) = x
\end{split}
\end{equation}
to find a solution $y(t) = y(t,x)$ for $a \leq t \leq b$.  Then we
find $x_b$ such that $y(b,x_b) - \beta = 0$ is satisfied to get the
solution $y(t) = y(t,x_b)$ of (\ref{IntroGen2ndOrder}).

When no analytical solution of (\ref{IntroShoot}) is available,
numerical solutions of the initial value problem have to be found to
approximate $y(b)$.  This means that a value of $x$ has to be chosen
and a numerical solution of (\ref{IntroShoot}) has to be found
to be able to compute $y(b) = y(b,x)$.  If $y(b) \neq \beta$, then
another value of $x$ has to be chosen and another numerical solution
of (\ref{IntroShoot}) has to be found to get a new value of
$y(b) = y(b,x)$.  This has to be repeated until we find $x_b$ such
that $y(b) = y(b,x_b)$ is closed enough to $\beta$ to meet the
required accuracy.  This approach bears some resemblance to shooting
where one tries to adjust the initial velocity to reach the target.

We present a more general approach of the shooting method than the one
usually found in textbooks.  Solving (\ref{IntroShoot}) using
the numerical methods that we have presented requires rewriting 
(\ref{IntroShoot}) has a system of first order differential equations.
Moreover, the boundary conditions may be more complex than the simple
ones that we used given above.  Our approach will take all that into
consideration.

\subsection{Shooting Method for Linear Boundary Value Problems}
\label{SMLBVP}

Let $\GL{n}$ be the group of \nn matrices with real entries.
We consider the
{\bfseries boundary value problem}\index{Boundary Value Problem}
\begin{equation} \label{LinBVP}
\begin{split}
P(\VEC{y}(t)) \equiv \VEC{y}'(t) - A(t) \VEC{y}(t) &= f(t)
\quad , \quad a \leq t \leq b  \\
B_a\VEC{y}(a) + B_b\VEC{y}(b) &= \VEC{y}_c
\end{split}
\end{equation}
where $\VEC{y} : [a,b] \rightarrow \RR^n$,
$A:[a,b] \rightarrow \GL{n}$ and $f:[a,b]\rightarrow \RR^n$
are sufficiently differentiable functions, and
$B_a, B_b \in \GL{n}$.

To solve this problem, we proceed as follows:

\begin{algo}[Shooting Method] \label{AlgoSimpleSM}
\begin{enumerate}
\item We solve the initial value problems
\begin{equation} \label{partS}
\begin{split}
P(\VEC{y}_0(t)) &= f(t) \quad , \quad a \leq t \leq b \\
\VEC{y}_0(a) &= \VEC{y}_c
\end{split}
\end{equation}
and
\begin{equation} \label{genS}
\begin{split}
P(\VEC{y}_j(t)) &= \VEC{0} \quad , \quad a \leq t \leq b \\
\VEC{y}_j(a) &= \VEC{e}_j
\end{split}
\end{equation}
for $j=1$, $2$, \ldots, $n$.  Any other vector than $\VEC{y}_c$ would
have been acceptable.
\item The general solution $\VEC{y}_g:[a,b]\to \RR^n$ of the
differential equation in (\ref{LinBVP}) is of the form
\[
\VEC{y}_g(t) = \VEC{y}_0(t) + \sum_{j=1}^n d_j \VEC{y}_j(t) =
\VEC{y}_0(t) + Y(t) \VEC{d} \ ,
\]
where
$Y(t) = \begin{pmatrix}
\VEC{y}_1(t) & \VEC{y}_2(t) & \ldots & \VEC{y}_n(t) \end{pmatrix}$
and $\VEC{d} =
\begin{pmatrix} d_1 & d_2 & \ldots & d_n \end{pmatrix}^\top \in \RR^n$.
\item $\VEC{y}_g$ will be the solution of the boundary value
problem (\ref{LinBVP}) if there exists $\VEC{d} \in \RR^n$ such that
\begin{equation} \label{BVcond}
\VEC{y}_c - B_a\VEC{y}_0(a) - B_b\VEC{y}_0(b) = Q\VEC{d} \ ,
\end{equation}
where $Q = B_a + B_b Y(b)$.  With this value of $\VEC{d}$, we have
that $B_a\VEC{y}_g(a) + B_b\VEC{y}_g(b) = \VEC{y}_c$.  \label{ShootStep3}
\end{enumerate}
\end{algo}

\begin{egg}[Example~\ref{egg_BVP} continued]
The boundary value problem of Example~\ref{egg_BVP} can be restated in
the format (\ref{LinBVP}) with $a=0$, $b=\pi/2$,
$\displaystyle \VEC{y}(t) = \begin{pmatrix} x(t) \\ x'(t) \end{pmatrix}$, 
$\displaystyle A = \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}$,
$\displaystyle f(t) = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$,
$\displaystyle B_a = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$,
$\displaystyle B_b = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}$ and
$\VEC{y}_c = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$.

Since the general solution of
$\displaystyle P(\VEC{y}(t)) = \VEC{y}'(t) - A(t) \VEC{y}(t) = \VEC{0}$ is
\[
\VEC{y}(t) = e^{tA} \VEC{y}(0) =
\begin{pmatrix} \cos(t) & \sin(t) \\ -\sin(t) & \cos(t) \end{pmatrix}
\VEC{y}(0) \ ,
\]
we get
$\displaystyle \VEC{y}_0(t) =
\begin{pmatrix} \sin(t) \\ \cos(t) \end{pmatrix}$,
$\displaystyle \VEC{y}_1(t) =
\begin{pmatrix} \cos(t) \\ -\sin(t) \end{pmatrix}$,
$\displaystyle \VEC{y}_2(t) =
\begin{pmatrix} \sin(t) \\ \cos(t) \end{pmatrix}$ and
$\displaystyle Y(t) = \begin{pmatrix}
\cos(t) & \sin(t) \\  -\sin(t) & \cos(t) \end{pmatrix}$.  Thus
\[
\VEC{y}_g(t) =
\begin{pmatrix} \sin(t) \\ \cos(t) \end{pmatrix}
+ \begin{pmatrix} \cos(t) & \sin(t) \\ -\sin(t) & \cos(t) \end{pmatrix}
\begin{pmatrix} d_1 \\ d_2 \end{pmatrix}
\]
and $\displaystyle Q = \Id$.  Since
$\displaystyle \VEC{y}_c - B_a\VEC{y}_0(a) - B_b\VEC{y}_0(b) = \VEC{0}$
and $Q$ is non-singular, the only solution of
(\ref{BVcond}) is $\VEC{d} = \VEC{0}$.  We find the solution
$\VEC{y}_g(t) = \VEC{y}_0(t)$ as expected.
\end{egg}

\begin{theorem}
If $A:[a,b]\to \GL{n}$ and $f:[a,b]\to \RR^n$ are functions of
class $C^r$, then the boundary value problem (\ref{LinBVP}) has a unique
solution of class $C^{r+1}$ if and only if $Q$ defined in
step~\ref{ShootStep3} above is invertible.  \label{BVPCdiff}
\end{theorem}

\begin{proof}
The existence (and uniqueness) of the solutions to
(\ref{partS}) and (\ref{genS}) is proved in a basic course on ordinary
differential equations.  It is proved in basic linear algebra that
(\ref{BVcond}) has a unique solution if and only if $Q$ is invertible.
\end{proof}

The following code implement the shooting method for our linear
boundary value problem.(\ref{LinBVP}).

\begin{code}[Shooting Method]
To approximate the solution of the boundary value problem
$y' - A(t) = f(f)$ with $B_a y(a) + B_b y(b) = y_c$  for $a \leq t \leq b$.
The classical fourth order Runga-Kutta is use to solve initial value
problems in the algorithm.  For $N$ given, the step size is $h = (b-a)/N$ and
$t_i = a + i h $ for  $0 \leq i \leq N$.\\
\subI{Input} The vector valued function $f:[a,b] \to \RR^n$ (f in the
code below).\\
The \nn matrix valued function $A$ defined on $[a,b]$ (A in the code below).\\
The \nn matrix $B_a$ (Ba in the code below).\\
The \nn matrix $B_b$ (Bb in the code below).\\
The (column) vector $y_c$ (yc in the code below).\\
The number $N$ of equal partitions of $[a,b]$.\\
The endpoints $a$ and $b$ of the interval of integration $[a,b]$ \\
\subI{Output} The \nm{n}{(N+1)} matrix ww that contains the
approximation $w_{k,i}$ of $y_{k,i} = y_k(t_i)$ for $1 \leq k \leq n$
and $0 \leq i \leq N$, and the vector tt that contains
$t_i$ for $0 \leq i \leq N$.
\small
\begin{verbatim}
function [tt,ww] = shooting(f,A,Ba,Bb,yc,N,a,b)
  funct1 = @(t,y) A(t)*y + f(t);
  funct2 = @(t,y) A(t)*y; 
  h = (b-a)/N;
  n = length(yc);

  %   Solve the initial value problem
  %   y'(t) - A(t) y(t) = f(t)  with  y(a) = y_c
  [tt,ww1] = rgkt4(funct1,h,N,a,yc);

  %   Solve the initial value problems
  %   y'(t) - A(t) y(t) = 0  with  y(a) = e_i
  %   for 1 <= j <= n
  WW = repmat(NaN,n,N+1,n);
  for j=1:1:n
    yj = zeros(n,1);
    yj(j) = 1;
    [tt,ww2] = rgkt4(funct2,h,N,a,yj);
    WW(:,:,j) = ww2;
  end

  %  Solve yc -B_a y_0(a) - B_b y_0(b) = Q d
  %  with Q = B_a + B_b Y(b)
  Y = yc - Ba*ww1(:,1) -Bb*ww1(:,N+1);
  Q = Ba + Bb*squeeze(WW(:,N+1,:));
  d = linsolve(Q,Y);

  ww2 = repmat(0,n,N+1);
  for j=1:1:n
    ww2 =  ww2 + d(j)*squeeze(WW(:,:,j));
  end
  ww = ww1 + ww2;
end
\end{verbatim}
\end{code}

We could have used one of the ``ode'' solvers in Matlab.  However, we
have chosen to use our own implementation of Runage-Kutta in $\RR^n$.
It is basically the same code that we have presented in
Code~\ref{rgkt4}.  We give it below.

\begin{code}[Runge-Kutta of Order Four]
To approximate the solution of the initial value problem
\[
\begin{split}
\VEC{y}'(t) &= f(t,\VEC{y}(t)) \quad , \quad t \geq t_0 \\
\VEC{y}(0) &= \VEC{y}_0
\end{split}
\]
\subI{Input} The function $f(t,\VEC{y})$ (funct in the code below). \\
The step-size $h$.\\
The number of steps $N$.\\
The initial time $t_0$ (t0 in the code below) and the initial
conditions $\VEC{y}_0$ (y0 in the code below) at $t_0$.\\
\subI{Output} The approximations $\VEC{w}_i$ (ww(:,i+1) in the code below)
of $\VEC{y}(t_i)$ at $t_i$ (tt(i+1) in the code below).
\small
\begin{verbatim}
function [tt,ww] = rgkt4(funct,h,N,t0,y0)
  tt(1) = t0;
  ww(:,1) = y0;
  h2 = h/2;
  for j=1:N
    tt(j+1) = tt(1)+j*h;
    k1 = h*funct(tt(j),ww(:,j));
    k2 = h*funct(tt(j)+h2,ww(:,j)+k1/2);
    k3 = h*funct(tt(j)+h2,ww(:,j)+k2/2);
    k4 = h*funct(tt(j+1),ww(:,j)+k3);
    ww(:,j+1) = ww(:,j) + (k1+2*(k2+k3)+k4)/6;
  end
end
\end{verbatim}
\end{code}

\begin{egg}
Consider the following boundary value problem
\[
  y_1'(t) = y_2(t) \quad , \quad y_2'(t) = 4 y_1(t) - 3 e^t
\]
with
\[
y_1(0) = 1 \quad , \quad y_2(1) = e
\]
This problem can be restated as
$\VEC{y}'(t) = A(t) \VEC{y}(t) + f(t)$ with
$B_a \VEC{y}(0) + B_b\VEC{y}(1) = \VEC{y}_c$, where
\[
\VEC{y} = \begin{pmatrix} y_1(t) \\ y_2(t) \end{pmatrix} \ ,
\ A(t) = \begin{pmatrix} 0 & 1 \\ 4 & 0 \end{pmatrix} \ ,
\ f(t) = \begin{pmatrix} 0 \\ -3 e^t \end{pmatrix} \ ,
\ B_a = \begin{pmatrix} 1 & 0 \\  0 & 0 \end{pmatrix} \ ,
\ B_b = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} \ \text{and}
\ \VEC{y}_c = \begin{pmatrix} 1 \\ e \end{pmatrix} \ .
\]
If we use the code above with $N=25$, we find the following approximations of
the solution.
\[
\begin{array}{llllc@{\hspace{2em}}llll}
i & t_i & w_{1,i} & w_{2,i} & & i & t_i & w_{1,i} & w_{2,i} \\
\cline{1-4} \cline{6-9}
0 & 0 & 1.0 & 1.0  & & 17 & 0.68 & 1.9738778 & 1.9738778 \\
1 & 0.04 & 1.0408108 & 1.0408111 & & 18 & 0.72 & 2.0544333 & 2.0544332 \\
2 & 0.08 & 1.0832871 & 1.0832873 & & 19 & 0.76 & 2.1382763 & 2.1382762 \\
3 & 0.12 & 1.1274969 & 1.1274971 & & 20 & 0.80 & 2.2255410 & 2.2255409 \\
4 & 0.16 & 1.1735109 & 1.1735111 & & 21 & 0.84 & 2.3163670 & 2.3163669 \\
5 & 0.20 & 1.2214028 & 1.2214030 & & 22 & 0.88 & 2.4108997 & 2.4108996 \\
6 & 0.24 & 1.2712492 & 1.2712494 & & 23 & 0.92 & 2.5092904 & 2.5092903 \\
7 & 0.28 & 1.3231299 & 1.3231300 & & 24 & 0.96 & 2.6116965 & 2.6116963 \\
8 & 0.32 & 1.3771278 & 1.3771280 & & 25 & 1.00 & 2.7182818 & 2.7182816 \\
\vdots & \vdots & \vdots & \vdots & & & & &
\end{array}
\]
where $w_{1,i} \approx y_{1,i} = y_1(t_i)$ and
$w_{2,i} \approx y_{2,i} = y_2(t_i)$ for all $i$.
All the approximations have at least $6$-digit accuracy.  The exact solution is
$\displaystyle \VEC{y}(t) = \begin{pmatrix} e^t \\ e^t \end{pmatrix}$.

For the sake of completeness, here is the code used to call the
shooting method.

\begin{code}
\small
\begin{verbatim}
  format long
  f = @(t) [ 0 ; -3*exp(t) ];
  A = @(t) [ 0 1 ; 4 0 ];
  Ba = [ 1 0 ; 0 0 ];
  Bb = [ 0 0 ; 1 0 ];
  yc = [ 1 ; exp(1) ];
  N = 25;
  [t,w] = shooting(f,A,Ba,Bb,yc,N,0,1)
\end{verbatim}
\end{code}
\label{eggShootCode1}
\end{egg}

\begin{egg}
The following example was used in \cite{CSD} to test the shooting
method and the parallel shooting method that we will see shortly.

Consider the boundary value problem
\[
  y^{(4)}(t) - 401 y''(t) + 400 y(t) +1 - 200 t^2 = 0
\]
with
\[
 y(0) = 1 \ , y'(0) = 1 \ , \ y(1) = \frac{3}{2} + \sinh(1) \
 \text{and}\  y'(1) = 1 + \cosh(1) \ .
\]
This problem can be rewritten as
$\VEC{y}'(t) = A(t) \VEC{y}(t) + f(t)$ with
$B_a \VEC{y}(0) + B_b\VEC{y}(1) = \VEC{y}_c$, where
\begin{align*}
\VEC{y}
&= \begin{pmatrix} y_1(t) \\ y_2(t) \\ y_3(t) \\ y_4(t) \end{pmatrix} \ ,
\ A(t) = \begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\ -400 & 0 & 401 & 0 \end{pmatrix} \ ,
\ f(t) = \begin{pmatrix} 0 \\ 0 \\ 0 \\ -1 + 200 t^2 \end{pmatrix} \ ,
\ B_a = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \end{pmatrix} \ , \\
B_b &= \begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \end{pmatrix} \ \text{and}
\ \VEC{y}_c = \begin{pmatrix} 1 \\ 1 \\ 3/2 + \sinh(1) \\ 1 + \cosh(1)
\end{pmatrix} \ .
\end{align*}

If we use the code above with $N=25$, we find the following approximations of
the solution $y(t) = y_1(t)$.
\[
\begin{array}{lllc@{\hspace{2em}}lllc@{\hspace{2em}}lll}
i & t_i & w_{1,i} & & i & t_i & w_{1,i} & & i & t_i & w_{1,i} \\ 
\cline{1-3} \cline{5-7} \cline{9-11}
0 & 0 & 1.0000000 & & 9 & 0.36 & 1.4326265 & & 18 & 0.72 & 2.0430405 \\
1 & 0.04 & 1.0408107 & & 10 & 0.40 & 1.4907523 & & 19 & 0.76 & 2.1241049 \\
2 & 0.08 & 1.0832854 & & 11 & 0.44 & 1.5511354 & & 20 & 0.80 & 2.2081060 \\
3 & 0.12 & 1.1274882 & & 12 & 0.48 & 1.6138455 & & 21 & 0.84 & 2.2951282 \\
4 & 0.16 & 1.1734835 & & 13 & 0.52 & 1.6789536 & & 22 & 0.88 & 2.3852584 \\
5 & 0.20 & 1.2213360 & & 14 & 0.56 & 1.7465317 & & 23 & 0.92 & 2.4785857 \\
6 & 0.24 & 1.2711106 & & 15 & 0.60 & 1.8166536 & & 24 & 0.96 & 2.5752018 \\
7 & 0.28 & 1.3228730 & & 16 & 0.64 & 1.8893942 & & 25 & 1.00 & 2.6752012 \\
8 & 0.32 & 1.3766894 & & 17 & 0.68 & 1.9648304 & &  & &
\end{array}
\]
where $w_{1,i} \approx y_i = y(t_i)$ for all $i$ because $y_1(t) = y(t)$
for all $t$.   All the approximations have at least $7$-digit
accuracy.  The exact solution is
$\displaystyle \VEC{y}(t) = 1 + t^2/2 + \sinh(t)$.

It is interesting to note how much more accurate our results are then
those in \cite{CSD}.  The difference is not in the algorithm used
because we both use a simple shooting method.  The difference is in
the fact that they use single precision arithmetic (common for the
main frame computers at that time) while we use double precision
arithmetic.

Moreover, the matrix $A(t)$ has eigenvalues $\pm 1$ and $\pm 20$.
So, we have a stiff ordinary differential equation.  However, the
solution that we are approximating is the one associated to the
eigenvalues $\pm 1$.  Fortunately, the fact that we use double
precision arithmetic and that we have imposed a condition at $t=1$
eliminate the part associated to the eigenvalue $20$.  This explain
why the shooting method could give us a reasonably good solution.
The reader is invited to numerically solve the
initial value problem $\VEC{y}'(t) = A(t) \VEC{y}(t) + f(t)$ with
$\VEC{y}(0) = \VEC{y}_c$ using the classical fourth order Runge-Kutta
methods.  The solution obtained is not even remotely closed to
$\displaystyle \VEC{y}(t) = 1 + t^2/2 + \sinh(t)$.  The part of the 
general solution associated to the eigenvalue $20$ dominates.
\label{eggShootCode2}
\end{egg}

\subsection{Numerical Aspect of the Shooting Method}

Let $\{t_i\}_{i=0}^N$ be a partition of $[a,b]$.  More
precisely, $t_0 = a$, $t_N = b$ , $t_{i+1} = t_i + h_i$ with $h_i > 0$
for $0\leq i < N$ and
$\displaystyle h = \max_{0\leq i < N} h_i \leq \theta \min_{0\leq i < N} h_i$
for some constant $\theta$.

\begin{rmk}[Important]
The constant $\theta \geq 1$ is an absolute constant for the entire
chapter.  In particular, $\theta$ does not vary with the choice of
partitions.  If $\theta = 1$, we have that $h_i = h$ for all $i$.  The
step size is constant.
\end{rmk}

We assume that a stable and convergent numerical method is used to
numerically solve (\ref{partS}) and (\ref{genS}).  Let
$\VEC{w}_{j,i}$ be the numerical approximation of $\VEC{y}_j(t_i)$
given by the numerical method for $0\leq i \leq N$ and $0\leq j \leq n$.
Suppose that
\begin{equation} \label{bigO}
\| \VEC{w}_{j,i} - \VEC{y}_j(t_i) \| = O(h^p) \quad , \quad 0 \leq i
\leq N \ ,
\end{equation}
for $0\leq j \leq n$.

If we set $Q_T = B_a + B_b W_N$, where
$\displaystyle W_i = \begin{pmatrix}
\VEC{w}_{1,i} & \VEC{w}_{2,i} & \ldots & \VEC{w}_{n,i} \end{pmatrix}
\in \GL{n}$ for $0 \leq i \leq N$, then (\ref{BVcond}) becomes
\begin{equation} \label{BVcondN}
\VEC{y}_c - B_a \VEC{w}_{0,0} - B_b \VEC{w}_{0,N} = Q_T\VEC{d}_T
\end{equation}
for some $\VEC{d}_T \in \RR^n$.
Since $\| Q - Q_T \| = O(h^p)$ from (\ref{bigO}) and $Q$ is
invertible, we get from Banach Lemma that $Q_T$ is invertible for $h$
small enough.  To be more precise, if $h$ is small enough to have
$\|Q - Q_T\| < 1/\|Q^{-1}\|$, then $Q_T$ is invertible.  Thus
(\ref{BVcondN}) has a unique solution.  Note that (\ref{BVcondN}) is a
system of linear equations which is not necessarily easy to solve.

An approximation of the solution $\VEC{y}_g$ of the boundary value
problem (\ref{LinBVP}) is given by
\[
\VEC{w}_i = \VEC{w}_{0,i} + W_i \VEC{d}_T \quad , \quad 0 \leq i \leq N \ .
\]

We now show that the approximation of
the solution given by the shooting method also satisfies
\[
\| \VEC{w}_i - \VEC{y}(t_i) \| = O(h^p) \ .
\]
Since $\| Q - Q_T \| = O(h^p)$, we have that $\|Q_T\|$ is uniformly
bounded for $h$ small enough.  To prove this, choose $h_0$ such that
$\|Q - Q_T\| < 1/(2\|Q^{-1}\|) $ for $h < h_0$ and note that
\[
\|Q_T^{-1}\| - \|Q^{-1}\| \leq \|Q_T^{-1} - Q^{-1}\|
= \| Q_T^{-1} (Q - Q_T) Q^{-1} \|
\leq \|Q_T^{-1}\| \, \|Q- Q_T \| \, \|Q^{-1}\|
\]
implies
\[
\|Q_T^{-1}\| \leq \frac{\|Q^{-1}\|}{1- \|Q-Q_T\|\,\|Q^{-1}\|} \leq 2\|Q^{-1}\|
\]
for $h < h_0$.  It follows that
\begin{equation} \label{bigOO}
\| Q^{-1} - Q_T^{-1} \| = \| Q^{-1}(Q_T - Q) Q_T^{-1}\|
\leq \underbrace{\| Q^{-1}\|}_{\text{bounded}}\,
\underbrace{\|Q_T - Q\|}_{=O(h^p)}\,\underbrace{\| Q_T^{-1}\|}_{\text{bounded}}
= O(h^p) \ .
\end{equation}
Since
\begin{align*}
\| \VEC{y}(t_i) - \VEC{w}_i \|
&= \| \VEC{y}_0(t_i) + Y(t_i)\VEC{d} - \VEC{w}_{0,i}
- W_i \VEC{d}_T \| \\
& \leq \underbrace{\| \VEC{y}_0(t_i) - \VEC{w}_{0,i}
\|}_{\text{$=O(h^p)$ by (\ref{bigO})}} +
\underbrace{\| Y(t_i) - W_i
\|}_{\text{$=O(h^p)$ by (\ref{bigO})}}
\,\| \VEC{d} \| +
\underbrace{\| W_i \|}_{\text{bounded}}
\,\| \VEC{d} - \VEC{d}_T \|
\end{align*}
and
\begin{align*}
\| \VEC{d} - \VEC{d}_T \| &
= \| Q^{-1} \left( \VEC{y}_c - B_a \VEC{y}_0(a)
- B_b \VEC{y}_0(b)\right)
- Q_T^{-1} \left( \VEC{y}_c - B_a \VEC{w}_{0,0}
- B_b \VEC{w}_{0,N} \right) \| \\
&\leq \| Q^{-1} \| \bigg( \| B_a \| \,
\underbrace{\| \VEC{y}_0(a) - \VEC{w}_{0,0}
\|}_{\text{$=O(h^p)$ by (\ref{bigO})}}
+ \| B_b \| \,
\underbrace{\| \VEC{y}_0(b) - \VEC{w}_{0,N}
\|}_{\text{$=O(h^p)$ by (\ref{bigO})}} \bigg) \\
& \qquad + \underbrace{\| Q^{-1} - Q^{-1}_T
\|}_{\text{$=O(h^p)$ by (\ref{bigOO})}}
\,\underbrace{\| \VEC{y}_c - B_a \VEC{w}_{0,0} - B_b \VEC{w}_{0,N}
\|}_{\text{bounded}} \ ,
\end{align*}
we get
\[
\| \VEC{y}(t_i) - \VEC{w}_i \| = O(h^p) \ .
\]
This shows that the order of the shooting method is determined by the
order of the numerical methods used to solve the initial value
problems (\ref{partS}) and (\ref{genS}).

Obviously, in the previous discussion, we have ignored rounding errors.

\subsection{Separated and Partially Separated Boundary Conditions}
\label{SepPartSep}
  
For the boundary value problem (\ref{LinBVP}), we generally assumed that
\begin{equation} \label{RankCond}
\rank \begin{pmatrix} B_a & B_b \end{pmatrix} = n
\end{equation}
to get $n$ linearly independent boundary conditions.   This is a
necessary condition to get $Q$ invertible.

Suppose that $\rank B_b = q < n$.   There exists an \nn invertible
matrix $R_b$ (built from operations on the rows of $B_b$) such that
\[
R_b B_b = \begin{pmatrix} 0 \\ B_b^{[b]} \end{pmatrix} \ ,
\]
where $B_b^{[b]}$ is a \nm{q}{n} matrix of rank $q$.

We define
\[
\begin{pmatrix} \VEC{y}_{c}^{[a]} \\ \VEC{y}_{c}^{[b]} \end{pmatrix} = R_b
\VEC{y}_c \ ,
\]
where $\VEC{y}_{c}^{[b]} \in \RR^q$ and $\VEC{y}_{c}^{[a]} \in \RR^{n-q}$, and
\[
\begin{pmatrix} B_a^{[a]} \\ B_a^{[b]} \end{pmatrix} = R_b B_a \; ,
\]
where $B_a^{[a]}$ is an \nm{(n-q)}{n} matrix and $B_a^{[b]}$ is a \nm{q}{n}
matrix.  We have applied the operations on the rows of $B_b$ above to
the rows of $B_a$.   Note that $B_a^{[a]}$ is of rank $n-q$ because of
(\ref{RankCond}).

The boundary conditions in (\ref{LinBVP}) can then be rewritten as 
\begin{equation}\label{LinBVPpartSep}
\begin{split}
B_a^{[a]} \VEC{y}(a) &= \VEC{y}_{c}^{[a]} \\
B_a^{[b]} \VEC{y}(a) + B_b^{[b]} \VEC{y}(b) &= \VEC{y}_{c}^{[b]}
\end{split}
\end{equation}

The boundary conditions are
{\bfseries separable}\index{Boundary Conditions!Separable} if
$B_a^{[b]} = 0$ and
{\bfseries partially separable}\index{Boundary Conditions!Partially
Separable} if $B_a^{[b]} \neq 0$.

We now explain how to solve the boundary value problem (\ref{LinBVP}).
Let $D_a$ be a \nm{q}{n} matrix such that
\[
M_a = \begin{pmatrix} B_a^{[a]} \\ D_a \end{pmatrix}
\]
is invertible. This is possible because $B_a^{[a]}$ is of rank $n-q$ and
thus the rows of $B_a^{[a]}$ are linearly independent.  Let $F_a$ be the
\nm{n}{q} matrix defined by 
\[
M_a^{-1} = \begin{pmatrix} E_a & F_a \end{pmatrix} \ .
\]
Note that $F_a$ is of rank $q$ because $M_a$ is invertible.

To solve this problem, we may proceed as follows:

\begin{algo}
\begin{enumerate}
\item We solve the initial value problems
\begin{align}
P(\VEC{y}_0(t)) &= f(t) \quad , \quad a \leq t \leq b \nonumber \\
B_a^{[a]} \VEC{y}_0(a) &= \VEC{y}_{c}^{[a]} \label{FiYccond1}
\end{align}
and
\begin{align*}
P(\VEC{y}_j(t)) &= \VEC{0} \quad , \quad a \leq t \leq b \\
\VEC{y}_j(a) &= F_a\VEC{e}_j
\end{align*}
for $\VEC{e}_j \in \RR^q$ and $j=1$, $2$, \ldots, $q$.  Since $q < n$,
there are less initial value problems to solve.
\item The general solution $\VEC{y}_g:[a,b]\to \RR^n$ of the differential
equation in (\ref{LinBVP}) is of the form
\[
\VEC{y}_g(t) = \VEC{y}_0(t) + \sum_{j=1}^q d_j \VEC{y}_j(t) =
\VEC{y}_0(t) + V(t) \VEC{d} \ ,
\]
where
$\displaystyle V(t) = \begin{pmatrix}
\VEC{y}_1(t) & \VEC{y}_2(t) & \ldots & \VEC{y}_q(t) \end{pmatrix}$
and $\displaystyle \VEC{d} =
\begin{pmatrix} d_1 & d_2 & \ldots & d_q \end{pmatrix}^\top \in \RR^q$.
\item $\VEC{y}_g$ above will be a solution of the boundary value
problem (\ref{LinBVP}) if there exists $\VEC{d} \in \RR^q$
such that
\begin{equation} \label{RForm}
B_a^{[b]} \VEC{y}_g(a) + B_b^{[b]} \VEC{y}_g(b) = \VEC{y}_{c}^{[b]} \ .
\end{equation}
\end{enumerate}
\end{algo}

Note that
\[
B_a^{[a]} \VEC{y}_g(a) = B_a^{[a]} \VEC{y}_0(a) + B_a^{[a]} F_a \VEC{d}
 = B_a^{[a]} \VEC{y}_0(a) = \VEC{y}_{c}^{[a]} \ .
\]
The second equality in the previous equation comes from $B_a^{[a]} F_a=0$
because $M_a\,M_a^{-1} = \Id$ and the last equality comes from
(\ref{FiYccond1}).

Using the general form of $\VEC{y}_g$ and $V(a) = F_a \Id_q$, we get
from (\ref{RForm}) that
\begin{equation} \label{RFormExpand}
\left( B_a^{[b]} F_a + B_b^{[b]} V(b) \right)\VEC{d} = \VEC{y}_c^{[b]}
- B_a^{[b]} \VEC{y}_0(a) - B_b^{[b]} \VEC{y}_0(b) \ .
\end{equation}
Since $q<n$, we have a smaller system of linear equations to solve
than in the general shooting method.


Since $V(t) = Y(t) F_a$, where $Y$ is the fundamental solution
given in (\ref{genS}), the equation in (\ref{RFormExpand}) can be rewritten
\[
Q_b \VEC{d} = \VEC{y}_c^{[b]} - B_a^{[b]}\VEC{y}_0(a) - B_b^{[b]}\VEC{y}_0(b) \ ,
\]
where $Q_b = \left( B_a^{[b]} + B_b^{[b]} Y(b) \right) F_a$
because $V(b) = Y(b) F_a$.  The matrix $Q_b$
is an invertible \nm{q}{q} matrix if and only if
$Q = \tilde{B}_a + \tilde{B}_b Y(b)$
with $\displaystyle \tilde{B}_a = \begin{pmatrix} B_a^{[a]} \\ B_a^{[b]}
\end{pmatrix}$
and $\displaystyle \tilde{B}_b = \begin{pmatrix} 0 \\ B_b^{[b]}
\end{pmatrix}$ is invertible.  To prove the last sentence, it suffices
to note that
\[
Q M_a^{-1} = \begin{pmatrix} B_a^{[a]} \\ B_a^{[b]} + B_b^{[b]}Y(b) \end{pmatrix}
\begin{pmatrix} E_a & F_a \end{pmatrix}
= \begin{pmatrix} Id & 0 \\ * & Q_b \end{pmatrix}
\]
because $M_a M_a^{-1} = \Id$.


\begin{rmk}
Similarly, if $\rank B_a = p < n$, we can rewrite the boundary
conditions in (\ref{LinBVP}) as
\begin{align*}
B_a^{[a]} \VEC{y}(a) + B_b^{[a]} \VEC{y}(b) &= \VEC{y}_c^{[a]} \\
B_b^{[b]} \VEC{y}(b) &= \VEC{y}_c^{[b]}
\end{align*}
where $B_a^{[a]}$ is a \nm{p}{n} matrix of rank $p$, $B_b^{[b]}$ is a
\nm{(n-p)}{n} matrix of rank $n-p$, and $B_b^{[a]}$ is a \nm{p}{n}
matrix.

Obviously, there is also the alternative to reorder the coordinates
of $\VEC{y}$ to reduce this case to the previous case.
\end{rmk}

\subsection{Parallel Shooting for Linear Boundary Value Problems}
\label{SectParShootLinBVP}

The parallel shooting method that we present in this section and the
procedure presented in the next section to determine the $F_i$ and
$\VEC{y}_{c,i}$ used in the parallel shooting method are based on
\cite{CSD,K}. 

A potential serious issue with the simple shooting method is that
the solutions $\VEC{y}_j(t)$ given by (\ref{genS}) may become more and
more dependent as $t$ increases; namely, the matrix
$Y(t) = \begin{pmatrix}
\VEC{y}_1(t) & \VEC{y}_2(t) & \ldots & \VEC{y}_n(t) \end{pmatrix}$
may become more and more singular, and so ill-conditioned, as $t$
increases.  In particular, $Y(b)$ could be ill-conditioned.
Therefore, solving (\ref{BVcond}) may lead to serious numerical errors.

The {\bfseries first step to address the issue above} is to integrate
on shorter interval of time instead of the full interval $[a,b]$.
It is crucial to do so if the solution is rapidly increasing in some
regions of the interval $[a,b]$.  The basic idea is to apply the
shooting method on each interval  $[t_{i-1},t_i]$.

We consider the boundary value problem (\ref{LinBVP}).  As usual, we
let $\{ t_i \}_{i=0}^N$ be a partition of $[a,b]$
with $t_0 = a$, $t_N = b$ , $t_{i+1} = t_i + h_i$ with $h_i > 0$
for $0\leq i < N$ and
$\displaystyle h = \max_{0\leq i < N} h_i \leq \theta \min_{0\leq i < N} h_i$
for some constant $\theta$.

On each subinterval $[t_i,t_{i+1}]$, we solve the initial value
problems
\begin{equation} \label{parshootyci}
\begin{split}
P(\VEC{y}_{i,0}(t)) &= f(t) \quad , \quad t_i \leq t \leq t_{i+1} \\
\VEC{y}_{i,0}(t_i) &= \VEC{y}_{c,i}
\end{split}
\end{equation}
and
\begin{equation}\label{parshootFi}
\begin{split}
P(\VEC{y}_{i,j}(t)) &= \VEC{0} \quad , \quad t_i \leq t \leq t_{i+1} \\
\VEC{y}_{i,j}(t_i) &= F_i\VEC{e}_j
\end{split}
\end{equation}
for the canonical vectors $\VEC{e}_j \in \RR^n$ and $1 \leq j \leq n$.
The matrices $F_i$ of rank $n$ and the vector $\VEC{y}_{c,i}$ will be
defined in Section~\ref{Fiyci} below.

We look for a solution $\VEC{y}_g:[a,b]\to \RR^n$ of the boundary
value problem (\ref{LinBVP}) defined on each interval $[t_i,t_{i+1}]$ by
\[
\VEC{y}_g(t) = \VEC{y}_i(t) = \VEC{y}_{i,0}(t) + V_i(t) \VEC{d}_i
\quad , \quad t_i \leq t \leq t_{i+1} \ ,
\]
where $\displaystyle V_i(t) =
\begin{pmatrix}
\VEC{y}_{i,1}(t) & \VEC{y}_{i,2}(t) & \ldots & \VEC{y}_{i,n}(t)  
\end{pmatrix}$ for $t_i \leq t \leq t_{i+1}$ and $\VEC{d}_i \in \RR^n$.

To get a continuous solution $\VEC{y}$ at the points $t_i$ for
$0 < i < N$, we impose the condition
\[
\VEC{y}_i(t_i) = \VEC{y}_{i-1}(t_i) \quad , \quad 1 < i < N \ .
\]
Namely,
\begin{equation} \label{ContCond}
\VEC{y}_{c,i} + F_i\VEC{d}_i = \VEC{y}_{i-1,0}(t_i) +
V_{i-1}(t_i)\VEC{d}_{i-1} \quad , \quad 1 < i < N \ .
\end{equation}

Moreover, the boundary condition in (\ref{LinBVP}) gives
\begin{equation} \label{EndsCond}
B_a \left( \VEC{y}_{c,0} + F_0\VEC{d}_0 \right) + B_b \left(
\VEC{y}_{N-1,0}(b) + V_{N-1}(b)\VEC{d}_{N-1} \right) = \VEC{y}_c \ .
\end{equation}

We can combine (\ref{ContCond}) and (\ref{EndsCond}) to get the system
$A_S \VEC{d} = B_S$, where
\begin{align}
A_S &= \begin{pmatrix}
B_a F_0 & 0 & 0 & \ldots & 0 & B_b V_{N-1}(b)\\
-V_0(t_1) & F_1 & 0 & \ldots & 0 & 0 \\
0 & -V_1(t_2) & F_2 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & -V_{N-2}(t_{N-1}) & F_{N-1}
\end{pmatrix}
\ ,  \label{MFPSBVPa} \\
\VEC{d} &= \begin{pmatrix} \VEC{d}_0 \\ \VEC{d}_1 \\ \vdots \\
\VEC{d}_{N-1} \end{pmatrix}
\quad \text{and} \quad
B_S = \begin{pmatrix}
\VEC{y}_c - B_a \VEC{y}_{c,0} -B_b\VEC{y}_{N-1.0}(b) \\
\VEC{y}_{0,0}(t_1) - \VEC{y}_{c,1} \\
\vdots \\
\VEC{y}_{N-2,0}(t_{N-1}) - \VEC{y}_{c,N-1}
\end{pmatrix} \ . \label{MFPSBVPb}
\end{align}

\begin{rmkList} \label{ParShootPartSepRmk}
\begin{enumerate}
\item If the $F_i$ for $0 \leq i < N$ are invertible, the
parallel shooting method is equivalent to the simple shooting method.
In fact, we have
\[
A_S = \begin{pmatrix} Q_0 & Q_1 & Q_2 & \ldots & Q_{N-1} \\
0 & Id & 0 & \ldots & 0 \\
0 & 0 & Id & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & Id
\end{pmatrix}
\begin{pmatrix} F_0 & 0 & 0 & \ldots & 0 \\
-V_0(t_1) & F_1 & 0 & \ldots & 0 \\
0 & -V_1(t_2) & F_2 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & -V_{N-2}(t_{N-1}) & F_{N-1}
\end{pmatrix} \ ,
\]
where
\[
Q_i =
\begin{cases}
B_b V_i(b)F_i^{-1} & \quad \text{for} \quad i = N-1 \\
Q_{i+1} V_i(t_{i+1}) F_i^{-1} & \quad \text{for} \quad  i= N-2, N-3, \ldots, 1 \\
B_a + Q_{i+1} V_i(t_{i+1})F_i^{-1} & \quad \text{for} \quad i = 0
\end{cases}
\]
Since $V_i(t) = Y_i(t) F_i$ for $t_i \leq t \leq t_{i+1}$,
where $Y_i(t)$ is the fundamental solution of $P(\VEC{y}(t)) =\VEC{0}$
on $[t_i,t_{i+1}]$, in particular $Y_i(t_i) = \Id$, we have
that
\begin{align*}
Q_0 &= B_a + B_b V_{N-1}(t_N)F_{N-1}^{-1} V_{N-2}(t_{N-1})F_{N-2}^{-1}
\ldots V_0(t_1)F_0^{-1} \\
&= B_a + B_b Y_{N-1}(b) Y_{N-2}(t_{N-1}) \ldots Y_0(t_1) \\
&= B_a + B_b Y(b) = Q \ ,
\end{align*}
where $Y$ is the fundamental solution of
$P(\VEC{y}(t)) = \VEC{0}$ on $[a,b]$.  To get the second to last
equality in the previous equation, we have use the uniqueness of
solutions for initial value problems.

Thus $A_S$ is invertible if and only if $Q$ is invertible.
\item Note that the decomposition of $A_S$ above is an LU
decomposition of $A_S$ that may be used (with care) to solve
$A_S \VEC{d} = B_S$.
\item If we have separated or partially separated end-conditions, then
we may assume that the row operations (i.e.\ $R_b$ in
Section~\ref{SepPartSep}) have been performed to get
$\displaystyle \VEC{y}_c = \begin{pmatrix} \VEC{y}_c^{[a]} \\ \VEC{y}_c^{[b]}
\end{pmatrix}$, 
$\displaystyle B_a = \begin{pmatrix} B_a^{[a]} \\ B_a^{[b]}\end{pmatrix}$ and
$\displaystyle B_b = \begin{pmatrix} 0 \\ B_b^{[b]} \end{pmatrix}$.
The matrices $F_i$ in (\ref{parshootFi}) are now \nm{n}{q}
matrices of rank $q$.  We can then repeat the reasoning in this
section to get a system of linear equations $A_S \VEC{d} = B_S$ with $A_S$,
$B_S$ and $\VEC{d}$ defined by
\[
A_S = \begin{pmatrix}
B_a^{[a]} F_0 & 0 & 0 & \ldots & 0 & 0 \\
-V_0(t_1) & F_1 & 0 & \ldots & 0 & 0 \\
0 & -V_1(t_2) & F_2 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & -V_{N-2}(t_{N-1}) & F_{N-1} \\
B_a^{[b]} F_0 & 0 & 0 & \ldots & 0 & B_b^{[b]} V_{N-1}(b)
\end{pmatrix}
\quad , \quad
\VEC{d} = \begin{pmatrix} \VEC{d}_0 \\ \VEC{d}_1 \\ \vdots \\
\VEC{d}_{N-1} \end{pmatrix}
\]
and
\[
B_S = \begin{pmatrix}
\VEC{y}_c^{[a]} - B_a^{[a]} \VEC{y}_{c,0} \\
\VEC{y}_0(t_1) - \VEC{y}_{c,1} \\
\vdots \\
\VEC{y}_{N-2}(t_{N-1}) - \VEC{y}_{c,N-2}\\
\VEC{y}_{c}^{[b]} - B_a^{[b]} \VEC{y}_{c,0} - B_b^{[b]} \VEC{y}_{N-1,0}(b)
\end{pmatrix} \ .
\]
The first $n-q$ rows of $A_S$ above are the first $n-q$ rows of $A_S$
in (\ref{MFPSBVPa}), and the last $q$ rows of $A_S$ above are
the $q$ rows from the $(n-q+1)^{th}$ to the $n^{th}$ row inclusively
of $A_S$ in (\ref{MFPSBVPa}).  We have a similar statement for $B_S$
above and $B_S$ in (\ref{MFPSBVPb}).

Matrices like $A_S$ (i.e.\ lower or almost lower block triangular) often
appear in the numerically solution of systems of partial differential
equations.  This type of matrices has been extensively studied in
numerical analysis.  \label{ParShootPartSepRmkItem3}
\end{enumerate}
\end{rmkList}

\subsection{The Choice of $F_i$ and $\VEC{y}_{c,i}$}\label{Fiyci}

The simple shooting method is given by $F_i = \Id$ for $0 \leq i < N$,
$\VEC{y}_{c,i} = \VEC{0}$ for $1 \leq i < N$, and
$\VEC{y}_{c,0} = \VEC{y}_c$, where $\VEC{y}_c$ is defined in
(\ref{LinBVP}).  But this is not the one interesting us.

The {\bfseries second step to address the issue} mentioned at the
beginning of Section~\ref{SectParShootLinBVP} is to replace
(\ref{ContCond}) and (\ref{EndsCond}) by equations that no longer
involve the $V_{i-1}(t_i)$ but only matrices $F_{i-1}$ that have
orthonormal columns.

The {\bfseries last step to address the issue} mentioned at the
beginning of Section~\ref{SectParShootLinBVP} is to ensure that
$\VEC{y}_{c,i}$ is not in the range of $V_{i-1}(t_i)$ in order to
provided a transition from the integration on the interval
$[t_{i-1},t_i]$ to the interval $[t_i,t_{i+1}]$.  It is traditional to
take $\VEC{y}_{c,i}$ in the orthogonal complement of the column span
of $V_{i-1}(t_i)$.

We implement these two steps below.

From now on, we assume that the boundary conditions are partially
separated.

\begin{enumerate}
\item Let $F_0 = F_a$ and $\VEC{y}_{c,0} = \VEC{y}_0(a)$, where
$F_a$ and $\VEC{y}_0$ are defined in Section~\ref{SepPartSep}.
\item Suppose that we have determined $V_{i-1}(t_i)$.  The $q$ columns
of $F_i$ are the $q$ columns of $V_{i-1}(t_i)$ after they have
been orthonormalized.  Therefore, $F_i = V_{i-1}(t_i) P_{i-1}$ for
some \nm{q}{q} upper-triangular matrix $P_{i-1}$.  Usually, the
Gram-Schmidt method seen in linear algebra is used for this purpose.
\item $\VEC{y}_{c,i}$ is the projection of $\VEC{y}_{i-1,0}(t_i)$ on
the orthogonal complement of the column span of $F_i$.  Therefore,
$\VEC{y}_{c,i} = (\Id - F_iF_i^\top) \VEC{y}_{i-1,0}(t_i)$.
\end{enumerate}

We now explain how to compute the $\VEC{d}_i$ for $0\leq i < N$
that are used to define the $\VEC{y}_i$ of the parallel shooting method.

We rewrite (\ref{ContCond}) as 
\[
  \VEC{y}_{c,i} + F_i\VEC{d}_i = \VEC{y}_{i-1,0}(t_i) + V_{i-1}(t_i)
\VEC{d}_{i-1}
\]
for $i=N-1$, $N-2$, \ldots, $1$.  If we interpret this equation for
the shooting method with partially separated boundary conditions, we get
\[
\left(\Id - V_{i-1}(t_i)P_{i-1}F_i^\top \right)\VEC{y}_{i-1,0}(t_i) +
V_{i-1}(t_i)P_{i-1}\VEC{d}_i = \VEC{y}_{i-1,0}(t_i) + V_{i-1}(t_i)\VEC{d}_{i-1}
\]
for $i=N-1$, $N-2$, \ldots, $1$.  This equation can be simplified to
yield
\[
V_{i-1}(t_i)P_{i-1} \left( \VEC{d}_i - F_i^\top\VEC{y}_{i-1,0}(t_i)
\right) = V_{i-1}(t_i)\VEC{d}_{i-1}
\]
for $i=N-1$, $N-2$, \ldots, $1$.

Since $V_{i-1}(t_i)$ has rank $q$ because the columns of $V_{i-1}$
are $q$ linearly independent solutions of $P(\VEC{y}(t)) = \VEC{0}$
\footnote{We use the uniqueness of solutions for ordinary differential
equations to conclude that if $\displaystyle \{y_{i,j}(t)\}_{j=1}^q$
is a linearly independent set of solutions, then
$\displaystyle \{y_{i,j}(s)\}_{j=1}^q$ is a linear independent set of
vectors in $\RR^n$ for every $s\in [t_i,t_{i+1}]$.},
we can simplify the previous equation to get
\begin{equation} \label{defFy}
\VEC{d}_{i-1} = P_{i-1} \left( \VEC{d}_i - F_i^\top\VEC{y}_{i-1,0}(t_i)
\right) \in \RR^q
\end{equation}
for $j=N$, $N-1$, $N-2$, \ldots, $1$.  Note that we have extended
(\ref{defFy}) to $i = N$. the extra $F_N$ and $\VEC{y}_{c,N}$
are also given by the previous $3$-step procedure.

We first show that the condition
\begin{equation}\label{FiYccond2}
  B_a^{[a]} F_0 \VEC{d}_0 = \VEC{y}_c^{[a]} - B_a^{[a]} \VEC{y}_{c,0} 
\end{equation}
from the first $n-q$ rows of (\ref{EndsCond})
(Item~\ref{ParShootPartSepRmkItem3} of
Remark~\ref{ParShootPartSepRmk}) is automatically 
satisfied by construction.  We have that (\ref{FiYccond2}) is equivalent
to $B_a^{[a]} \VEC{y}_{c,0} = \VEC{y}_c^{[a]}$ because $F_0 = F_a$ and
$B_a^{[a]} F_a = 0$ since $M M^{-1} = \Id$.  Moreover, it follows from
(\ref{FiYccond1}) that $B_a^{[a]} \VEC{y}_{c,0} = \VEC{y}_c^{[a]}$ is
satisfied because we assume that $\VEC{y}_{c,0} = \VEC{y}_0(a)$.

We now consider the condition
\[
B_a^{[b]} F_0 \VEC{d}_0 +B_b^{[b]} V_{N-1}(b) \VEC{d}_{N-1}
= \VEC{y}_c^{[b]} - B_a^{[b]} \VEC{y}_{c,0} - B_b^{[b]} \VEC{y}_{N-1,0}(b) 
\]
from the last $q$ rows of (\ref{EndsCond})
(Item~\ref{ParShootPartSepRmkItem3} of
Remark~\ref{ParShootPartSepRmk}).  Using (\ref{defFy}) for $i=N$, we
get
\begin{align*}
&B_a^{[b]} F_0 \VEC{d}_0 +B_b^{[b]} \underbrace{V_{N-1}(b) P_{N-1}}_{=F_N} \left(
\VEC{d}_N - F_N^\top\VEC{y}_{N-1,0}(t_N)\right)
= \VEC{y}_c^{[b]} - B_a^{[b]} \VEC{y}_{c,0} - B_b^{[b]} \VEC{y}_{N-1,0}(b) \\
&\Rightarrow
B_a^{[b]} F_0 \VEC{d}_0 +B_b^{[b]} F_N \VEC{d}_N
- B_b^{[b]} F_NF_N^\top \VEC{y}_{N-1,0}(t_N)
= \VEC{y}_c^{[b]} - B_a^{[b]} \VEC{y}_{c,0} - B_b^{[b]} \VEC{y}_{N-1,0}(b) \\
&\Rightarrow
B_a^{[b]} F_0 \VEC{d}_0 +B_b^{[b]} F_N \VEC{d}_N
= \VEC{y}_c^{[b]} - B_a^{[b]} \VEC{y}_{c,0} - B_b^{[b]}
\underbrace{\left( \Id - F_NF_N^\top \right)
\VEC{y}_{N-1,0}(t_N)}_{= \VEC{y}_{c,N}}  \\
&\Rightarrow
B_a^{[b]} F_0\VEC{d}_0 + B_b^{[b]} F_N \VEC{d}_N = \VEC{y}_c^{[b]} -
B_a^{[b]} \VEC{y}_{c,0} - B_b^{[b]} \VEC{y}_{c,N}  \ .
\end{align*}

Therefore, the vectors $\VEC{d}_i$ are given by the system of linear
equations $A_S \VEC{d} = B_S$, where
\[
A_S = \begin{pmatrix}
-\Id & P_0 & 0 & \ldots & 0 & 0 \\
0 & -\Id & P_1 & \ldots & 0 & 0 \\
0 & 0 & -\Id & \ldots & 0 & 0 \\
 & & & \ddots & & \\
0 & 0 & 0 & \ldots & -\Id & P_{N-1} \\
B_a^{[b]} F_0 & 0 & 0 & \ldots & 0 & B_b^{[b]} F_N
\end{pmatrix}
\quad , \quad
\VEC{d} = \begin{pmatrix} \VEC{d}_0 \\ \VEC{d}_1 \\ \vdots \\
\VEC{d}_N \end{pmatrix}
\]
and
\[
B_S = \begin{pmatrix}
P_0 F_1^\top \VEC{y}_{0,0}(t_1) \\
\vdots \\
P_{N-1} F_N^\top \VEC{y}_{N-1,0}(t_N) \\
\VEC{y}_{c}^{[b]} - B_a^{[b]} \VEC{y}_{c,0} - B_b^{[b]} \VEC{y}_{c,N}
\end{pmatrix} \ .
\]

\begin{rmk}
The method proposed above could be improved.  Since orthonormalization
is costly in computation time and prone to numerical round off errors,
it may be preferable to perform it only when the matrix $V_{i-1}(t_i)$ is
``nearly singular'' only.  A method to determine if orthonormalization
is required is to test for the size of the angles between the column
vectors of $V_{i-1}(t_i)$.  If the angles get ``too close'' to $0$,
orthonormalization should be used.  Recall that the cosine of the
angle between two vectors $\VEC{a}$ and $\VEC{b}$ is determined by
$\ps{\VEC{a}}{\VEC{a}} / (\|\VEC{a}\|\,\|\VEC{b}\|)$.  Unfortunately,
even this method is kind of computer time intensive.

The method could also be improved by appropriately choosing the mesh
points $\{t_i\}_{i=0}^N$ such that $t_{i+1}-t_i$ is ``small'' when the
solution is ``rapidly'' increasing.  We select the mesh points as $i$
increases to ensure that $V_{i-1}(t_i)$ does not get too ``large.''
\end{rmk}

\begin{code}[Parallel Shooting Method for Linear Problems with
Partially Separated End-conditions] \label{parShootCode}
To approximate the solution of the boundary value problem
$y' - A(t) = f(f)$ with $B_a y(a) + B_b y(b) = y_c$  for $a \leq t \leq b$.
We consider the intervals $[t_i,t_{i+1}]$ for $0 \leq i < N$ with
$t_i = a + i H$ and $H = (b-a)/N$.  We use the classical fourth order
Runge-Kutta on each interval $[t_i,t_{i+1}]$ with the step size
$h = (t_i - t_{i+1})/M$ to solve initial value problems.\\
Let $t_{i,j} = t_i + j h$ for $0\leq i < N$ and $0 \leq j \leq M$.\\
\subI{Input} The vector valued function $f:[a,b] \to \RR^n$ (f in the
code below).\\
The \nn matrix valued function $A$ defined on $[a,b]$ (A in the code
below).\\
The \nm{(n-q)}{n} matrix $B_a^{[a]}$ (Baa in the code below).\\
The \nm{q}{n} matrix $B_a^{[b]}$ (Bab in the code below).\\
The \nm{q}{n} matrix $B_b^{[b]}$ (Bbb in the code below).\\
The (column) vector $y_c \in R^n$ (yc in the code below).\\
The number $N$ of partitions of $[a,b]$.\\
The number $M$ of partitions of each $[t_i,t_{i+1}]$.\\
The endpoints $a$ and $b$ of the interval of integration $[a,b]$.\\
\subI{Output} The \nm{n}{(M N +1)} matrix ww that contains the approximations
$w_{k,iM+j}$ of $y_k(t_{i,j})$ and the vector tt that contains
$t_{iM+j} = t_{i,j}$ for $1 \leq k \leq n$, $0 \leq i < N$, and
$0 \leq j < M$ if $i<N-1$ or $0 \leq j \leq M$ if $i=N-1$.  
\small
\begin{verbatim}
function [tt,ww] = par_shooting(f,A,Baa,Bab,Bbb,yc,M,N,a,b)
  funct1 = @(t,y) A(t)*y + f(t);
  funct2 = @(t,y) A(t)*y;
  n = length(yc);
  q = size(Bbb,1);
  nmq = n - q;
  ttt = repmat(NaN,M+1,N);
  WW1 = repmat(NaN,n,M+1,N);
  WWW = repmat(NaN,n,M+1,q,N);
  PPi = repmat(NaN,q,q,N+1);
  FFi = repmat(NaN,n,q,N+1);
  yci = repmat(NaN,n,N+1);
    
  % We choose the matrix D_a such that M_a is invertible
  Da = zeros(q,n);
  s = 1;
  for j=1:1:n
    v = zeros(1,n);
    v(j) = 1;
    MM = [Baa ; v];
    if ( rank(MM) > nmq )
      Da(s,:) = v;
      s = s +1;
    end
    if ( rank(Da) == q )
      break;
    end
  end

  % We find the matrix F_a
  Ma = [Baa ; Da];
  Mainv = inv(Ma);
  Fa = Mainv(:,(q+1):n);

  % We now compute the approximation of y_i(t_j) and
  % V_i(t_j) for 0 <= i < N and 0 \leq j \leq M, and the
  % F_i and R_i for 0 <= i <= N.
  % Warning: the indices i and j in matlab are shifted by 1
  %          because vectors start with the index 1.
  H = (b-a)/N;
  h = H/M; 
  ti = a;
  FFi(:,:,1) = Fa;

  % We solve M_a y_{c,0} = y_c instead of B_a^{[a]} y_{c,0} = y_c^{[a]}
  % to ensure that there is only one solution for Matlab to find. 
  yci(:,1) = linsolve(Ma,yc);

  for i=1:1:N+1
    %   Solve the initial value problem
    %   y'(t) - A(t) y(t) = f(t)  with  y_{i,0}(t_i) = y_{c,i}
    if ( i <= N )
      [t,ww1] = rgkt4(funct1,h,M,ti,yci(:,i));
      WW1(:,:,i) = ww1;
    end

    %   Solve the initial value problems
    %   y'(t) - A(t) y(t) = 0  with  y_{i,j}(t_i) = F_i e_j
    %   for 1 <= j <= q
    WW = repmat(NaN,n,M+1,q);
    for j=1:1:q
      yj = zeros(q,1);
      yj(j) = 1;
      y = FFi(:,:,i)*yj;
      [tt,ww2] = rgkt4(funct2,h,M,ti,y);
      WW(:,:,j) = ww2;
    end
    if ( i <= N )
      ttt(:,i) = tt;
      WWW(:,:,:,i) = WW;
    end

    % We choose F_{i+1} and Y_{c,i+1} for the next interval
    % The function par_QR is defined in the following code.
    % It is used to find F_i and P_{i-1}.
    Vi = squeeze(WW(:,M+1,:));
    [Fi,R] = par_QR(Vi);
    FFi(:,:,i+1) = Fi;
    PPi(:,:,i) = inv(R);
    if ( i <= N )
      yci(:,i+1) = (eye(n) - Fi*Fi')*ww1(:,M+1);
    end
    ti = ti + H;
  end

  % We now find the vector d_i for 0 <= i < N .
  qN = q*N;
  qNp1 = q*(N+1);
  As = zeros(qNp1,qNp1);
  Bs = zeros(qNp1,1);
  for i=1:1:N
    qi = q*i;
    qim1 = qi - q + 1;
    As(qim1:qi, qim1:qi) = - eye(q);
    As(qim1:qi, qi+1:qi+q) = squeeze(PPi(:,:,i));
    Bs(qim1:qi,1) = PPi(:,:,i)*(FFi(:,:,i+1)')*WW1(:,M+1,i);
  end
  As(qN+1:qNp1, 1:q) = Bab*FFi(:,:,1);
  As(qN+1:qNp1, qN+1:qNp1) = Bbb*FFi(:,:,N+1);
  Bs(qN+1:qNp1,1) = yc(nmq+1:n,1) - Bab*yci(:,1) - Bbb*yci(:,N+1);

  D = linsolve(As,Bs);

  % The results
  ww = [];
  tt = [];
  for i=1:1:N
    w = zeros(n,M+1);
    for j=1:1:q
      w =  w + D((i-1)*q+j)*squeeze(WWW(:,:,j,i));
    end
    if ( i < N )
      ww = [ww, WW1(:,1:M,i) + w(:,1:M)];
      tt = [tt, ttt(1:M,i)'];
    else
      ww = [ww, WW1(:,:,i) + w];
      tt = [tt, ttt(:,i)'];
    end
  end
end
\end{verbatim}
\end{code}

Finding the QR decomposition of a matrix is normally seen in a first
course on linear algebra.  We also presented it in
Section~\ref{C14L25}i of Chapter~\ref{chapEigVal}.

\begin{code}[QR Decomposition]
Find the QR decomposition of a matrix $A$: namely, $A=QR$ where the
columns of $Q$ are orthonormal and $R$ is upper triangular.   The
columns of $A$ must be linearly independent.\\
\subI{Input} The \nm{n}{q} matrix $A$.\\
\subI{Output} The matrices $Q$ and $R$ of the QR decomposition of $A$.
\small
\begin{verbatim}
% [Q,R] = par_QR(A)
%
function [Q,R] = par_QR(A)
  n = size(A,1);
  q = size(A,2);
  Q = repmat(NaN,n,q);
  R = zeros(q);

  if ( rank(A) < q )
    return;
  end

  R(1,1) = norm(A(:,1));
  Q(:,1) = (1/R(1,1))*A(:,1);
  for i = 2:1:q
      v = A(:,i);
      for j = 1:1:i-1
          R(j,i) = Q(:,j)'*A(:,i);
          v = v - R(j,i)*Q(:,j); 
      end
      R(i,i) = norm(v);
      Q(:,i) = (1/R(i,i))*v;
  end
end
\end{verbatim}
\end{code}

We now revisit the two examples that we have considered with our code
for the parallel shooting method.

\begin{egg}[Example~\ref{eggShootCode1} Continued]
If we use the Code~\ref{parShootCode} with $N = M = 10$, we get the
following approximations of the solution.
\[
\begin{array}{llllc@{\hspace{2em}}llll}
  i & t_i & w_{1,i} & w_{2,i} & & i & t_i & w_{1,i} & w_{2,i} \\
  \cline{1-4} \cline{6-9}
0 & 0 & 1 & 1 && 93 & 0.92 & 2.5092904 & 2.5092904 \\
1 & ).01 & 1.0100502 & 1.0100502 && 93 & 0.93 & 2.5345092 & 2.5345092 \\
2 & 0.02 & 1.0202013 & 1.0202013 && 94 & 0.94 & 2.5599814 & 2.5599814 \\
3 & 0.03 & 1.0304545 & 1.0304545 && 95 & 0.95 & 2.5857097 & 2.5857097 \\
4 & 0.04 & 1.0408108 & 1.0408108 && 96 & 0.96 & 2.6116965 & 2.6116965 \\
5 & 0.05 & 1.0512711 & 1.0512711 && 97 & 0.97 & 2.6379445 & 2.6379445 \\
6 & 0.06 & 1.0618365 & 1.0618365 && 98 & 0.98 & 2.6644562 & 2.6644562 \\
7 & 0.07 & 1.0725082 & 1.0725082 && 99 & 0.99 & 2.6912345 & 2.6912345 \\
8 & 0.08 & 1.0832871 & 1.0832871 && 100 & 1.00 & 2.7182818 & 2.7182818 \\
\vdots & \vdots & \vdots & \vdots && & & &
\end{array}
\]
where $w_{1,i} \approx y_{1,i} = y_1(t_i)$ and
$w_{2,i} \approx y_{2,i} = y_2(t_i)$ for all $i$.
All the approximations have at least $8$-digit accuracy.

For the sake of completeness, here is the code used to call the
parallel shooting method.
\begin{code}
\small
\begin{verbatim}
format long
f = @(t) [ 0 ; -3*exp(t) ];
A = @(t) [ 0 1 ; 4 0 ];
Baa = [ 1 0 ];
Bab = [ 0 0 ];
Bbb = [ 1 0 ];
yc = [ 1 ; exp(1) ];
N = 10;
M = 10; 
[t,w] = par_shooting(f,A,Baa,Bab,Bbb,yc,M,N,0,1)
\end{verbatim}
\end{code}
\end{egg}

\begin{egg}[Example~\ref{eggShootCode1} Continued]
If we use the Code~\ref{parShootCode} with $N = M = 10$, we get the
following approximations of the solution.
\[
\begin{array}{lllc@{\hspace{2em}}lllc@{\hspace{2em}}lll}
i & t_i & w_{1,i} & & i & t_i & w_{1,i} & & i & t_i & w_{1,i} \\ 
\cline{1-3} \cline{5-7} \cline{9-11}
0 & 0.00 & 1.00000000000 && 9 & 0.09 & 1.09417154921 && 92 & 0.92 & 2.47858567444 \\
1 & 0.01 & 1.01005016666 && 10 & 0.10 & 1.10516675001 && 93 & 0.93 & 2.50242773364 \\
2 & 0.02 & 1.02020133336 && 11 & 0.11 & 1.11627196757 && 94 & 0.94 & 2.52647679150 \\
3 & 0.03 & 1.03045450020 && 12 & 0.12 & 1.12748820742 && 95 & 0.95 & 2.55073431794 \\
4 & 0.04 & 1.04081066751 && 13 & 0.13 & 1.13881647619 && 96 & 0.96 & 2.57520179373 \\
5 & 0.05 & 1.05127083593 && 14 & 0.14 & 1.15025778172 && 97 & 0.97 & 2.59988071063 \\
6 & 0.06 & 1.06183600647 && 15 & 0.15 & 1.16181313314 && 98 & 0.98 & 2.62477257154 \\
7 & 0.07 & 1.07250718067 && 16 & 0.16 & 1.17348354101 && 99 & 0.99 & 2.64987889067 \\
8 & 0.08 & 1.08328536063 && \vdots & \vdots & \vdots && 100 & 1.00 & 2.67520119364
\end{array}
\]
where $w_{1,i} \approx y_i = y(t_i)$ for all $i$.
All the approximations have at least $10$-digit accuracy.
\end{egg}

\subsection{Shooting Method for Non-Linear Boundary Value
Problems}\label{simpleShootM}

We consider the boundary value problem
\begin{equation} \label{NLBVP}
\begin{split}
\VEC{y}'(t) &= f(t,\VEC{y}(t)) \quad , \quad a \leq t \leq b \\
g(\VEC{y}(a), \VEC{y}(b)) &= \VEC{0}
\end{split}
\end{equation}
This problem can be reformulated as follows.  Find $\VEC{s} \in \RR^n$
such that the solution $\VEC{u}(t,\VEC{s})$ of
\begin{equation} \label{RNLBVP}
\begin{split}
\pdydx{\VEC{u}}{t}(t,\VEC{s}) &= f(t, \VEC{u}(t,\VEC{s})) \quad , \quad
a \leq t \leq b \\
\VEC{u}(a,\VEC{s})&= \VEC{s}
\end{split}
\end{equation}
satisfies
\begin{equation} \label{Iter}
\phi(\VEC{s}) = g(\VEC{s},\VEC{u}(b,\VEC{s})) = \VEC{0} \ .
\end{equation}
We have reduced the problem to finding the roots of
$\phi(\VEC{s})$.  Hence, $\VEC{u}(t,\VEC{s})$ will be a solution
of (\ref{NLBVP}) if $\VEC{s}$ is a root of $\phi(\VEC{s})$.

The following theorem (assuming that $g$ in (\ref{NLBVP}) is also
sufficiently differentiable) will ensure that the solution of
(\ref{NLBVP}), if there is one, is sufficiently differentiable.
Moreover, the following theorem will also be used to justify
the use of the Newton Method to find a root of $\phi(\VEC{s})$.

\begin{theorem}
Suppose that $\VEC{y}_g$ is a solution of (\ref{NLBVP}) and that there
exist two positive constants $K$ and $\delta$ such that
\[
\| f(t,\VEC{v}) - f(t,\VEC{w}) \| \leq
K \| \VEC{v} - \VEC{w} \|
\]
for all $(t,\VEC{v}),(t,\VEC{w}) \in T_\delta(\VEC{y})$, where
\[
T_\delta(\VEC{y}) = \{ (t,\VEC{v}) \in \RR\times \RR^n : a \leq t \leq b
\text{ and } \| \VEC{y}(t) - \VEC{v} \| \leq \delta \} \ .
\]
(Figure~\ref{DomBVPNM})
If $\VEC{s} \in \{ \VEC{s}\in\RR^n : \| \VEC{y}(a) - \VEC{s}\|
\leq \delta e^{-K(b-a)} \}$, then there exists a unique solution
$\VEC{u}$ of (\ref{RNLBVP}).  Moreover, if
$f$ is continuously differentiable on $T_\delta(\VEC{y})$, then
$\displaystyle U(t,\VEC{s}) = \diff_{\VEC{s}} \VEC{u}(t,\VEC{s})$
exists for $a \leq t \leq b$ and is the fundamental solution of
\begin{equation} \label{EforFS}
U'(t) = \diff_{\VEC{y}} f(t,\VEC{u}(t,\VEC{s})) U(t) \quad , \quad
a \leq t \leq b \ .
\end{equation}
In particular, $U(a,\VEC{s}) = \Id$.
\label{smoothSol}
\end{theorem}

\pdfF{bound_value_probl/BVP_domain}{Domain to ensure the existence of
solutions for an initial value problem}{The domain $T_f(\VEC{y})$ of
Theorem~\ref{smoothSol} used to define conditions that will ensure
solutions of an initial value problem.  Note that
$S_\delta(\VEC{y}(c)) = \{ \VEC{v} : \|\VEC{v}-\VEC{y}(c)\| < \delta\}$.}
{DomBVPNM}

\begin{proof}
The domain $T_\delta(\VEC{y})$ is sketched in Figure~\ref{DomBVPNM}.
These are classical results of ordinary differential equations.
\end{proof}

The Newton Method applied to (\ref{Iter}) is
\begin{equation} \label{NewtonBVP}
  Q(\VEC{s}^{[j]}) \left( \VEC{s}^{[j+1]} - \VEC{s}^{[j]} \right)
  = -\phi(\VEC{s}^{[j]}) \quad , \quad j \geq 0 \ ,
\end{equation}
where
\[
Q(\VEC{s}) = \diff_{\VEC{s}} \phi(\VEC{s})
= \diff_{\VEC{y}_1}g(\VEC{s},\VEC{u}(b,\VEC{s}))
+ \diff_{\VEC{y}_2}g(\VEC{s},\VEC{u}(b,\VEC{s})) U(b,\VEC{s})
\]
for
\begin{align*}
g:\RR^n \times \RR^n &\to \RR^n \\
(\VEC{y}_1,\VEC{y}_2) &\mapsto g(\VEC{y}_1,\VEC{y}_2)
\end{align*}

The following theorem justifies the use of the Newton Method to find a
root of $\phi$.  It also gives a (not that useful) hint
on how to choose $\VEC{s}_0$.

\begin{theorem}
Suppose that $\phi$ has an isolate root $\VEC{s}_\ast$
and that there exist $\rho_\ast>0$, $\beta$ and $\gamma$ such that
\begin{align*}
\| Q^{-1}(\VEC{s}_\ast) \| &< \beta \ , \\
\| Q(\VEC{s}) - Q(\tilde{\VEC{s}}) \| &\leq \gamma
\| \VEC{s} - \tilde{\VEC{s}}\|
\end{align*}
for all $\VEC{s}, \tilde{\VEC{s}} \in S_{\rho_\ast}(\VEC{s}_\ast) =
\{ \VEC{s} : \| \VEC{s} - \VEC{s}_\ast \| \leq \rho_\ast \}$,
and $\displaystyle \rho_\ast \beta \gamma < \frac{2}{3}$.

Then, for all $\VEC{s}^{[0]} \in S_{\rho_\ast}(\VEC{s}_\ast)$, the
sequence $\{\VEC{s}^{[j]}\}_{j=0}^\infty$ given by the iterative method
defined in (\ref{NewtonBVP}) stays in $S_{\rho_\ast}(\VEC{s}_\ast)$
and converge to $\VEC{s}_\ast$.  The convergence is quadratic; namely,
\begin{equation} \label{contract}
\| \VEC{s}^{[j+1]} - \VEC{s}_\ast \| \leq \alpha \|
\VEC{s}^{[j]} - \VEC{s}_\ast \|^2 \ ,
\end{equation}
where $\displaystyle \alpha = \frac{\beta\gamma}{2(1-\rho_\ast \beta\gamma)}$.
\label{CforNewton}
\end{theorem}

\begin{proof}
For all $\VEC{s}$,
\[
Q(\VEC{s}) = Q(\VEC{s}_\ast) \left( \Id - Q^{-1}(\VEC{s}_\ast) \left(
Q(\VEC{s}_\ast) - Q(\VEC{s}) \right) \right) \ .
\]
Since
\[
\| Q^{-1}(\VEC{s}_\ast) \left( Q(\VEC{s}) - Q(\VEC{s}_\ast)
\right) \| \leq \| Q^{-1}(\VEC{s}_\ast) \| \,
\| Q(\VEC{s}_\ast) - Q(\VEC{s}) \|  \leq \beta\gamma
\| \VEC{s} - \VEC{s}_\ast \| \leq \beta\gamma\rho_\ast
< \frac{2}{3} < 1
\]
for all $\VEC{s} \in S_{\rho_\ast}(\VEC{s}_\ast)$, it follows from
the Banach Lemma (Proposition~\ref{Banach} and
Corollary~\ref{Banach_cor}) that
$\Id - Q^{-1}(\VEC{s}_\ast) \left( Q(\VEC{s}_\ast) - Q(\VEC{s}) \right)$
is invertible for all
$\VEC{s} \in S_{\rho_\ast}(\VEC{s}_\ast)$.  Thus $Q(\VEC{s})$ is
invertible and
\[
\| Q^{-1}(\VEC{s}) \| <
\| \left( \Id - Q^{-1}(\VEC{s}_\ast) \left(
Q(\VEC{s}_\ast) - Q(\VEC{s}) \right) \right)^{-1} \| \,
\| Q^{-1}(\VEC{s}_\ast) \| \leq
\frac{\beta}{1-\rho_\ast \beta\gamma}
\]
for all $\VEC{s} \in S_{\rho_\ast}(\VEC{s}_\ast)$.

We prove that $\VEC{s}^{[j]} \in S_{\rho_\ast}(\VEC{s}_\ast)$ for all $j$
by induction.  We have that $\VEC{s}^{[0]} \in S_{\rho_\ast}(\VEC{s}_\ast)$.
We assume that $\VEC{s}^{[j]} \in S_{\rho_\ast}(\VEC{s}_\ast)$ and show
that this implies that $\VEC{s}^{[j+1]} \in S_{\rho_\ast}(\VEC{s}_\ast)$.
Since $\phi(\VEC{s}_\ast) = \VEC{0}$ and $Q^{-1}(\VEC{s})$
exists for all $\VEC{s} \in S_{\rho_\ast}(\VEC{s}_\ast)$, we get
from (\ref{NewtonBVP}) that
\begin{align*}
\VEC{s}^{[j+1]} - \VEC{s}_\ast &= \left( \VEC{s}^{[j]} - \VEC{s}_\ast \right)
+ Q^{-1}(\VEC{s}^{[j]}) \left( \phi(\VEC{s}_\ast) -
\phi(\VEC{s}^{[j]})\right) \\
&= Q^{-1}(\VEC{s}^{[j]}) \left( Q(\VEC{s}^{[j]}) -
Q(\VEC{s}_\ast, \VEC{s}^{[j]}) \right) \left( \VEC{s}^{[j]}
- \VEC{s}_\ast \right)  \ ,
\end{align*}
where
\[
Q(\VEC{s},\tilde{\VEC{s}}) = \int_0^1 \;
Q(\theta \VEC{s} + (1-\theta) \tilde{\VEC{s}}) \dx{\theta} \ .
\]
Note that
\[
Q(\theta \VEC{s} + (1-\theta)\tilde{\VEC{s}})
\left(\VEC{s} - \tilde{\VEC{s}} \right) =
\dfdx{\phi(\theta \VEC{s} + (1-\theta)\tilde{\VEC{s}})}{\theta}
\]
since $\displaystyle Q(\VEC{s}) = \diff_{\VEC{s}}\phi(\VEC{s})$.
Hence,
\begin{align}
\| \VEC{s}^{[j+1]} - \VEC{s}_\ast \|
& \leq \| Q^{-1}(\VEC{s}^{[j]}) \|\,
\| Q(\VEC{s}^{[j]}) - Q(\VEC{s}_\ast,\VEC{s}^{[j]}) \| \,
\| \VEC{s}^{[j]} - \VEC{s}_\ast \| \nonumber \\
&\leq
\underbrace{\left(\frac{\beta}{1-\rho_\ast \beta\gamma}\right)
\frac{\gamma}{2}}_{=\alpha}
\| \VEC{s}^{[j]} - \VEC{s}_\ast \|^2 \ , \label{Pcontract}
\end{align}
where the last inequality comes from
\begin{align}
\| Q(\VEC{s}^{[j]}) - Q(\VEC{s}_\ast,\VEC{s}^{[j]}) \|
&= \left\| \int_0^1 \left( Q(\VEC{s}^{[j]}) - 
Q(\theta \VEC{s}_\ast + (1-\theta)\VEC{s}^{[j]}) \right) \dx{\theta} \right\|
\nonumber \\
& \leq \int_0^1 \left\| Q(\VEC{s}^{[j]}) - 
Q(\theta \VEC{s}_\ast + (1-\theta)\VEC{s}^{[j]}) \right\| \dx{\theta}
\nonumber \\
& \leq \gamma \int_0^1 \left\| \VEC{s}^{[j]} - 
\theta \VEC{s}_\ast - (1-\theta)\VEC{s}^{[j]}) \right\| \dx{\theta}
\nonumber \\
& = \gamma  \left\|-\VEC{s}_\ast +\VEC{s}^{[j]} \right\|
\int_0^1 \theta  \dx{\theta}
= \frac{\gamma}{2}  \left\|-\VEC{s}_\ast +\VEC{s}^{[j]} \right\| \ .
\label{CforNewtonInt}
\end{align}

It follows from (\ref{Pcontract}) that
\[
\| \VEC{s}^{[j+1]} - \VEC{s}_\ast \|
\leq \alpha \rho_\ast^2 < \rho_\ast
\]
because $\alpha < 3\beta\gamma/2 < 1/\rho_\ast$
which is a consequence of $\rho_\ast \beta \gamma < 2/3$.
Thus $\VEC{s}^{[j]} \in S_{\rho_\ast}(\VEC{s}_\ast)$ for all $j$ by
induction.

Since $S_{\rho_\ast}(\VEC{s}_\ast)$ is complete, there is a subsequence of
$\{\VEC{s}^{[j]}\}_{j=0}^\infty$ that converges.  However, we also
have from (\ref{Pcontract}) that
\begin{equation} \label{CforNewtonFP}
\| \VEC{s}^{[j+1]} - \VEC{s}_\ast \|
\leq
\underbrace{\left(\frac{\rho_\ast \beta \gamma}{2(1-\rho_\ast \beta\gamma)}
\right)}_{<1} \| \VEC{s}^{[j]} - \VEC{s}_\ast \|
\end{equation}
again because $\rho_\ast \beta \gamma < 2/3$.  As we have done in the
proof of the Fixed Point Theorem, Theorem~\ref{FxPtTh}, we can show
that $\{\VEC{s}^{[j]}\}_{j=0}^\infty$ converges to $\VEC{s}_\ast$.

Finally, we also have from (\ref{Pcontract}) that
(\ref{contract}) is satisfied.
\end{proof}

\begin{rmk}
To compute $\VEC{s}^{[j+1]}$, we must solve (\ref{RNLBVP}) and compute the
fundamental solution of (\ref{EforFS}), where $\VEC{s}$ is replaced by
$\VEC{s}^{[j]}$.
\end{rmk}

\subsection{Error Analysis}

Numerical computations are never exact.  We now consider the
effect of truncation (e.g.\ in numerical integration) on the Newton
Method (\ref{NewtonBVP}).  To simplify the discussion, we assume that
there is no round off error which, in practice, is not negligible.
Because of truncation, instead of (\ref{NewtonBVP}), we actually
compute
\begin{equation} \label{NBVPtrunc}
Q(\tilde{\VEC{s}}^{[j]}) \left( \tilde{\VEC{s}}^{[j+1]}
- \tilde{\VEC{s}}^{[j]}\right) =
- \phi(\tilde{\VEC{s}}^{[j]}) + \delta_{j+1}(h)
\end{equation}
for  $j \geq 0$, where $h$ is the maximum step size of the partition
of $[a,b]$ for the converging and stable numerical method used to
solve the differential equation.  We assume that, for all $j$,
$\| \delta_j(h) \| \leq M h^p$ for some constant $M$ and positive
integer $p$.

\begin{theorem}
Suppose that the hypothesis of Theorem~\ref{CforNewton} are
satisfied with $\rho_\ast$ replaced by
$\tilde{\rho} = \rho_\ast + \delta_\ast$ and
$\rho_\ast\beta\gamma < 2/3$ replaced by $\tilde{\rho}\beta\gamma < 1/2$.
Suppose that $\theta \in ]0,1[$ satisfies
\begin{equation} \label{errorA}
2\gamma M h^p \left( \frac{\beta\gamma}{1-2\tilde{\rho}\beta\gamma}\right)^2
\leq \theta 
\end{equation}
and
\begin{equation} \label{errorB}
\sigma \equiv \frac{1}{1+\sqrt{1-\theta}}\left(
\frac{2\beta M h^p}{1-2\tilde{\rho}\beta\gamma} \right)
\leq \delta_\ast \ . 
\end{equation}
Then, if $\VEC{s}^{[0]} \in S_{\rho_\ast}(\VEC{s}_\ast)$ satisfies
$\| \VEC{s}^{[1]} - \VEC{s}^{[0]} \| \leq \rho_\ast$, the
sequences $\{ \VEC{s}^{[j]} \}_{j=0}^\infty$ of (\ref{NewtonBVP}) and
$\{ \tilde{\VEC{s}}^{[j]} \}_{j=0}^\infty$ of (\ref{NBVPtrunc}) with
$\tilde{\VEC{s}}^{[0]} = \VEC{s}^{[0]}$ satisfy
\[
\| \VEC{s}^{[j]} - \tilde{\VEC{s}}^{[j]} \|  \leq \sigma
\quad \text{and} \quad
\| \tilde{\VEC{s}}^{[j]} - \VEC{s}_\ast \| \leq
\frac{1}{\alpha} \left( \alpha
\| \VEC{s}^{[0]} - \VEC{s}_\ast \|  \right)^{2^j} + \sigma
\]
for all $j\geq 0$, where $\alpha$ is defined in the statement of
Theorem~\ref{CforNewton}.
\end{theorem}

\begin{proof}
As in Theorem~\ref{CforNewton}, we can show that $Q(\VEC{s})$ is
invertible and
\[
\| Q^{-1}(\VEC{s}) \| \leq
\frac{\beta}{1-\tilde{\rho}\beta\gamma}
\]
for all $\VEC{s} \in S_{\tilde{\rho}}(\VEC{s}_\ast)$.  Moreover, as in
Theorem~\ref{CforNewton}, we can show that
$\VEC{s}^{[j]} \in S_{\rho_\ast}(\VEC{s}_\ast)$ and
\begin{equation}\label{succs}
\| \VEC{s}^{[j+1]} - \VEC{s}^{[j]} \| \leq \rho_\ast
\end{equation}
for $j\geq 0$ if $\VEC{s}_0 \in S_{\rho_\ast}(\VEC{s}_\ast)$
because $\rho_\ast \beta \gamma \leq \tilde{\rho} \beta \gamma < 1/2 < 2/3$.
In particular, (\ref{succs}) follows from the hypothesis that
$\| \VEC{s}^{[1]} - \VEC{s}^{[0]} \| \leq \rho_\ast$ and
\[
\| \VEC{s}^{[j+1]} - \VEC{s}^{[j]}\| \leq \lambda \|
\VEC{s}^{[j]} - \VEC{s}^{[j-1]} \|
\]
with $\displaystyle \lambda =
\frac{\rho_\ast \beta \gamma}{2(1-\rho_\ast \beta\gamma)} < 1$
that can be proved as (\ref{CforNewtonFP}) was proved.

Let $\VEC{r}^{[j]} = \VEC{s}^{[j]} - \tilde{\VEC{s}}^{[j]}$.   If we subtract
(\ref{NBVPtrunc}) from (\ref{NewtonBVP}), we get
\begin{equation} \label{smx}
\begin{split}
Q(\tilde{\VEC{s}}^{[j]})\,\VEC{r}^{[j+1]} &= \left( Q(\tilde{\VEC{s}}^{[j]}) -
Q(\VEC{s}^{[j]},\tilde{\VEC{s}}^{[j]}) \right) \VEC{r}^{[j]} \\
&\qquad + \left( Q(\tilde{\VEC{s}}^{[j]}) -Q(\VEC{s}^{[j]}) \right)
\left( \VEC{s}^{[j+1]} - \VEC{s}^{[j]} \right) - \Delta_j(h) \ .
\end{split}
\end{equation}

\stage{A} If $\tilde{\VEC{s}}^{[j]} \in S_{\tilde{\rho}}(\VEC{s}_\ast)$,
we show that
$\| \VEC{r}^{[j]} \| < \sigma$ and
$\tilde{\VEC{s}}^{[j+1]} \in S_{\tilde{\rho}}(\VEC{s}_\ast)$.
From (\ref{smx}), we get
\begin{align}
\| \VEC{r}^{[j+1]} \| & \leq
\| Q^{-1}(\tilde{\VEC{s}}^{[j]}) \|
\bigg( \underbrace{\| Q(\tilde{\VEC{s}}^{[j]}) -
Q(\VEC{s}^{[j]},\tilde{\VEC{s}}^{[j]}) \|}_{\leq 
(\gamma/2) \| \VEC{r}^{[j]}\| \text{ as in (\ref{CforNewtonInt})}}
\| \VEC{r}^{[j]} \| \nonumber \\
& \qquad \qquad +
\underbrace{\| Q(\tilde{\VEC{s}}^{[j]}) -Q(\VEC{s}^{[j]}) \|}_{
\substack{\leq \gamma \| \VEC{r}^{[j]}\| \text{ by Hypothesis of}\\
\text{Theorem~\ref{CforNewton} with}\\
\text{$\rho_\ast$ replaced by $\tilde{\rho}$}}}
\quad \underbrace{\| \VEC{s}^{[j+1]} - \VEC{s}^{[j]} \|}_{\leq
\tilde{\rho} \text{ from (\ref{succs})}}
+ \underbrace{\| \delta_{j+1}(h) \|}_{\leq Mh^p} \bigg)\nonumber \\
& = \frac{\beta}{1-\tilde{\rho}\beta\gamma} \left
( \frac{\gamma}{2}\,\| \VEC{r}^{[j]}\|^2 + \gamma \tilde{\rho} \|
\VEC{r}^{[j]} \| - \frac{1-\tilde{\rho}\beta\gamma}{\beta} \| \VEC{r}^{[j]} \|
+ Mh^p \right) + \| \VEC{r}^{[j]} \| \nonumber \\
&= q(\| \VEC{r}^{[j]} \|) + \| \VEC{r}^{[j]} \| \ ,  \label{rjp1srj}
\end{align}
where
\[
q(z) = \frac{\beta}{1-\tilde{\rho}\beta\gamma}
\left( \frac{\gamma z^2}{2} - \big
( \frac{1-2\tilde{\rho}\beta\gamma}{\beta} \big) z + Mh^p \right)
\]
because
\[
  \gamma\tilde{\rho} - \frac{1-\tilde{\rho} \beta\gamma}{\beta}
  = \frac{2\tilde{\rho}\beta\gamma -1}{\beta} \ .
\]

Since $((1-2\tilde{\rho}\beta\gamma)/\beta)^2 - 2\gamma Mh^p > 0$
from (\ref{errorA}), and $(1-2\tilde{\rho}\beta\gamma)/\beta > 0$ because
$\tilde{\rho}\beta\gamma < 1/2$,  the quadratic polynomial $q(z)$ has two
positive roots $z_+ > z_-$.
We show by induction that $\| \VEC{r}^{[j]} \| \leq z_-$
for all $j$.  The result is true for $j=0$ because
$\| \VEC{r}^{[0]} \| = 0$.  Suppose that
$\| \VEC{r}^{[j]}\| \leq z_-$.  Since
\[
  q'(z) = \frac{\beta}{1-\tilde{\rho}\beta\gamma} \left(
\gamma z - \big( \frac{1-2\tilde{\rho}\beta\gamma}{\beta} \big)
\right) \ ,
\]
we have that
\[
-1 < -\frac{1-2\tilde{\rho}\beta\gamma}{1-\tilde{\rho}\beta\gamma}
= q'(0) \leq q'(z) \leq q'(z_-) < 0
\]
for $0 \leq z \leq z_-$ and $q$ is concave up.  We get the following
figure
\pdfbox{bound_value_probl/error_anl}
It follows that $z + q(z) < z_-$ for $0 \leq
z \leq z_-$.  Therefore $\| \VEC{r}^{[j+1]} \| \leq z_-$ from
(\ref{rjp1srj}).  This completes the proof by induction.

We now show that $z_- \leq \sigma$.  Since $z_+ z_- = 2Mh^p/\gamma$ and
\[
z_+ = \frac{1-2\tilde{\rho}\beta\gamma}{\beta \gamma}
\left( 1 + \sqrt{1 -
2\gamma M h^p\left(\frac{\beta}{1-2\tilde{\rho}\beta\gamma}\right)^2} \right)
\geq \frac{1-2\tilde{\rho}\beta\gamma}{\beta \gamma}
\left( 1 + \sqrt{1 - \theta}\right) > 0
\]
because
\[
  2\gamma M h^p\left(\frac{\beta}{1-2\tilde{\rho}\beta\gamma}\right)^2 \leq
\theta
\]
according to (\ref{errorA}), it follows that
\[
z_- = \frac{2Mh^p}{\gamma z_+} \leq \frac{2Mh^p}{1+\sqrt{1-\theta}} \,
\left( \frac{\beta}{1-2\tilde{\rho}\beta\gamma}\right) = \sigma \ .
\]

Finally,
\begin{align*}
\| \tilde{\VEC{s}}^{[j+1]} - \VEC{s}_\ast\|  &\leq
\| \tilde{\VEC{s}}^{[j+1]} - \VEC{s}^{[j+1]} \| +
\| \VEC{s}^{[j+1]} - \VEC{s}_\ast \|
= \| \VEC{r}^{[j+1]} \| +
\| \VEC{s}^{[j+1]} - \VEC{s}_\ast \| \\
&\leq \sigma + \rho_\ast \leq \delta_\ast + \rho_\ast = \tilde{\rho} \ ,
\end{align*}
where the last inequality comes from the
hypothesis $\sigma < \delta_\ast$.  Thus,
$\tilde{\VEC{s}}^{[j+1]} \in S_{\rho}(\VEC{s}_\ast)$.

\stage{B} By induction, we get from (\ref{contract}) that
\[
  \| \VEC{s}^{[j]} - \VEC{s}_\ast \| \leq
  \alpha^{2^j-1}\|\VEC{s}^{[0]}- \VEC{s}_\ast\|^{2^j} \ ,
\]
where
$\displaystyle \alpha = \frac{\beta\gamma}{2(1-\rho_\ast \beta\gamma)}$.
Hence,
\[
\| \tilde{\VEC{s}}^{[j]} - \VEC{s}_\ast \| \leq
\| \tilde{\VEC{s}}^{[j]} - \VEC{s}^{[j]} \| +
\| \VEC{s}^{[j]} - \VEC{s}_\ast \|
\leq \delta + \alpha^{2^j-1} \| \VEC{s}^{[0]} - \VEC{s}_\ast \|^{2^j}
\leq \delta + \frac{1}{\alpha}
\left( \alpha \| \VEC{s}^{[0]}-\VEC{s}_\ast  \| \right)^{2^j} \ .  \qedhere
\]
\end{proof}

It follows from the previous theorem that the accuracy of the
approximation $\displaystyle \tilde{\VEC{s}}^{[j]}$ of $\VEC{s}_\ast$
is limited by $\sigma$.  Moreover, recall that
$\alpha \left\|\VEC{s}^{[0]} - \VEC{s}_\ast\right\|
\leq \alpha \rho_\ast < 2/3 < 1$.  Thus, the error
$\displaystyle \left\| \tilde{\VEC{s}}^{[j]} - \VEC{s}_\ast \right\|$
does not grow as $j$ increases.

\subsection{Parallel Shooting for Non-Linear Boundary Value Problems}

As usual, let $\{t_i\}_{i=0}^N$ be a partition of $[a,b]$ such that
$t_0 = a$, $t_N = b$ , $t_{i+1} = t_i + h_i$ with $h_i > 0$
for $0\leq i < N$ and
$\displaystyle h = \max_{0\leq i < N} h_i \leq \theta \min_{0\leq i < N} h_i$
for some constant $\theta$.

Parallel Shooting Method applied to the boundary value problem
(\ref{NLBVP}) can be summarized as follows.  Solve the initial value
problems
\begin{align*}
\VEC{y}_i'(t, \VEC{s}_i) &= f(t,\VEC{y}_i(t,\VEC{s}_i))
\quad , \quad t_i \leq t \leq t_{i+1} \\
\VEC{y}_i(t_i,\VEC{s}_i) &= \VEC{s}_i
\end{align*}
for $0\leq i < N$, where the initial conditions $\VEC{s}_i$ are
such that the function $\VEC{y}:[a,b]\to \RR^n$ defined by
\[
\VEC{y}_g(t)= \VEC{y}_i(t,\VEC{s}_i) \quad , \quad t_i \leq t \leq t_{i+1}
\]
is a solution of the differential equation in (\ref{NLBVP}) satisfying
\[
\phi(\VEC{s}) \equiv \begin{pmatrix}
g(\VEC{s}_0,\VEC{y}_{N-1}(b,\VEC{s}_{N-1})) \\
\VEC{s}_1 - \VEC{y}_0(t_1,\VEC{s}_0) \\
\vdots \\
\VEC{s}_{N-1} - \VEC{y}_{N-2}(t_{N-1},\VEC{s}_{N-2})
\end{pmatrix}
= \VEC{0} \ , \quad \text{where} \quad 
\VEC{s} = \begin{pmatrix}
\VEC{s}_0 \\ \VEC{s}_1 \\ \vdots \\ \VEC{s}_{N-1} \end{pmatrix} \ .
\]
The first $n$ equations in $\phi(\VEC{s}) = \VEC{0}$ are the boundary
conditions and the other equations are to ensure that we get a
continuous (and differentiable) solution at $t_i$ for $1\leq i \leq N-1$.

The Parallel Shooting Method can be rewritten as a simple Shooting
Method.  Let
\begin{align*}
\VEC{z}_i(\tau) &= \VEC{y}((t_{i-1} + \tau (t_i - t_{i-1})) \quad
\text{for} \quad 1 \leq i \leq N \ , \\ 
\VEC{z}(\tau) &= \begin{pmatrix}
\VEC{z}_i(\tau) \\ \VEC{z}_2(\tau) \\ \vdots \\ \VEC{z}_N(\tau)
\end{pmatrix} \quad , \quad
F(\tau,\VEC{z}(\tau)) = \begin{pmatrix}
(t_1 - t_0 ) f(t_0+\tau (t_1-t_0),\VEC{z}_1(\tau)) \\
(t_2 - t_1 ) f(t_1+\tau (t_2-t_1),\VEC{z}_2(\tau)) \\
\vdots \\
(t_N - t_{N-1} ) f(t_{N-1}+\tau (t_N-t_{N-1}),\VEC{z}_N(\tau))
\end{pmatrix} \\
& \text{and}  \quad G(\VEC{v},\VEC{w}) = \begin{pmatrix}
g(\VEC{v}_1,\VEC{w}_N) \\
\VEC{v}_2 - \VEC{w}_1 \\
\vdots \\
\VEC{v}_N - \VEC{w}_{N-1}
\end{pmatrix}
\end{align*}
for $0 \leq \tau \leq 1$ and
$\VEC{v}, \VEC{w} \in (\RR^n)^N \cong \RR^{nN}$.

The boundary value problem (\ref{NLBVP}) can be rewritten as
\begin{equation}\label{NLBVPinnN}
\begin{split}
\VEC{z}'(\tau) &= F(\tau,\VEC{z}(\tau)) \quad , \quad 0 \leq \tau
\leq 1 \\
G(\VEC{z}(0), \VEC{z}(1)) &= \VEC{0}
\end{split}
\end{equation}
We get the Shooting Method
\begin{equation} \label{LargeShoot}
\begin{split}
\VEC{u}'(\tau,\VEC{s}) &= F(\tau,\VEC{u}(\tau,\VEC{s}) \quad ,
\quad 0 \leq \tau \leq 1 \\
\VEC{u}(0,\VEC{s}) &= \VEC{s}
\end{split}
\end{equation}
where $\VEC{s} \in \RR^{nN}$ is a solution of
\begin{equation} \label{LargeIter}
\phi(\VEC{s}) \equiv G(\VEC{s},\VEC{u}(1,\VEC{s})) = \VEC{0}
\end{equation}
and
\[
u(\tau,\VEC{s}) = \begin{pmatrix}
u_1(\tau,\VEC{s}_1) \\
u_2(\tau,\VEC{s}_2) \\
\vdots \\
u_N(\tau,\VEC{s}_N
\end{pmatrix} \ .
\]

\begin{theorem}
Suppose that $t_i - t_{i-1} = h > 0$ for $1\leq i \leq N$,
and that the hypothesis of Theorem~\ref{smoothSol} are satisfied
(with $\VEC{y}$ replaced by $\VEC{z}$ and (\ref{NLBVP}) replaced by 
\ref{NLBVPinnN}).  Then, there exists a unique solution of
(\ref{LargeShoot}) for any
\[
\VEC{s} \in \left\{ \VEC{s} \in (\RR^n)^N \cong \RR^{nN} :
\| \VEC{s} - \VEC{z}(0) \| = \left( \sum_{i=1}^N
\,\| \VEC{s}_i - \VEC{z}_i(0)  \|^2 \right)^{1/2} \leq
\delta e^{-K h} \right\} \ .
\]
\end{theorem}

\begin{proof}
The conclusion follows from Theorem~\ref{smoothSol} (with $K$ replaced
by $hK$ and $[a,b]$ by $[0,1]$) after we note that
\begin{align*}
\| F(\tau,\VEC{u}) - F(\tau,\tilde{\VEC{u}}) \|
& = \left( \sum_{i=1}^N\, \| h f_i(\tau,\VEC{u}_i) -
h f_i(\tau,\tilde{\VEC{u}}_i) \|^2 \right)^{1/2} \\
&\leq \left( \sum_{i=1}^N\, h^2 K^2
\| \VEC{u}_i - \tilde{\VEC{u}}_i \|^2 \right)^{1/2}
= h K \| \VEC{u} - \tilde{\VEC{u}} \|
\end{align*}
for all $(\tau,\VEC{u})$ and $(\tau,\tilde{\VEC{u}})$ in
\[
\left\{ (\tau,\VEC{u}) \in [0,1]\times (\RR^n)^N :
0 \leq \tau \leq 1 \text{ and }
\| \VEC{u} - \VEC{z}(\tau) \| = \left(
\sum_{i=1}^N \,\| \VEC{u}_i - \VEC{z}_i(\tau) \|^2 \right)^{1/2} <
\delta \right\} \ .  \qedhere
\]
\end{proof}

It follows from the previous theorem that we only need to solve
(\ref{LargeIter}) to get the solution of (\ref{NLBVPinnN}).

If Newton method is used to approximate the solution
of $\phi(\VEC{s}) = \VEC{0}$ in (\ref{LargeIter}), then 
the initial condition $\VEC{s}_0$ should be taken from
the disk of radius $\delta e^{-Kh}$ centred at $\VEC{z}(0)$.

If we compare with the simple Shooting Method of
Section~\ref{simpleShootM},  The dimension of the system for the 
Parallel Shooting Method (i.e.\ $nN$) is larger than the dimension of
the system for the simple Shooting Method (i.e.\ $n$).  However,
the initial condition $\VEC{s}_0$ for the Newton method can be chosen
from a disk of larger radius for the Parallel Shooting Method
(i.e.\ $\delta e^{-Kh}$) than for the simple Shooting Method
(i.e.\ $\delta e^{-K(b-a)}$).  The integration time is also shorter
for the Parallel Shooting Method (i.e.\ $h$) than for the simple
Shooting Method (i.e.\ $b-a$) though repeated $n$ times.

\begin{rmk}
Theorem~\ref{CforNewton} can also be applied to
(\ref{NLBVPinnN}) and (\ref{LargeIter}). The Newton Method is
\[
Q(\VEC{s}^{[j]}) \left( \VEC{s}^{[j+1]} - \VEC{s}^{[j]} \right) =
-\phi(\VEC{s}^{[j]}) \quad , \quad j \geq 0 \ ,
\]
where
\begin{align*}
&Q(\VEC{s}) = \diff_{\VEC{s}} \phi(\VEC{s}) = \\
&\begin{pmatrix}
\diff_{\VEC{y}_1}g(\VEC{s}_1,\VEC{u}_N(1,\VEC{s}_N))
& 0 & \ldots & 0 &
\diff_{\VEC{y}_2}g(\VEC{s}_1,\VEC{u}_N(1,\VEC{s}_N))U_N(1,\VEC{s}_N) \\
-U_1(1,\VEC{s}_1) & \Id &  \ldots & 0 & 0 \\
0 & -U_2(1,\VEC{s}_2) &  \ldots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & -U_{N-1}(1,\VEC{s}_{N-1}) & \Id \\
\end{pmatrix}\ ,
\end{align*}
where
\[
  U_i(\tau,\VEC{s}_i) = \diff_{\VEC{s}_i} \VEC{u}_i(\tau,\VEC{s}_i)
\]
for $1\leq i \leq N$.
\end{rmk}

\subsection{Family of Solutions}

One of the difficulty with the Shooting Method is to choose
$\VEC{s}^{[0]}$ in the Newton Method.  If the boundary value problem
that we want to solve is ``closed'' to another boundary value problem
for which we know how to choose $\VEC{s}^{[0]}$, we may perhaps use
this information to guess $\VEC{s}^{[0]}$ for our boundary value
problem.  For us, two ``closed'' boundary value problems will mean
that they are two ``closed'' members of a family of boundary value
problems.

Suppose that $f$ and $g$ in (\ref{NLBVP}) depend on a
parameter $\sigma \in [\sigma_a,\sigma_b]$.  We get the following
{\bfseries family of boundary value problems}\index{Family of Boundary
Value Problems} 
\begin{equation} \label{familyBVP}
\begin{split}
\VEC{y}'(t,\sigma) & = f(t,\VEC{y}(t,\sigma),\sigma)
\quad , \quad a \leq t \leq b \\
g(\VEC{y}(a,\sigma),\VEC{y}(b,\sigma),\sigma) &= \VEC{0}
\end{split}
\end{equation}
for $\sigma_a \leq \sigma \leq \sigma_b$.  Recall that
$\displaystyle \VEC{y}'(t,\sigma) = \dydx{\VEC{y}}{t}(t,\sigma)$.
We generally assume that
(\ref{familyBVP}) has a unique isolated solution
$\VEC{y}(t,\sigma)$ for each $\sigma$ and that the dependence of
$\VEC{y}(t,\sigma)$ on $\sigma$ is sufficiently differentiable.  We get the
{\bfseries family of solutions}\index{Boundary Value Problems!Family
of Solutions}
$\{ \VEC{y}(t,\sigma) : \sigma_a \leq \sigma \leq \sigma_b\}$.

A simple boundary value problem like (\ref{NLBVP}) can be included in
a family of boundary value problems as in (\ref{familyBVP}) by
defining
\begin{align*}
F(t,\VEC{y},\sigma) & = \sigma
f(t,\VEC{y}) + (1-\sigma) \left( A(t)\VEC{y} + \VEC{g}(t) \right)
\intertext{and}
G(\VEC{v},\VEC{w},\sigma) & = \sigma g(\VEC{v},\VEC{w})
+ (1-\sigma) \left( B_a \VEC{v} + B_b \VEC{w} - \VEC{y}_c \right)
\end{align*}
in (\ref{familyBVP}).  We have a linear boundary
value problem (that we may have chosen) for $\sigma =0$, and
our original non-linear boundary value problem for $\sigma = 1$.

To compute a branch of solutions of (\ref{familyBVP}), we solve
\begin{equation}\label{familyShoot1}
\begin{split}
\VEC{u}'(t,\VEC{s},\sigma) &= F(t,\VEC{u}(t,\VEC{s},\sigma),\sigma)
\quad , \quad a \leq t \leq b \\
\VEC{u}(a,\VEC{s},\sigma) &= \VEC{s}
\end{split}
\end{equation}
where $\VEC{s}$ is a solution of
\begin{equation}\label{familyShoot2}
\phi(\VEC{s},\sigma) \equiv
G(\VEC{s},\VEC{u}(b,\VEC{s},\sigma),\sigma) = \VEC{0} \ .
\end{equation}

\begin{theorem}
Suppose that:
\begin{enumerate}
\item  There is a solution $\VEC{u}$ of (\ref{familyShoot1}) and
(\ref{familyShoot2}) for $\sigma = \sigma_\ast \in [\sigma_a,\sigma_b]$
and $\VEC{s} = \VEC{s}_\ast$.
\item There exist $\eta_1$ and $\eta_2$ such that $F(t,\VEC{w},\sigma)$ is of
class $C^1$ in the tubular neighbourhood of
$\{ (t,\VEC{u}(t,\VEC{s}_\ast,\sigma_\ast),\sigma_\ast) : a\leq t \leq b\}$
defined by
\[
\left\{ (t,\VEC{w},\sigma) :
a \leq t\leq b, |\sigma - \sigma_\ast|<\eta_1
\text{ and } \|\VEC{w} - \VEC{u}(t,\VEC{s}_\ast,\sigma_\ast) \| <
\eta_2 \right\} \ ,
\]
and $G(\VEC{r},\VEC{w}, \sigma)$ is of class $C^1$ in the
neighbourhood of
$(\VEC{s}_\ast,\VEC{u}(b,\VEC{s}_\ast,\sigma_\ast),\sigma_\ast)$ defined
by
\[
\left\{ (\VEC{r},\VEC{w},\sigma) :
\|\VEC{r} - \VEC{s}_\ast\| < \eta_2 \ , \ 
\|\VEC{w} - \VEC{u}(b,\VEC{s}_\ast,\sigma_\ast)\| < \eta_2
\text{ and } |\sigma - \sigma_\ast| < \eta_1 \right\} \ .
\]
\item $\displaystyle \diff_{\VEC{s}}\phi(\VEC{s}_\ast,\sigma_\ast)$ is
non-singular.
\end{enumerate}
Then, there exist $\delta>0$ and a continuously differentiable function
$\VEC{s}:]\sigma_\ast -\delta,\sigma_\ast + \delta[ \to \RR^n$
such that $s(\sigma_\ast) = \VEC{s}_\ast$ and
$\VEC{u}(t,\VEC{s}(\sigma),\sigma)$ for $a\leq t \leq b$ is
a solution of (\ref{familyShoot1}) and (\ref{familyShoot2}).
\end{theorem}

\begin{proof}
We have that $\phi(\VEC{s}_\ast,\sigma_\ast) = \VEC{0}$ and
$\diff_{\VEC{s}}\phi(\VEC{s}_\ast,\sigma_\ast)$ is non-singular.
It follows from the implicit function theorem that all solutions
of $\phi(\VEC{s},\sigma) = \VEC{0}$ in a sufficiently small
open neighbourhood of $(\VEC{s}_\ast,\sigma_\ast)$ is of the form
$(\VEC{s}(\sigma),\sigma)$ for a differentiable function
$\VEC{s} : ]\sigma_\ast-\delta,\sigma_\ast+\delta[ \to \RR^n$ with
$\delta$ sufficiently small.  Moreover,
$\VEC{s}(\sigma_\ast) = \VEC{s}_\ast$.

The continuous differentiability of $\VEC{s}$ comes from our usual
assumption that $f$ is as smooth as needed.  In the present case, we
need $f$ to be continuously differentiable.
\end{proof}

\begin{rmkList}
\begin{enumerate}
\item Since $\VEC{s}:]\sigma_\ast -\delta,\sigma_\ast + \delta[ \to \RR^n$
given by the previous theorem is of class $C^1$, we may derive
$\phi(\VEC{s}(\sigma), \sigma) = \VEC{0}$ with respect to $\sigma$
to get
\[
\dfdx{\phi(\VEC{s}(\sigma), \sigma)}{\sigma}
= \diff_{\VEC{s}}\phi(\VEC{s},\sigma)\bigg|_{\VEC{s} = \VEC{s}(\sigma)}\;
\dydx{\VEC{s}}{\sigma}(\sigma)
+ \pdydx{\phi}{\sigma}(\VEC{s}(\sigma),\sigma) = \VEC{0} \ .
\]
This is a differential equation for $\VEC{s}(\sigma)$ with initial
condition $\VEC{s}(\sigma_\ast) = \VEC{s}_\ast$.
Note that
\[
\pdydx{\phi}{\sigma}(\VEC{s},\sigma)
= \pdydx{G}{\sigma}(\VEC{s},\VEC{u}(b,\VEC{s},\sigma),\sigma) +
\diff_{\VEC{y}_2}G(\VEC{s},\VEC{u}(b,\VEC{s},\sigma),\sigma)
\,V(b,\VEC{s},\sigma) \ ,
\]
where
\[
V(t,\VEC{s},\sigma) = 
\pdydx{\VEC{u}}{\sigma}(t,\VEC{s},\sigma)
\]
is the solution of
\[
V'(t,\VEC{s},\sigma) =
\diff_{\VEC{u}} F(t,\VEC{u}(t,\VEC{s},\sigma),\sigma)
\,V(t,\VEC{s},\sigma) +
\pdydx{F}{\sigma}(t,\VEC{u}(t,\VEC{s},\sigma),\sigma) \ .
\]
\item From $\VEC{s}(\sigma_\ast+\delta) = \VEC{s}(\sigma_\ast) +
\VEC{s}'(\sigma_\ast)\, \delta + O(\delta^2)$, we may choose
$\VEC{s}(\sigma_\ast) + \VEC{s}'(\sigma_\ast)\delta$ as initial value
in the Newton iterative method for the boundary value problem
(\ref{familyShoot1}) and (\ref{familyShoot2}) given by
$\sigma = \sigma_\ast + \delta$.  Recursively, we may be able to
``find'' a branch of solutions $\VEC{s}:[\sigma_a,\sigma_b]\to \RR^n$
for the family of boundary value problems given by
(\ref{familyShoot1}) and (\ref{familyShoot2}).  This subject of
``path following'' is another exciting subject of numerical analysis
that unfortunately we will not address in this book.
\end{enumerate}
\end{rmkList}

\section{Finite Difference Methods}

This section is based on Keller's lectures \cite{K} and Ascher et
al.'s book \cite{AMR}.

The next chapter will cover finite difference methods to solve partial
differential equations.  The present section can be seen as an
introduction to this broader subject since a boundary value problem
for ordinary differential equation is a one-dimensional boundary value
problem for partial differential equation.

The general boundary value problem is of the form
\begin{equation}\label{NLBVPreapeat}
\begin{split}
P(\VEC{y}(t)) \equiv \VEC{y}'(t) - f(t,\VEC{y}(t)) &= \VEC{0}
\quad , \quad a \leq t \leq b\\
g(\VEC{y}(a), \VEC{y}(b)) &= \VEC{0}
\end{split}
\end{equation}

As usual, let $\{t_i\}_{i=0}^N$ be a partition of $[a,b]$ such that
$t_0 = a$, $t_N = b$ , $t_{i+1} = t_i + h_i$ with $h_i > 0$
for $0\leq i < N$ and
$\displaystyle h = \max_{0\leq i < N} h_i \leq \theta \min_{0\leq i < N} h_i$
for some constant $\theta$.

The associated general form of a finite difference method to
approximate the solution of (\ref{NLBVPreapeat}) is
\begin{equation} \label{GFFDM}
\begin{split}
P_{i,h}(\VEC{W}) &= \VEC{0}
\quad , \quad 0 \leq i < N \\
g(\VEC{w}_0, \VEC{w}_N) &= \VEC{0}
\end{split}
\end{equation}
where $\displaystyle \VEC{W} =
\begin{pmatrix} \VEC{w}_0 \\ \VEC{w}_1 \\ \vdots \\
\VEC{w}_N \end{pmatrix} \in (\RR^n)^{N+1}$.
We hope that the solution $\{\VEC{w}_i\}_{i=0}^N$ of this finite
difference equation (\ref{GFFDM}) will provide an approximation of the
solution of (\ref{NLBVPreapeat}).  Namely, we hope that
$\VEC{w}_i$ will be a good approximation of
$\VEC{y}_i \equiv \VEC{y}(t_i)$ for $0 \leq i \leq N$.

\begin{egg}
The {\bfseries trapezoidal method} or {\bfseries scheme}
\index{Finite Difference Methods!Trapezoidal Scheme} to solve a
general boundary value problem is a one-step method defined by
\begin{align*}
P_{i,h}(\VEC{W}) = \frac{\VEC{w}_{i+1}-\VEC{w}_i}{h_i} - \frac{1}{2}
\left( f(t_{i+1},\VEC{w}_{i+1}) + f(t_i,\VEC{w}_i)\right)
&= \VEC{0} \quad , \quad 0 \leq i < N\\
g(\VEC{w}_0, \VEC{w}_N) &= \VEC{0}
\end{align*}
where $h_i = t_{i+1}- t_i$.
\end{egg}

\begin{rmk}[Important]
In this section, when we write $\displaystyle \lim_{h\to 0} E = 0$
for some expression $E$ that depends on $h$, we mean that for each
$\epsilon >0$, there exist $h_\epsilon >0$ such that
$|E| < \epsilon$ for all partition $\{t_i\}_{i=0}^N$ as defined
above such that $h<h_\epsilon$.  If $N$ is included in the expression
$E$, it is the $N$ associated to the partition with maximum step size
$h$ under consideration in the expression $E$. 

The same consideration applies if we say that an expression $E$ that
depends on $h$ is true for $h < h_0$.  Namely, it means that $E$ is
true for all partition $\{t_i\}_{i=0}^N$ as defined
above such that $h<h_0$ and, if $N$ is included in the expression
$E$, then $N$ is associated to the partition with maximum step size
$h$ under consideration in the expression $E$.
\end{rmk}

To determine the quality of a finite difference method to approximate
the solution of a boundary value problem, we will use concepts similar
to those used before for the initial value problems; namely,
convergence, consistency and stability.

\begin{defn}
The method (\ref{GFFDM}) is
{\bfseries convergent}\index{Finite Difference Methods!Convergence} if, for all
well-posed boundary value problem (\ref{NLBVPreapeat}),
\[
\lim_{h \rightarrow 0}\,\max_{0\leq i \leq N} \,
\| \VEC{y}(t_i) - \VEC{w}_i \| = 0 \ .
\]
\end{defn}

\begin{rmk}
As for the shooting methods, we will not consider any perturbation of
(\ref{NLBVPreapeat}) as we did for the initial value problems.  We
will come back on convergence of finite difference methods in
Chapter~\ref{FiniteDiffMeth}. 
\end{rmk}

\begin{defn}
The {\bfseries local truncation error}\index{Finite Difference
Methods!Local Truncation Error} of a finite difference method
as in (\ref{GFFDM}) is
\[
\VEC{\tau}_i(\VEC{y}) = P_{i,h}(\VEC{Y}) \quad , \quad 0 \leq i < N \ ,
\]
where $\displaystyle \VEC{Y} =
\begin{pmatrix} \VEC{y}_0 \\ \VEC{y}_1 \\ \vdots \\
\VEC{y}_N \end{pmatrix} \in (\RR^n)^{N+1}$.
Recall that $\VEC{y}_i = \VEC{y}(t_i)$ for $0 \leq i \leq N$, where
$\{t_i\}_{i=0}^N$ is any partition of $[a,b]$ with the
maximum step-size $h$.

The method (\ref{GFFDM}) is of
{\bfseries order}\index{Finite Difference Methods!Orde} $p>0$ if there
exist a function $\tau :\RR^n \to [0,\infty[$ such that
$\| \VEC{\tau}_i(\VEC{y}) \| \leq \VEC{\tau}(\VEC{y})  = O(h^p)$ for
$0 \leq i < N$.
\end{defn}

\begin{defn}
The finite difference method (\ref{GFFDM}) is
{\bfseries consistent}\index{Finite Difference Methods!Consistency} if, for all
well-posed boundary value problem (\ref{NLBVPreapeat}),
\[
\lim_{h \rightarrow 0}\, \max\left\{
\max_{0 \leq i < N} \, \| \VEC{\tau}_i(\VEC{y}) \| .
\| g(\VEC{y}_0, \VEC{y}_N \| \right\} = 0 \ .
\]
\end{defn}

\begin{defn}
The finite difference method (\ref{GFFDM}) is
{\bfseries stable}\index{Finite Difference Methods!Stable} if, for any
well-posed boundary value problem (\ref{NLBVPreapeat}), there exist
$K>0$, $h_0 >0$ and $\delta>0$ such that
\begin{equation} \label{FDMS}
\| \VEC{u}_i - \VEC{v}_i \| \leq
K \max \Big\{ \| g(\VEC{u}_0,\VEC{u}_N) -
\VEC{g}(\VEC{v}_0,\VEC{v}_N) \| ,
\max_{1\leq i < N} \| P_{i,h}(\VEC{U}) - P_{i,h}(\VEC{V}) \| \Big\}
\end{equation}
for all
$\displaystyle \VEC{U} =
\begin{pmatrix} \VEC{u}_0 \\ \VEC{u}_1 \\ \vdots \\ \VEC{u}_N \end{pmatrix}$
and
$\displaystyle \VEC{V} =
\begin{pmatrix} \VEC{v}_0 \\ \VEC{v}_1 \\ \vdots \\ \VEC{v}_N \end{pmatrix}$
in
\[
S_\delta(\VEC{y}) = \left\{ \VEC{w} \in (\RR^n)^{N+1} :
\| \VEC{w}_i - \VEC{y}_i \| < \delta
\quad \text{for} \quad 0 \leq i \leq N \right\} \ ,
\]
and all $h < h_0$.
\end{defn}

The following theorem will be proved in Chapter~\ref{FiniteDiffMeth}
(Theorem~\ref{StabConstConv}) about finite difference methods for
partial differential equations.

\begin{theorem}
If a method like (\ref{GFFDM}) is stable and consistent for the
linear boundary value problem (\ref{NLBVPreapeat}), then the method is
convergent.
\label{SCssiC}
\end{theorem}

We will prove a version of this theorem for the linear boundary value
problems in the next section.

\subsection{Finite Difference Methods for Linear Boundary Value
Problems}

As we did for the shooting methods, we start with the linear boundary
value problem
\begin{equation}\label{LinBVPrepeat}
\begin{split}
P(\VEC{y}(t)) = \VEC{y}'(t) - A(t)\VEC{y}(t) - f(t) &= \VEC{0}
\quad , \quad a \leq t \leq b \\
B_a\VEC{y}(a) + B_b\VEC{y}(b) - \VEC{y}_c &= \VEC{0}
\end{split}
\end{equation}

The general form of a finite difference method to approximate the solution of
(\ref{LinBVPrepeat}) is
\begin{equation} \label{LGFFDM}
\begin{split}
P_{i,h}(\VEC{W}) = L_{i,h}(\VEC{W}) - F_{i,h}(f) &= \VEC{0} 
\quad , \quad 0 \leq i < N \\
B_a\VEC{w}_0 + B_b\VEC{w}_N - \VEC{y}_c &= \VEC{0}
\end{split}
\end{equation}
where
$\displaystyle \VEC{W} =
\begin{pmatrix} \VEC{w}_0 \\ \VEC{w}_1 \\ \vdots \\ \VEC{w}_N \end{pmatrix}
\in (\RR^n)^{N+1}$ and
$L_{i,h}(\VEC{W})$ is associated to the linear part
$L(\VEC{y}(t)) = \VEC{y}'(t) - A(t)\VEC{y}(t)$.

\begin{egg}
The
{\bfseries midpoint scheme}\index{Finite Difference Methods!Midpoint Scheme}
or
{\bfseries centred Euler scheme}\index{Finite Difference
Methods!Centred Euler Scheme}
to solve linear boundary value problems is a one-step method defined by
\begin{align*}
P_{i,h}(\VEC{W}) = L_{i,h}(\VEC{W}) - F_{i,h}(f) &= \VEC{0} \quad ,
\quad 0 \leq i < N \\
B_a\VEC{w}_0 + B_b\VEC{w}_N &= \VEC{y}_c
\end{align*}
where
$\displaystyle L_{i,h}(\VEC{W})
= \frac{\VEC{w}_{i+1} - \VEC{w}_i}{h_i} - \frac{1}{2}
A(t_i+h_i/2) \left( \VEC{w}_{i+1} + \VEC{w}_i\right)$
and
$\displaystyle F_{i,h}(f) = f(t_i+h_i/2)$.
\label{eggmidpointscheme}
\end{egg}

\begin{egg}
The trapezoidal scheme 
to solve linear boundary value problems is a one-step method defined by
\begin{align*}
P_{i,h}(\VEC{W}) = L_{i,h}(\VEC{W}) - F_{i,h}(f) &= \VEC{0} \quad ,
\quad 0 \leq i < N \\
B_a\VEC{w}_0 + B_b\VEC{w}_N &= \VEC{y}_c
\end{align*}
where
$\displaystyle L_{i,h}(\VEC{W})
= \frac{\VEC{w}_{i+1} - \VEC{w}_i}{h} - \frac{1}{2}
\left( A(t_i+h) \VEC{w}_{i+1} + A(t_i) \VEC{w}_i \right)$
and
$\displaystyle F_{i,h}(f) = \frac{1}{2}\left( f(t_i+h) + f(t_i) \right)$.
\label{eggtrapezSchemeLin}
\end{egg}

Because of the very special form of (\ref{LGFFDM}), in particular the
linearity of $L_{i,h}$, the stability condition
(\ref{FDMS}) can be reduced to
\begin{equation} \label{lin_form_FDMS}
\| \VEC{u}_i \| \leq K
\max \left\{ \| B_a\VEC{u}_0 + B_b\VEC{u}_N \| ,
\max_{0\leq j < N} \| L_{j,h}(\VEC{U}) \|
\right\} \quad , \quad 0 \leq i \leq N \ ,
\end{equation}
for all
$\displaystyle \VEC{U} = \begin{pmatrix} 
\VEC{u}_0 \\ \VEC{u}_1 \\ \vdots \\ \VEC{u}_N \end{pmatrix}
\in (\RR^n)^{N+1}$ and all $h < h_0$.

\begin{prop}
If a method like (\ref{LGFFDM}) is stable and consistent for the
linear boundary value problem (\ref{LinBVPrepeat}), then it is
convergent.
\label{FEMST_C}
\end{prop}

\begin{proof}
To prove this result, 
let $\VEC{r}_i = \VEC{y}(t_i) - \VEC{w}_i$ for $0\leq i \leq N$, and
$\displaystyle \VEC{R} = \begin{pmatrix}
\VEC{r}_0 \\ \VEC{r}_1 \\ \vdots \\ \VEC{r}_N \end{pmatrix}$.
The local truncation error is
\[
\VEC{\tau}_i(\VEC{y}) =
L_{i,h}(\VEC{Y}) - F_{i,h}(f)
= L_{i,h}(\VEC{R}) + \underbrace{L_{i,h}(\VEC{W})
- F_{i,h}(f)}_{=P_{i,h}(\VEC{W}) = \VEC{0}}
= L_{i,h}(\VEC{R}) \quad , \quad 0 \leq i < N \ ,
\]
and $B_a\VEC{r}_0 + B_b\VEC{r}_N = \VEC{0}$ because
$B_a\VEC{w}_0 + B_b\VEC{w}_N -\VEC{y}_c = \VEC{0}$ and
$B_a\VEC{y}_0 + B_b\VEC{y}_N - \VEC{y}_c = \VEC{0}$.
Since the method is consistent,
$\displaystyle \lim_{h\to 0} \max_{0\leq i < N}\|\VEC{\tau}_i(\VEC{y}) \|= 0$.
Finally, since the method is stable, we have from the remark before
the statement of the proposition that
\[
\| \VEC{r}_i \| \leq
K \max \left\{ \| B_a\VEC{r}_0 +B_b\VEC{r}_N \| ,
\max_{1\leq j < N} \| L_{j,h}(\VEC{R}) \|
\right\}
\leq K \max_{0\leq j < N} \| \VEC{\tau}_j(\VEC{y}) \|
\]
for some constant $K$ and $0 \leq i \leq N$.
Thus
\[
0 \leq \lim_{h\to 0}  \max_{1\leq i \leq N} \| \VEC{r}_i \| 
\leq K \lim_{h\to 0} \max_{0\leq j < N} \| \VEC{\tau}_j(\VEC{y}) \| = 0 \ ,
\]
where, as usual, $N$ is associated to the chosen partition of
$[a,b]$ of maximum size $h$.
\end{proof}

The result is also true for finite difference
methods (\ref{GFFDM}) applied to the general boundary value problem
(\ref{NLBVPreapeat}) (Theorem~\ref{SCssiC} above) but the proof is
not as direct as for the linear boundary value problems above.

The next two propositions will be used to prove that the method
(\ref{LGFFDM}) applied to the linear boundary value problem
(\ref{LinBVPrepeat}) is stable and consistent if it is stable and
consistent when applied to the initial value problem 
\begin{align*}
L(\VEC{y}(t)) &= \VEC{y}'(t) - A(t) \VEC{y}(t) = \VEC{0} \\
\VEC{y}(a) &= \VEC{s} \in \RR^n
\end{align*}
(Corollary~\ref{CorSC_FDM} below).

\begin{prop}
Consider two linear boundary value problems
\begin{equation} \label{TBVP}
\begin{split}
L(\VEC{y}(t)) = \VEC{y}'(t) - A(t)\VEC{y}(t) &= f(t) \quad ,
\quad a \leq t \leq b\\
B_a^{[\nu]}\VEC{y}(a) + B_b^{[\nu]}\VEC{y}(b) = \VEC{y}_c
\end{split}
\end{equation} 
for $\nu =0$ and $1$.
\begin{enumerate}
\item For $\nu$ fixed,
\begin{equation} \label{TBVPsyst}
\begin{split}
L(Y^{[\nu]}(t)) &= 0 \quad , \quad a \leq t \leq b \\
B_a^{[\nu]}Y^{[\nu]}(a) + B_b^{[\nu]}Y^{[\nu]}(b) &= \Id
\end{split}
\end{equation}
has a unique solution $Y^{[\nu]}(t)$ if and only if (\ref{TBVP}) has a
unique solution.
\item Moreover, if (\ref{TBVP}) for $\nu =0$ has a unique
solution, then (\ref{TBVP}) for $\nu =1$ has a unique solution if and
only if $B_a^{[1]} Y^{[0]}(a) + B_b^{[1]}Y^{[0]}(b)$ is invertible.
\end{enumerate}
\label{BasicProp}
\end{prop}

\begin{proof}
\stage{1}
For this part, we assume that $\nu$ is fixed.  From
Theorem~\ref{BVPCdiff}, the boundary value problem (\ref{TBVP}) 
has a unique solution if and only if
$Q^{[\nu]}= B_a^{[\nu]} +B_b^{[\nu]}Y(b)$ is invertible, where $Y(t)$
is the (fundamental) solution of $L(Y)=0$ with $Y(a) = \Id$.

Moreover, it follows from the second step in Algorithm~\ref{AlgoSimpleSM}
with $\VEC{y}_c = 0$ and $\VEC{y}_0(t) = 0$ for all $t$ that 
the solution of (\ref{TBVPsyst}) is of the form
$Y^{[\nu]}(t) = Y(t) R^{[\nu]}$, where $R^{[\nu]}$ is a constant
matrix satisfying
\[
\Id = B_a^{[\nu]}Y^{[\nu]}(a) + B_b^{[\nu]}Y^{[\nu]}(b) =
\left( B_a^{[\nu]} + B_b^{[\nu]} Y(b) \right)R^{[\nu]} = Q^{[\nu]}
R^{[\nu]} \ .
\]
Such a system has a unique solution $R^{[\nu]}$ if and only if
$Q^{[\nu]}$ is non-singular.  In that case,
$R^{[\nu]} = \left(Q^{[\nu]}\right)^{-1}$.

\stage{2} To prove this part of the theorem, suppose that
(\ref{TBVP}) with $\nu =0$ has a unique solution.  Then
(\ref{TBVPsyst}) with $\nu=0$ has a unique solution given by
$Y^{[0]}(t) = Y(t) \left(Q^{[0]}\right)^{-1}$.  In particular, 
$Q^{[0]}$ is invertible.  Since
\[
B_a^{[1]}Y^{[0]}(a) + B_b^{[1]}Y^{[0]}(b)
= \left( B_a^{[1]} + B_b^{[1]} Y(b) \right)
\left(Q^{[0]}\right)^{-1} = Q^{[1]}\left(Q^{[0]}\right)^{-1} \ ,
\]
we have that $Q^{[1]}$ is invertible if and only if
$B_a^{[1]}Y^{[0]}(a) + B_b^{[1]}Y^{[0]}(b)$ is invertible.  Thus,
(\ref{TBVPsyst}) with $\nu=1$ has a unique solution if and only if
$B_a^{[1]}Y^{[0]}(a) + B_b^{[1]}Y^{[0]}(b)$ is invertible.  The
conclusion from the first part for $\nu =1$.
\end{proof}

The general form (\ref{LGFFDM}) of a finite difference method for a
linear boundary value problem can be written explicitly as
\begin{equation} \label{MFFDM}
\begin{split}
L_{i,h}(\VEC{W}) = \sum_{k=0}^N\,C_{i,k} \VEC{w}_k
& = F_{i,h}(f) \quad , \quad  0 \leq i < N \\
B_a\VEC{w}_0 + B_b\VEC{w}_N &= \VEC{y}_c
\end{split}
\end{equation}
The $C_{i,k}$ may depend on $h$ and $t_i$.  If we set
\[
A = \begin{pmatrix}
B_a & 0 & \ldots & B_b \\
C_{0,0} & C_{0,1} & \ldots & C_{0,N} \\
\vdots & \vdots & \ddots & \vdots \\
C_{N-1,0} & C_{N-1,1} & \ldots & C_{N-1,N}
\end{pmatrix} \; , \;
\VEC{W} = \begin{pmatrix}
\VEC{w}_0 \\ \VEC{w}_q \\ \vdots \\ \VEC{w}_N
\end{pmatrix}
\quad \text{and} \quad
\VEC{F} = \begin{pmatrix}
\VEC{y}_c \\ F_{0,h}(f) \\ \vdots \\ F_{N-1,h}(f)
\end{pmatrix}  \ ,
\]
we can rewrite (\ref{MFFDM}) as
\begin{equation} \label{AWFfromFDM}
  A \VEC{W} = \VEC{F} \ .
\end{equation}

\begin{prop}
The finite difference method (\ref{MFFDM}) is stable for the linear
boundary value problem (\ref{LinBVPrepeat}) if and only if there exist
two constants $K$ and $h_0$ such that $A^{-1}$ exists and
$\| A^{-1} \|_\infty < K$ for $0< h < h_0$.
\label{BasicFDM}
\end{prop}

\begin{proof}
\stage{A} Suppose that (\ref{MFFDM}) is stable for the linear boundary
value problem  (\ref{LinBVPrepeat}).   Thus, there exist $K>0$ and
$h_0 >0$ such that (\ref{lin_form_FDMS}) is 
satisfied for all $\VEC{U} \in (\RR^n)^{N+1}$ and $h < h_0$.  But
(\ref{lin_form_FDMS}) is another way of saying that
$\| \VEC{U} \|_\infty \leq K \| A \VEC{U}\|_\infty$ for all
$\VEC{U} \in (\RR^n)^{N+1}$ and $h < h_0$.   From this relation, we
have that $A$ is one-to-one and therefore invertible.  We can then
write that $\| A^{-1} \VEC{U} \|_\infty \leq K \|\VEC{U} \|_\infty$
for all $\VEC{U} \in (\RR^n)^{N+1}$ and $h < h_0$; namely,
$\| A^{-1} \|_\infty \leq K$ for $h < h_0$.

\stage{B} Suppose that there exist two constants $K$ and $h_0$ such
that $A^{-1}$ exists and $\| A^{-1} \|_\infty < K$ for
$0<h < h_0$.  From the definition of the norm of matrices, we get
$\| A^{-1} \VEC{U} \|_\infty \leq K \|\VEC{U} \|_\infty$ for all
$\VEC{U} \in (\RR^n)^{N+1}$ and $h < h_0$.  Thus,
$\|\VEC{U} \|_\infty \leq K \| A \VEC{U} \|_\infty$ for all
$\VEC{U} \in (\RR^n)^{N+1}$ and $h < h_0$.  This is exactly the statement of
(\ref{lin_form_FDMS}); namely, (\ref{MFFDM}) is stable for the linear
boundary value problem  (\ref{LinBVPrepeat}).
\end{proof}

\begin{theorem}
Consider the linear boundary value problems (\ref{TBVP}) and the finite
difference methods
\begin{equation} \label{TBVP_FDM}
\begin{split}
L_{i,h}(\VEC{W}) = \sum_{k=0}^N\,C_{i,k}\,\VEC{w}_k
& = F_{i,h}(f) \quad , \quad 0 \leq i < N \\
B_a^{[\nu]}\VEC{w}_0 + B_b^{[\nu]}\VEC{w}_N &= \VEC{y}_c
\end{split}
\end{equation}
for $\nu=0$ and $1$.  Suppose that both linear boundary value problems
in (\ref{TBVP}) have a unique solution.  The method (\ref{TBVP_FDM})
with $\nu=0$ is stable and consistent for (\ref{TBVP}) with $\nu=0$ if
and only if the method (\ref{TBVP_FDM}) with $\nu=1$ is stable and
consistent for (\ref{TBVP}) with $\nu=1$.
\label{Equ_0and1}
\end{theorem}

\begin{rmk}
Before proving this theorem, it will help to
review some of the properties of the infinity norm.

We have $\displaystyle \|\VEC{u} \|_\infty = \max_{1 \leq i \leq n} |u_i|$ for
$\VEC{u}\in \RR^n$ and
$\displaystyle \|\VEC{U}\|_\infty = \max_{0 \leq i \leq N} \| \VEC{u}_i \|_\infty$
for 
$\displaystyle \VEC{U} = \begin{pmatrix}
\VEC{u}_0 \\ \VEC{u}_1 \\ \vdots \\ \VEC{u}_N \end{pmatrix}
\in (\RR^n)^{N+1}$.  The associated norm of the linear mapping
from $(\RR^n)^{N+1}$ to itself defined by the matrix
\[
M = \begin{pmatrix}
M_{0,0} & M_{0,1} & \ldots & M_{0,N} \\
M_{1,0} & M_{1,1} & \ldots & M_{1,N} \\
 \vdots & \vdots & \ddots & \vdots \\
M_{N,0} & M_{N,1} & \ldots & M_{N,N}
\end{pmatrix} \ ,
\]
where the $M_{i,j}$ are \nn matrices, is given by
$\displaystyle \| M \|_\infty = \max_{\substack{\VEC{U} \in (\RR^n)^{N+1}\\
\VEC{U} \neq \VEC{0}}}
\frac{\|M \VEC{U}\|_\infty}{\|\VEC{U}\|_\infty}$.

We now show that there exists $H>1$ such that
\begin{equation} \label{CannotPEqu}
\frac{1}{H}  \max_{0\leq i \leq N} \sum_{j=0}^N \|M_{i,j}\|_\infty
\leq \| M \|_\infty \leq \max_{0\leq i \leq N}
\sum_{j=0}^N \|M_{i,j}\|_\infty \ .
\end{equation}
Since all norms on a finite dimensional space are
equivalent, there exist constants $C_1$ and $C_2$ such that
\begin{equation} \label{CannotPEquA}
C_1 \max_{0\leq i \leq N} \sum_{j=0}^N \|M_{i,j}\|_\infty
\leq \| M \|_\infty \leq C_2 \max_{0\leq i \leq N} \sum_{j=0}^N
\|M_{i,j}\|_\infty
\end{equation}
because
$\displaystyle \max_{0\leq i \leq N} \sum_{j=0}^N \|M_{i,j}\|_\infty$
is a norm on the space of \nm{n(N+1)}{n(N+1)} matrices.

If (\ref{CannotPEquA}) is true with $C_1 \geq 1$, then it is obviously
true if we replace $C_1$ by a constant $1/H$ with $H>1$.
We can take $C_2 = 1$ because
\begin{align*}
\| M \VEC{U} \|_\infty
&= \max_{0 \leq i \leq N} \| \sum_{j=0}^N M_{i,j} \VEC{u}_j \|_\infty
\leq \max_{0\leq i \leq N} \left( \sum_{j=0}^N \| M_{i,j}\|_\infty
\| \VEC{u}_j \|_\infty \right) \\
&\leq \max_{0\leq i \leq N} \left( \sum_{j=0}^N \| M_{i,j}\|_\infty \right)
\max_{0 \leq j \leq N} \| \VEC{u}_j \|_\infty
= \max_{0\leq i \leq N} \left( \sum_{j=0}^N \| M_{i,j} \|_\infty \right)
\| \VEC{U} \|_\infty
\end{align*}
for all $\VEC{U} \in (\RR^n)^{N+1}$.  Thus
$\displaystyle \| M \|_\infty \leq
\max_{0\leq i \leq N} \sum_{j=0}^N \|M_{i,j}\|_\infty$.
\label{FDMrmkNorm}
\end{rmk}

\begin{proof}[Proof of Theorem~\ref{Equ_0and1}]
Since $P_{i,h}(\VEC{W}) = L_{i,h}(\VEC{W}) - F_{i,h}(f)$ is
independent of $\nu$, the local truncation error
$\VEC{\tau}_i(\VEC{y}^{[\nu]})$ for $0\leq i < N$ are identical for
both problems.  Hence the methods are either both consistent or both
non-consistent.  Recall that the boundary conditions are exactly
satisfied in both cases.

Suppose that the method (\ref{TBVP_FDM}) with $\nu=0$ is stable
and consistent for (\ref{TBVP}) with $\nu=0$.  Let
\[
A^{[\nu]} = \begin{pmatrix}
B_a^{[\nu]} & \VEC{0} & \ldots & B_b^{[\nu]} \\
C_{0,0} & C_{0,1} & \ldots & C_{0,N} \\
\vdots & \vdots & \ddots & \vdots \\
C_{N-1,0} & C_{N-1,1} & \ldots & C_{N-1,N}
\end{pmatrix} \ .
\]
For $h$ small enough, we can write
\begin{equation}\label{TBVP_FDMp1}
A^{[1]} = \left( \Id + D
 \left(A^{[0]}\right)^{-1}\right)A^{[0]} \ ,
\end{equation}
where
\[
  D = A^{[1]} - A^{[0]}
= \begin{pmatrix}
B_a^{[1]} - B_a^{[0]} & 0 & \ldots & B_b^{[1]} - B_b^{[0]} \\
0 & 0 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots &0
\end{pmatrix} \ ,
\]
because $\left(A^{[0]}\right)^{-1}$ exists.  To be precise, according to
Proposition~\ref{BasicFDM}, there exist $K>0$ and $h_0>0$ such that
$\left(A^{[0]}\right)^{-1}$ exists and
$\| \left(A^{[0]}\right)^{-1} \|_\infty \leq K$ if $0<h < h_0$.

Suppose that
\[
\left(A^{[0]}\right)^{-1}= \begin{pmatrix}
Z_{0,0} & Z_{0,1} & \ldots & Z_{0,N} \\
Z_{1,0} & Z_{1,1} & \ldots & Z_{1,N} \\
\vdots & \vdots & \ddots & \vdots \\
Z_{N,0} & Z_{N,1} & \ldots & Z_{N,N}
\end{pmatrix} \ ,
\]
where the matrices $Z_{i,j}$ are \nn matrices.
Since $A^{[0]} \left(A^{[0]}\right)^{-1}= \Id$, we get
$B_a^{[0]}Z_{0,j} + B_b^{[0]} Z_{N,j} = 0$ for $1\leq j \leq N$
and $B_a^{[0]}Z_{0,0} + B_b^{[0]} Z_{N,0} = \Id$.  Therefore,
\[
\Id - D \left(A^{[0]}\right)^{-1} = \begin{pmatrix}
Q_{0,0} & Q_{0,1} & \ldots & Q_{0,N} \\
0 & \Id &  \ldots & 0 \\
\vdots  & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \Id
\end{pmatrix} \ ,
\]
where
\[
Q_{0,j} = B_a^{[1]} Z_{0,j} + B_b^{[1]}Z_{N,j}
\]
for $j=0$, $1$, \ldots, $N$.

Moreover, $A^{[0]}\left(A^{[0]}\right)^{-1} = \Id$ implies that
$B_a^{[0]} Z_{0,0} + B_b^{[0]} Z_{N,0} = \Id$ and $L_{i,h}(Z_0) = 0$ for
$1 \leq i < N$ where
$\displaystyle Z_0 = \begin{pmatrix} Z_{0,0} \\ Z_{1,0} \\ \vdots \\
Z_{N,0} \end{pmatrix}$.
Thus $\{ Z_{i,0} \}_{i=0}^N$ is an
approximation of the solution of
\begin{align*}
L(Y(t)) = Y'(t) - A(t)Y(t) &= 0 \quad ,
\quad a \leq t \leq b\\ 
B_a^{[0]}Y(a) + B_b^{[0]}Y(b) &= \Id
\end{align*}
Since the method (\ref{TBVP_FDM}) with $\nu=0$ is
consistent and stable, it is convergent according to
Proposition~\ref{FEMST_C}.  Therefore, if we use the method
(\ref{TBVP_FDM}) for the linear boundary value problem
(\ref{TBVP}) with $\nu =0$ and $f=0$, we get that
$\displaystyle
\lim_{h \to 0} \max_{0\leq i \leq N} \| Z_{i,0} - Y^{[0]}(t_i)\|_\infty = 0$.
Hence
\begin{equation}\label{TBVP_FDMp2}
\begin{split}
&\left\| Q_{0,0} - B_a^{[1]}Y^{[0]}(a) -B_b^{[1]}Y^{[0]}(b) \right\|_\infty \\
&\qquad \qquad
= \left\| B_a^{[1]}(Z_{0,0} -Y^{[0]}(a)) -B_b^{[1]}(Z_{N,0} -Y^{[0]}(b))
\right\|_\infty \to 0
\end{split}
\end{equation}
as $h \to 0$.  From Proposition~\ref{BasicProp} and our hypothesis
about the uniqueness of the solutions, we have that
$B_a^{[1]} Y^{[0]}(a) + B_b^{[1]} Y^{[0]}(b)$ is invertible.  Thus, if
we select $\tilde{h}_0 < h_0$ small enough such that
\[
\left\| Q_{0,0} - B_a^{[1]}Y^{[0]}(a) -B_b^{[1]}Y^{[0]}(b) \right\|_\infty \\
< \frac{1}{2}\,\left\| \big(B_a^{[1]} Y^{[0]}(a)
  + B_b^{[1]} Y^{[0]}(b)\big)^{-1}\right\|^{-1}_\infty
\]
for $h <\tilde{h}_0$, then it follows from the Banach Lemma
(Corollary~\ref{Banach_corG}) that $Q_{0,0}$ is invertible for
$h < \tilde{h}_0$ and
\[
\left\|Q_{0,0}^{-1}\right\| \leq T \equiv
2 \left\| \big(B_a^{[1]} Y^{[0]}(a)
  + B_b^{[1]} Y^{[0]}(b)\big)^{-1}\right\|_\infty \ .
\]
Therefore, $\Id - D\left(A^{[0]}\right)^{-1}$
is invertible for $h < \tilde{h}_0$ and it follows from
(\ref{TBVP_FDMp1}) that $A^{[1]}$ is invertible for $h < \tilde{h}_0$.
In fact, we have
\begin{equation}\label{TBVP_FDMp3}
\left(A^{[1]}\right)^{-1} = \left(A^{[0]}\right)^{-1} \begin{pmatrix}
Q^{-1}_{0,0} & -Q^{-1}_{0,0} Q_{0,1}  &
\ldots & -Q^{-1}_{0,0} Q_{0,N} \\
0 & \Id & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \Id
\end{pmatrix} \ .
\end{equation}

Finally, we now use (\ref{CannotPEqu}) in Remark~\ref{FDMrmkNorm} to
obtain
\begin{align*}
\sum_{j=1}^N \| Q_{0,j} \|_\infty
&= \sum_{j=1}^N \| B_a^{[1]} Z_{0,j} + B_b^{[1]} Z_{N,j} \|_\infty
\leq \|B_a^{[1]}\|_\infty \underbrace{\sum_{j=1}^N \| Z_{0,j} \|_\infty}_{\leq
H \left\|(A^{[0]})^{-1}\right\|_\infty}
+ \| B_b^{[1]}\|_\infty \underbrace{\sum_{j=1}^N \| Z_{N,j} \|_\infty}_{\leq
H \left\|(A^{[0]})^{-1}\right\|_\infty} \\
&\leq H \left\| \left( A^{[0]}\right)^{-1}\right\|_\infty
\left( \| B_a^{[1]} \|_\infty + \| B_b^{[1]} \|_\infty \right)
\leq K H \left( \| B_a^{[1]} \|_\infty + \| B_b^{[1]} \|_\infty \right) \ .
\end{align*}
Hence, we get from
(\ref{TBVP_FDMp3}) that
\[
\left\| \left(A^{[1]}\right)^{-1} \right\|_\infty \leq K T \left(1 + K H \left(
\| B_a^{[1]} \|_\infty + \| B_b^{[1]} \|_\infty \right)\right)
\]
for $h < \tilde{h}_0$.  Thus, from Proposition~\ref{BasicFDM}, the finite
difference method (\ref{TBVP_FDM}) with $\nu=1$ is stable for the
linear boundary value problem (\ref{TBVP}) with $\nu=1$.

The opposite implication follows by interchanging $\nu=0$ and $\nu=1$.
\end{proof}

\begin{cor}
Suppose that (\ref{LinBVPrepeat}) has a unique solution.  The
finite difference method (\ref{MFFDM}) is stable and consistent for
(\ref{LinBVPrepeat}) if and only if the finite difference method
\begin{equation} \label{FDM_IVP}
\begin{split}
L_{i,h}(\VEC{W}) &= \sum_{k=0}^N\,C_{i,k}\,\VEC{w}_k
= F_{i,h}(f) \quad , \quad 0 \leq i < N \\
\VEC{w}_0 &= \VEC{y}_c
\end{split}
\end{equation}
is stable and consistent for the initial value problem
\begin{equation} \label{BVP_IVP}
\begin{split}
L(\VEC{y}(t)) &= \VEC{y}'(t) - A(t)\VEC{y}(t) = f(t) \quad ,
\quad a \leq t \leq b\\
\VEC{y}(a) &= \VEC{y}_c
\end{split} \ .
\end{equation}
\label{CorSC_FDM}
\end{cor}

\begin{proof}
The conclusion follows from Theorem~\ref{Equ_0and1} with
(\ref{LinBVPrepeat}) and (\ref{MFFDM}) as the linear boundary value
problem with its associated finite difference method for $\nu=1$, and
(\ref{BVP_IVP}) and (\ref{FDM_IVP}) as the linear boundary value
problem with its associated finite difference method for $\nu=0$.
Note that (\ref{BVP_IVP}) has a unique solution.
\end{proof}

\subsection{Numerical Aspect of the One-Step Finite Difference
Method for Linear Boundary Value Problems}

If only $\VEC{w}_i$ and $\VEC{w}_{i+1}$ are used in (\ref{MFFDM}), we
say that the method is a one-step finite difference method.

\begin{eggList}
\begin{enumerate}
\item For the midpoint scheme, we have
$\displaystyle C_{i,i} = -\frac{1}{h_i}\,\Id -\frac{1}{2}\,A(t_i+h_i/2)$,
$\displaystyle C_{i,i+1} = \frac{1}{h_i}\,\Id - \frac{1}{2}\,A(t_i+h_i/2)$
and $\displaystyle F_{i,h}(f) = f(t_i+h_i/2)$ for $0 \leq i < N$.
We also have that $C_{i,j} = 0$ otherwise.
\item For the trapezoidal scheme, we have
$\displaystyle C_{i,i} =  -\frac{1}{h_i}\,\Id -\frac{1}{2}\,A(t_i)$,
$\displaystyle C_{i,i+1} = \frac{1}{h_i}\,\Id -\frac{1}{2}\,A(t_i+h)$
\text{and}
$\displaystyle F_{i,h}(f) = \frac{1}{2}\,\left( f(t_i) + f(t_i+h) \right)$
for $0 \leq i < N$.  We also have that $C_{i,j} = 0$ otherwise.
\end{enumerate}
\end{eggList}

If the boundary conditions are separable, namely $B_a^{[b]}=0$ in
(\ref{LinBVPpartSep}) of Section~\ref{SepPartSep}, we can rewrite $A$ as
\begin{equation} \label{sepM}
A = \begin{pmatrix}
B_a^{[a]} & 0 & \ldots & 0 & 0 \\
C_{0,0} & C_{0,1} & \ldots & 0 & 0 \\
0 & C_{1,1} & \ldots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & C_{N-1,N-1} & C_{N-1,N} \\
0 & 0 & \ldots & 0 & B_b^{[b]}
\end{pmatrix} \ ,
\end{equation}
where $B_a^{[a]}$ is a \nm{(n-q)}{n} matrix and $B_b^{[b]}$ is a
\nm{q}{n} matrix.  We also rewrite $\VEC{F}$ to get
\[
\VEC{F} = \begin{pmatrix} \VEC{y}_c^{[a]} \\ F_0(f) \\ \vdots \\
 F_{N-1}(f) \\ \VEC{y}_c^{[b]} \end{pmatrix} \ .
\]

As we can see, the problem now is to solve a large system of linear
equations.  The matrix $A$ in (\ref{sepM}) is a block tridiagonal matrix
of the form
\[
A = \begin{pmatrix}
A_0 & C_0 & 0 &  \ldots & 0 & 0 \\
B_1 & A_1 & C_1 & \ldots & 0 & 0 \\
0 & B_2 & A_2 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & A_{N-1} & C_{N-1} \\
0 & 0 & 0 & \ldots & B_N & A_N
\end{pmatrix} \ ,
\]
where each block is a \nn matrix, the $q$ last rows of the \nn
matrices $B_j$ and the $n-q$ first rows of the \nn matrices
$C_j$ are null.  Moreover, if $A$ is nonsingular then we can express
it as $A = L U$, where
\[
L_h = \begin{pmatrix}
L_{0,0} & 0 & \ldots & 0 & 0 \\
L_{1,0} & L_{1,1} & \ldots & 0 & 0 \\
0 & L_{2,1} &  \ldots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & L_{N,N-1} & L_{N,N}
\end{pmatrix}
\quad \text{and} \quad
U_h = \begin{pmatrix}
U_{0,0} & U_{0,1} & 0 & \ldots & 0 \\
0 & U_{1,1} & U_{1,2} & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & U_{N-1,N} \\
0 & 0 & 0 & \ldots &  U_{N,N}
\end{pmatrix} \ .
\]
The \nn matrices $L_{i,j}$ and $U_{i,j}$ satisfy
\begin{align*}
&\hspace{2.7em}  L_{0,0} U_{0,0} = A_0 \\
& \left.
\begin{array}{r@{\ =\ }l}
L_{i-1,i-1} U_{i-1,i} & C_{i-1} \\
L_{i,i-1} U_{i-1,i-1} & B_i \\
L_{i,i} U_{i,i} & A_i - L_{i,i-1} U_{i-1,i} 
\end{array} 
\right\} \quad , \quad i=1, 2, \ldots, N
\end{align*}

The $LU$ decomposition of $A$ above is not unique.  To determine a
unique $LU$ decomposition, it is standard to set $L_{i,i} = \Id$ for
$0\leq i \leq N$.  We do that below for the case of the partially
separable boundary conditions.  It is proved in \cite{K} that
this decomposition can be obtained using row interchanges on the first
$n-q$ rows, the last $q$ rows, and the $n$ rows between the
$(jn-q+1)^{th}$ and $((j+1)n-q)^{th}$ rows for $1\leq j\leq N$.

Let $\displaystyle \VEC{F} = \begin{pmatrix}
\tilde{\VEC{f}}_0 \\ \tilde{\VEC{f}}_1 \\ \vdots \\
\tilde{\VEC{f}}_N \end{pmatrix}$,
where $\tilde{\VEC{f}}_j \in \RR^n$ for all $j$.
To solve $A \VEC{W} = \VEC{F}$ for $\VEC{W} \in (\RR^n)^{N+1}$, we first solve
$L\VEC{V} = \tilde{\VEC{F}}$ for $\VEC{V} \in (\RR^n)^{N+1}$.  Namely,
we use the forward substitution $L_{0,0}\VEC{v}_0 = \tilde{\VEC{f}}_0$ and
$L_{i,i}\VEC{v}_i = \tilde{\VEC{f}}_i - L_{i,i-1} \VEC{v}_{i-1}$ for $i=1$, $2$,
\ldots, $N$.
Then we solve $U \VEC{W} = \VEC{V}$ for $\VEC{W} \in (\RR^n)^{N+1}$.
Namely, we use the backward substitution $U_{N,N} \VEC{w}_N = \VEC{v}_N$ and
$U_{i,i} \VEC{w}_i = \VEC{v}_i - U_{i,i+1} \VEC{w}_{i+1}$ for $i=N-1$,
$N-2$, \ldots, $0$.

If the boundary condition are only partially separable, namely
$B_a^{[b]} \neq 0$ in (\ref{LinBVPpartSep}) of Section~\ref{SepPartSep},
we can rewrite $A$ as
\begin{equation} \label{part_sepM}
A = \begin{pmatrix}
B_a^{[a]} & 0 & \ldots & 0 & 0 \\
C_{0,0} & C_{0,1} & \ldots & 0 & 0 \\
0 & C_{1,1} & \ldots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & C_{N-1,N-1} & C_{N-1,N} \\
B_a^{[b]} & 0 & \ldots & 0 & B_b^{[b]}
\end{pmatrix} \ ,
\end{equation}
where $B_a^{[a]}$ is a \nm{(n-q)}{n} matrix, and $B_b^{[b]}$ and
$B_a^{[b]}$ are \nm{q}{n} matrices.

$A$ in (\ref{part_sepM}) is of the form
\[
A = \begin{pmatrix}
A_0 & C_0 & 0 & \ldots & 0 & 0 \\
B_1 & A_1 & C_1 & \ldots & 0 & 0 \\
0 & B_2 & A_2 &  \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots  \\
0 & 0 & 0 & \ldots & A_{N-1} & C_{N-1} \\
C_N & 0 & 0 & \ldots & B_N & A_N
\end{pmatrix} \ ,
\]
where each block is a \nn matrix, the $q$ last rows of the \nn
matrices $B_j$ and the $n-q$ first rows of the \nn matrices
$C_j$ are null.  If $A$ is non-singular, we can express it as $A = L
U$, where
\[
L = \begin{pmatrix}
\Id & 0 & \ldots & 0 & 0\\
L_{1,0} & \Id & \ldots & 0 & 0\\
0 & L_{2,1} &\ldots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots \\
L_{N,0} & L_{N,1} & \ldots & L_{N,N-1} & \Id
\end{pmatrix}
\quad \text{and} \quad
U = \begin{pmatrix}
U_{0,0} & U_{0,1} & \ldots & 0 & 0 \\
0 & U_{1,1} & \ldots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & U_{N-1,N-1} & U_{N-1,N} \\
0 & 0 & \ldots & 0 & U_{N,N}
\end{pmatrix} \ .
\]
The \nn matrices $L_{i,j}$ and $U_{i,j}$ satisfy
\begin{align*}
U_{0,0} &= A_0 \\
U_{0,1} &= C_0 \\
L_{N,0} U_{0,0} & = C_N \\
&\hspace{-6em} \left.
\begin{array}{r@{\ = \ }l}
  L_{i,i-1} U_{i-1,i-1} & B_i \\
  U_{i,i} & A_i - L_{i,i-1} U_{i-1,i} \\
  U_{i,i+1} & C_i \\
  L_{N,i} U_{i,i} & - L_{N,i-1}U_{i-1,i}
\end{array}
\right\} \quad , \quad i = 1, 2, \ldots, N-2 \\                    
L_{N-1,N-2} U_{N-2,N-2} &= B_{N-1} \\
U_{N-1,N-1} &= A_{N-1} - L_{N-1,N-2} U_{N-2,N-1} \\
U_{N-1,N} &= C_{N-1} \\
L_{N,N-1} U_{N-1,N-1} &= B_N - L_{N,N-2} U_{N-2,N-1} \\
U_{N,N} &= A_N - L_{N,N-1} U_{N-1,N}
\end{align*}

To solve $A \VEC{W} = \VEC{F}$ for $\VEC{W} \in (\RR^n)^{N+1}$, we first solve
$L\VEC{V} = \tilde{\VEC{F}}$ for $\VEC{V} \in (\RR^n)^{N+1}$ using forward
substitution as we did for the separable boundary conditions case
above; the last substitution is now
$\displaystyle \VEC{v}_N = \tilde{\VEC{f}}_N
-\sum_{j=0}^{N-1}\, L_{N,j} \VEC{v}_j$.
The second step is to solve $U\VEC{W} =\VEC{V}$ for $\VEC{W} \in (\RR^n)^{N+1}$
using backward substitution as for the separable boundary conditions
case above.

As for the separable case, the $LU$ decomposition of $A$ above is not
unique.  To determine a unique $LU$ decomposition, it is standard to
require that $L_{i,i} = \Id$ for $0\leq i \leq N$ as we did above.
It is proved in \cite{K} that this decomposition can be obtained with the same
restrictions on the row interchanges as above if $h$ is small enough,
the linear boundary value problem (\ref{LinBVPrepeat}) with the
boundary conditions expressed as in (\ref{LinBVPpartSep}) has a unique
solution, and
\begin{align*}
L_{i,h}(\VEC{W}) &= \sum_{k=0}^N\,C_{i,k}\,\VEC{w}_k =
F_{i,h}(f) \quad , \quad 0 \leq i < N \\
\VEC{w}_0 &= \VEC{y}_v
\end{align*}
is consistent and stable for the initial value problem
\begin{align*}
L(\VEC{y}(t)) &= \VEC{y}'(t) - A(t)\VEC{y}(t) = f(t) \quad , \quad
a \leq t \leq b \\
\VEC{y}(a) &= \VEC{y}_c
\end{align*}

\begin{code}[One-Step Finite Difference Method for Linear Boundary
Value Problems]
To approximate the solution of the boundary value problem
$y' - A(t) = f(f)$ with $B_a y(a) + B_b y(b) = y_c$  for $a \leq t \leq b$.
We consider the intervals $[t_i,t_{i+1}]$ for $0 \leq i < N$ with
$t_i = a + i h$ and $h = (b-a)/N$.\\
\subI{Input} The vector valued function $F:[a,b] \times \RR \to \RR^n$
(F in the code below).\\
The \nn matrix valued function $C_{i,i}$ defined on $[a,b] \times \RR$
(Ci in the code below).\\
The \nn matrix valued function $C_{i,i+1}$ defined on $[a,b] \times
\RR$ (Cii in the code below).\\
The \nm{(n-q)}{n} matrix $B_a^{[a]}$ (Baa in the code below).\\
The \nm{q}{n} matrix $B_a^{[b]}$ (Bab in the code below).\\
The \nm{q}{n} matrix $B_b^{[b]}$ (Bbb in the code below).\\
The (column) vector $y_c \in R^n$ (yc in the code below).\\
The number $N>2$ of partitions of $[a,b]$.\\
The endpoints $a$ and $b$ of the interval of integration $[a,b]$.\\
\subI{Output} The \nm{n}{(N +1)} matrix ww that contains the approximations
$\VEC{w}_i$ of $\VEC{y}(t_i)$ and the vector tt that contains $t_i$
for $0 \leq i \leq N$.
\small
\begin{verbatim}
function [tt,ww] = linearFDM(F,Ci,Cii,Baa,Bab,Bbb,yc,N,a,b)
  n = length(yc);
  q = size(Bbb,1);
  nmq = n - q;
  h = (b-a)/N;
    
  % We construct the matrix A and the vector F
  A = zeros(n,n,N+1);        % A(:,:,i) = A_{i-1}  for 1 <= i <= N+1
  B = zeros(n,n,N);          % B(:,:,i) = B_i      for 1 <= i <= N
  C = zeros(n,n,N+1);        % C(:,:,i) = C_{i-1}  for 1 <= i <= N+1
  FF = zeros(n,N+1);         % F(:,:,i) = \tilde{f}_{i-1}  for 1 <= i <= N+1

  A(1:nmq,:,1) = Baa;
  C(nmq+1:n,:,N+1) = Bab;
  t = (N-1)*h;
  Civ = Ci(t,h);
  Ciiv = Cii(t+h,h);
  B(1:nmq,:,N) = Civ(q+1:n,:);
  A(1:nmq,:,N+1) = Ciiv(q+1:n,:);
  A(nmq+1:n,:,N+1) = Bbb;
  FF(1:nmq,1) = yc(1:nmq,1);
  tt = [];
  for i=1:1:N
    t = a + (i-1)*h;
    tt = [tt t];
    Civ = Ci(t,h);
    Ciiv = Cii(t+h,h);
    A(nmq+1:n,:,i) = Civ(1:q,:);
    C(nmq+1:n,:,i) = Ciiv(1:q,:);
    B(1:nmq,:,i) = Civ(q+1:n,:);
    A(1:nmq,:,i+1) = Ciiv(q+1:n,:);
    v = F(t,h);
    FF(nmq+1:n,i) = v(1:q,1); 
    FF(1:nmq,i+1) = v(q+1:n,1); 
  end
  FF(nmq+1:n,N+1) = yc(nmq+1:n,1);
  tt = [tt b];

  % We construct the matrices L nad U
  Ud = zeros(n,n,N+1);       % Ud(:,:,i) = U_{i-1,i-1} , 1 <= i <= N+1
  Uu = zeros(n,n,N);         % Uu(:,:,i) = U_{i-1,i} , 1 <= i <= N
  Ll = zeros(n,n,N);         % Ll(:,:,i) = L_{i,i-1} , 1 <= i <= N
  Lr = zeros(n,n,N);         % Lr(:,:i) = L_{N.i-1} ,  1 <= i <= N-1

  Ud(:,:,1) = A(:,:,1);
  Uu(:,:,1) = C(:,:,1);
  % Lr(:,:,1) = C(:,:,N+1)*inv(Ud(:,:,1));
  Lr(:,:,1) = linsolve(Ud(:,:,1)',C(:,:,N+1)')';
  for i=1:1:N-2
    % Ll(:,:,i) = B(:,:,i)*inv(Ud(:,:,i));
    Ll(:,:,i) = linsolve(Ud(:,:,i)',B(:,:,i)')';
    Ud(:,:,i+1) = A(:,:,i+1) - Ll(:,:,i)*Uu(:,:,i);
    Uu(:,:,i+1) = C(:,:,i+1);
    % Lr(:,:,i+1) = -Lr(:,:,i)*Uu(:,:,i)*inv(Ud(:,:,i+1));
    Lr(:,:,i+1) = -linsolve(Ud(:,:,i+1)', Uu(:,:,i)'*Lr(:,:,i)')';
  end
  % Ll(:,:,N-1) = B(:,:,N-1)*inv(Ud(:,:,N-1));
  Ll(:,:,N-1) = linsolve(Ud(:,:,N-1)',B(:,:,N-1)')';
  Ud(:,:,N) = A(:,:,N) - Ll(:,:,N-1)*Uu(:,:,N-1);
  Uu(:,:,N) = C(:,:,N);
  % Ll(:,:,N) = (B(:,:,N) - Lr(:,:,N-1)*Uu(:,:,N-1))*inv(Ud(:,:,N));
  Ll(:,:,N) = linsolve(Ud(:,:,N)',(B(:,:,N) - Lr(:,:,N-1)*Uu(:,:,N-1))')';
  Ud(:,:,N+1) = A(:,:,N+1) - Ll(:,:,N)*Uu(:,:,N);

  % We now solve the system A W = F
  % First, we solve L V = F
  V = zeros(n,N+1);
  V(:,1) = FF(:,1);
  for i=2:1:N+1
    V(:,i) = FF(:,i) - Ll(:,:,i-1)*V(:,i-1);
  end
  for i=1:1:N-1
    V(:,N+1) = V(:,N+1) - Lr(:,:,i)*V(:,i);
  end
    
  % Second, we solve U W = V
  W = zeros(n,N+1);
  % W(:,N+1) = inv(Ud(:,:,N+1))*V(:,N+1);
  W(:,N+1) = linsolve(Ud(:,:,N+1),V(:,N+1));
  for i=N:-1:1
    % W(:,i) = inv(Ud(:,:,i))*(V(:,i) - Uu(:,:,i)*W(:,i+1));
    W(:,i) = linsolve(Ud(:,:,i),V(:,i) - Uu(:,:,i)*W(:,i+1));
  end
  ww = W;
end
\end{verbatim}
\end{code}

\begin{egg}[Example~\ref{eggShootCode1} Continued]
Recall that the boundary value problem was
$\VEC{y}'(t) = A(t) \VEC{y}(t) + f(t)$ with
$B_a \VEC{y}(0) + B_b\VEC{y}(1) = \VEC{y}_c$, where
\[
\VEC{y} = \begin{pmatrix} y_1(t) \\ y_2(t) \end{pmatrix} \ ,
\ A(t) = \begin{pmatrix} 0 & 1 \\ 4 & 0 \end{pmatrix} \ ,
\ f(t) = \begin{pmatrix} 0 \\ -3 e^t \end{pmatrix} \ ,
\ B_a = \begin{pmatrix} 1 & 0 \\  0 & 0 \end{pmatrix} \ ,
\ B_b = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} \ \text{and}
\ \VEC{y}_c = \begin{pmatrix} 1 \\ e \end{pmatrix} \ .
\]
We use the previous code with the trapezoidal method to numerically
solve the boundary value problem.  We need to set
\begin{align*}
C_{i,i} &=  -\frac{1}{h}\,\Id -\frac{1}{2}\,A(t_i)
= \begin{pmatrix} -1/h & -1/2 \\ -2 & -1/h \end{pmatrix} \ ,
\ C_{i,i+1} = \frac{1}{h}\,\Id -\frac{1}{2}\,A(t_i+h)
= \begin{pmatrix} 1/h & -1/2 \\ -2 & 1/h \end{pmatrix} \ , \\
F_{i,h}(f) &= \frac{1}{2}\,\left( f(t_i) + f(t_i+h) \right)
= \begin{pmatrix} 0 \\ -3 \left(e^{t_1} + e^{t_i+h}\right)/2 \end{pmatrix} \ ,
\ B_a^{[a]} = \begin{pmatrix} 1 & 0 \end{pmatrix} \ ,
\ B_a^{[b]} = \begin{pmatrix} 0 & 0 \end{pmatrix} \ , \\
B_b^{[b]} &= \begin{pmatrix} 1 & 0 \end{pmatrix}
\quad \text{and}
\quad \VEC{y}_c = \begin{pmatrix} 1 \\ e^1 \end{pmatrix} \ .
\end{align*}
If we use the code above with $N=100$, we find the following approximations of
the solution.
\[
\begin{array}{llllc@{\hspace{2em}}llll}
i & t_i & w_{1,i} & w_{2,i} & & i & t_i & w_{1,i} & w_{2,i} \\
\cline{1-4} \cline{6-9}
0 & 0 & 1 & 0.9999829 && 90 & 0.90 & 2.4596020 & 2.4595917 \\
1 & 0.01 & 1.0100501 & 1.0100332 && 91 & 0.91 & 2.4843215 & 2.4843112 \\
2 & 0.02 & 1.0202012 & 1.0201844 && 92 & 0.92 & 2.5092895 & 2.5092793 \\
3 & 0.03 & 1.0304543 & 1.0304377 && 93 & 0.93 & 2.5345084 & 2.5344983 \\
4 & 0.04 & 1.0408104 & 1.0407940 && 94 & 0.94 & 2.5599807 & 2.5599707 \\
5 & 0.05 & 1.0512707 & 1.0512544 && 95 & 0.95 & 2.5857091 & 2.5856991 \\
6 & 0.06 & 1.0618361 & 1.0618199 && 96 & 0.96 & 2.6116960 & 2.6116861 \\
7 & 0.07 & 1.0725076 & 1.0724916 && 97 & 0.97 & 2.6379449 & 2.6379343 \\
8 & 0.08 & 1.0832864 & 1.0832706 && 98 & 0.98 & 2.6644560 & 2.6644463 \\
9 & 0.09 & 1.0941736 & 1.0941578 && 99 & 0.99 & 2.6912343 & 2.6912248 \\
10 & 0.10 & 1.1051701 & 1.1051545 && 100 & 1.00 & 2.7182818 & 2.7182723 \\
\vdots & \vdots & \vdots & \vdots && & & & \\
\end{array}
\]
where $w_{1,i} \approx y_{1,i} = y_1(t_i)$ and
$w_{2,i} \approx y_{2,i} = y_2(t_i)$ for all $i$.
All the approximations have at least $5$-digit accuracy.  These results
are not as good as those that we found with the parallel
shooting method but we have to keep in mind that the trapezoidal
method is of order $2$ while the classical forth order Runge-Kutta
method that we have used for the parallel shooting if of order four.
There are finite difference schemes that can give better results.
However, those schemes will generally not be one-step scheme and,
therefore, the matrix $A$ will not be as nice as the one that we have
for one-step schemes.

Here is the code used to call the finite difference method.
\begin{code}
\small
\begin{verbatim}
  format long
  F = @(t,h) [ 0 ; -3*(exp(t)+exp(t+h))/2 ];
  Ci = @(t,h) [ -1/h -1/2 ; -2 -1/h];
  Cii = @(t,h) [ 1/h -1/2 ; -2  1/h];
  Baa = [ 1 0 ];
  Bab = [ 0 0 ];
  Bbb = [ 1 0 ];
  yc = [ 1 ; exp(1) ];
  N = 100;
  [t,w] = linearFDM(F,Ci,Cii,Baa,Bab,Bbb,yc,N,0,1)
\end{verbatim}
\end{code}
\end{egg}

Unfortunately, the trapezoidal method cannot be used to numerically
solve the boundary value problem of Example~\ref{eggShootCode2}.  The
matrix $A$ generated by this method is singular.  Other finite
difference schemes must be used.  We will not develop finite difference
methods with more than one-step.  This is left to the adventurous
readers.

\subsection{Finite Difference Methods for Non-Linear Boundary Value
Problems}

We consider finite difference methods of the form (\ref{GFFDM}) that
may be used to approximate the solution of a boundary value problem of
the form (\ref{NLBVPreapeat}).

In this subsection, we first study the stability of finite difference
methods like (\ref{GFFDM}) for boundary value problems like
(\ref{NLBVPreapeat}).  At the end, we will give a constructive proof
of the existence of a numerical approximation to the solution of
(\ref{NLBVPreapeat}).

Consider the following linear boundary value problem obtained from the
linearisation of (\ref{NLBVPreapeat}).
\begin{equation} \label{LinGBVP}
\begin{split}
L^{[\VEC{y}]}(\VEC{u}(t)) = \VEC{u}'(t) -
\diff_{\VEC{y}}f(t,\VEC{y}(t)) \VEC{u}(t) &= \VEC{0} \quad , \quad
a \leq t \leq b  \\
\diff_{\VEC{y}_1} g(\VEC{y}(a),\VEC{y}(b)) \VEC{u}(a) +
\diff_{\VEC{y}_2} g(\VEC{y}(a),\VEC{y}(b)) \VEC{u}(b) &= \VEC{0}
\end{split}
\end{equation}
where $\VEC{y}$ is the solution of the boundary value problem
(\ref{NLBVPreapeat}).

We also consider the finite difference method obtained from the
linearisation of (\ref{GFFDM}); namely,
\begin{equation} \label{LinGFFDM}
\begin{split}
L^{[\VEC{W}]}_{i,h} (\VEC{U}) =
\sum_{k=0}^N\,C_{i,k}(\VEC{W})\,\VEC{u}_k &= \VEC{0} \quad , \quad
0 \leq i < N \\
B_a(\VEC{W}) \,\VEC{u}_0 + B_b(\VEC{W})\,\VEC{u}_n &= \VEC{0}
\end{split}
\end{equation}
where
\begin{align*}
C_{i,k}(\VEC{W})  &= \diff_{\VEC{y}_k} P_{i,h}(\VEC{Y})\bigg|_{\VEC{Y}=\VEC{W}}
\ , \quad
B_a(\VEC{W}) = \diff_{\VEC{y}_1} g(\VEC{y}_0,\VEC{y}_N)\bigg|_{\VEC{Y}=\VEC{W}}
\ ,  \\
B_b[\VEC{W}) &= \diff_{\VEC{y}_2} g(\VEC{y}_0,\VEC{y}_N)\bigg|_{\VEC{Y}=\VEC{W}}
\ , \quad
\VEC{W} = \begin{pmatrix} \VEC{w}_0 \\
\VEC{w}_1 \\ \vdots \\ \VEC{w}_N \end{pmatrix} \in (\RR^n)^{N+1} \quad
\text{and} \quad
\VEC{U} = \begin{pmatrix} \VEC{u}_0 \\
\VEC{u}_1 \\ \vdots \\ \VEC{u}_N \end{pmatrix} \in (\RR^n)^{N+1} \ .
\end{align*}

let
\[
A(\VEC{Z}) = \begin{pmatrix}
B_a(\VEC{Z}) & 0 & \ldots & B_b(\VEC{Z}) \\
C_{0,0}(\VEC{Z}) & C_{1,1}(\VEC{Z}) & \ldots & C_{0,N}(\VEC{Z}) \\
\vdots & \vdots & \ddots & \vdots \\
C_{N-1,0}(\VEC{Z}) & C_{N,1}(\VEC{Z}) & \ldots & C_{N-1,N}(\VEC{Z})
\end{pmatrix} \quad \text{with} \quad
\VEC{Z} = \begin{pmatrix} \VEC{z}_0 \\ \VEC{z}_1\\
\vdots \\ \VEC{z}_N \end{pmatrix} \in (\RR^n)^{N+1} \ .
\]

\begin{theorem}
Suppose that $\VEC{y}$ is an isolate solution of (\ref{NLBVPreapeat}).
Let $\{t_i\}_{i=0}^N$ be a partition of $[a,b]$ satisfying our
standard conditions, and let
$\displaystyle \VEC{Y} = \begin{pmatrix} \VEC{y}_0 \\
\VEC{y}_1 \\ \vdots \\ \VEC{y}_N \end{pmatrix}$,
where $\VEC{y}_i = \VEC{y}(t_i)$ for $0\leq i \leq N$.
Suppose that the finite difference method
\begin{equation} \label{MforLinIVP}
\begin{split}
L^{[\VEC{Y}]}_{i,h} (\VEC{U}) &= \VEC{0} \quad , \quad 0 \leq i < N \\
\VEC{u}_0 &= \VEC{y}_c \in \RR^n
\end{split}
\end{equation}
is stable and consistent for the initial value problem
\begin{equation} \label{LinIVP}
\begin{split}
L^{[\VEC{y}]}(\VEC{u}(t)) &= \VEC{0} \quad , \quad a \leq t \leq b \\
\VEC{u}(a) &= \VEC{y}_c \in \RR^n
\end{split}
\end{equation}
Suppose that $L^{[\VEC{W}]}_{i,h}$ is Lipschitz continuous
with respect to $\VEC{W}$ in a neighbourhood of $\VEC{Y}$; namely,
there exist constants $\delta >0$, $K_L>0$ and $h_0 > 0$ such that
\begin{equation} \label{LipFDM}
\| L^{[\VEC{W}]}_{i,h} - L^{[\tilde{\VEC{W}}]}_{i,h} \|_\infty
\leq K_L \| \VEC{W} - \tilde{\VEC{W}} \|_\infty
\end{equation}
for all $\VEC{W}$ and $\tilde{\VEC{W}}$ in
\[
S_\delta(\VEC{Y}) = \left\{ \VEC{Z} \in (\RR^n)^{N+1} :
\| \VEC{z}_i - \VEC{y}_i \|_\infty < \delta
\quad \text{for} \quad 0 \leq i \leq N \right\}
\]
and all $h < h_0$.  Moreover, in this context, suppose that
\begin{equation} \label{Cont_BC}
\begin{split}
& \max \left\{ \| B_a(\VEC{W}) - B_a(\tilde{\VEC{W}}) \|_\infty ,
\| B_b(\VEC{W}) - B_b(\tilde{\VEC{W}}) \|_\infty \right\} \\
&\qquad \leq \frac{K_L}{2} \max \left\{\| \VEC{w}_0-\tilde{\VEC{w}}_0 \|_\infty,
\| \VEC{w}_N-\tilde{\VEC{w}}_N \|_\infty \right\}
\end{split}
\end{equation}
for all $\VEC{W}, \tilde{\VEC{W}} \in S_\delta(\VEC{Y})$ and all $h < h_0$.
Then, if $\delta$ is small enough, $A_h(\VEC{Z})$ has a uniformly
bounded inverse for all $\VEC{Z} \in S_\delta(\VEC{Y})$ and $h$
small enough.

Moreover, \ref{LinGFFDM} is stable for the linear boundary value problem
\ref{LinGBVP} \footnotemark.
\label{FDM_stable}
\end{theorem}

\footnotetext{It can be shown that this implies that (\ref{GFFDM}) is
stable for the nonlinear boundary value problem (\ref{NLBVPreapeat}).}

\begin{proof}
Since (\ref{MforLinIVP}) is stable and consistent for (\ref{LinIVP}),
we get from Corollary~\ref{CorSC_FDM} that
(\ref{LinGFFDM}) is stable and consistent for (\ref{LinGBVP}).
It follows from
Proposition~\ref{BasicFDM} that there exist $h_1 >0$ and $K>0$ such
that $\displaystyle \left(A(\VEC{Y})\right)^{-1}$ exists and
$\displaystyle \| \left(A(\VEC{Y})\right)^{-1} \| \leq K$ for
$h < h_1$.  We may assume that $h_0 < h_1$ by shrinking $h_0$ if
necessary.

Hence, if $\VEC{Z}$ is closed enough to $\VEC{Y}$, (\ref{LipFDM}) and
(\ref{Cont_BC}) imply that 
$A(\VEC{Z})$ is as closed as we want of the invertible matrix
$A(\VEC{Y})$ independently of $h < h_0$.  If we choose $\delta_0$
small enough to have
$\displaystyle \left\|A(\VEC{Y}) - A(\VEC{Z})\right\|_\infty
\, \left\| (A(\VEC{Y}))^{-1}\right\|_\infty < 1/2$ for $\delta < \delta_0$,
then it follows from the Banach Lemma
that $\displaystyle \left(A(\VEC{Z})\right)^{-1}$ exists.  Moreover, from
\begin{align*}
\| \left(A(\VEC{Z})\right)^{-1}\|_\infty
- \|\left(A(\VEC{Y})\right)^{-1}\|_\infty
& \leq \| \left(A(\VEC{Z})\right)^{-1} - \left(A(\VEC{Y})\right)^{-1}\|_\infty
\\
&= \| \left(A(\VEC{Y})\right)^{-1}
\left( A(\VEC{Y}) - A(\VEC{Z}) \right)
\left(A(\VEC{Z})\right)^{-1}\|_\infty \\
& \leq \underbrace{\| \left(A(\VEC{Y})\right)^{-1} \|_\infty \,
\|\left( A(\VEC{Y}) - A(\VEC{Z}) \right)\|_\infty}_{<1/2} \,
\|\left(A(\VEC{Z})\right)^{-1}\|_\infty \ ,
\end{align*}
we get
\[
\| \left(A(\VEC{Z})\right)^{-1} \|_\infty \leq
\frac{\|\left(A(\VEC{Y})\right)^{-1}\|_\infty}
{1-\|\left(A(\VEC{Y})\right)^{-1}\|_\infty\,
\|A(\VEC{Y})- A(\VEC{Z})\|_\infty}
\leq 2 \|\left(A(\VEC{Y})\right)^{-1}\|_\infty
\]
for all $\VEC{Z} \in S_\delta(\VEC{Y})$ with $h < h_0$ and
$\delta < \delta_0$.  We could have used Corollary~\ref{Banach_corG}
to directly draw the previous conclusion.  So
$\displaystyle \left(A(\VEC{Z})\right)^{-1}$
is uniformly bounded for $h$ and $\delta$ small enough.

Let
\begin{equation} \label{big_psi}
\Psi(\VEC{W}) =\begin{pmatrix}
g(\VEC{w}_0,\VEC{w}_N) \\
P_{0,h}(\VEC{W}) \\
\vdots \\
P_{N-1,h}(\VEC{W}) \\
\end{pmatrix} \ .
\end{equation}
Then
\begin{equation} \label{FDM_int}
\Psi(\VEC{Z}) - \Psi(\tilde{\VEC{Z}}) = A(\VEC{Z},\tilde{\VEC{Z}})
\left(\VEC{Z} - \tilde{\VEC{Z}} \right) \ ,
\end{equation}
where
\[
A(\VEC{Z}, \tilde{\VEC{Z}}) \equiv \int_0^1\,
A(s \VEC{Z} + (1-s)\tilde{\VEC{Z}})\,\dx{s}
\]
since $P_{i,h}$ and $g$ are assumed to be continuously
differentiable, and
$\displaystyle A(\VEC{W})
= \diff_{\VEC{Z}} \Psi(\VEC{Z})\big|_{\VEC{Z} = \VEC{W}}$.

Moreover, from (\ref{LipFDM}) and (\ref{Cont_BC}), we get
\begin{align*}
&\| A(\VEC{Z},\tilde{\VEC{Z}}) - A(\VEC{Y}) \|_\infty \\
&\qquad \leq
\max \big\{ B_a(\VEC{W}) - B_a(\tilde{\VEC{W}}) \|_\infty +
\| B_b(\VEC{W}) - B_b(\tilde{\VEC{W}}) \|_\infty,
\max_{0\leq i <N}
\| L^{[\VEC{W}]}_{i,h} - L^{[\tilde{\VEC{W}}]}_{i,h} \|_\infty \big\}
\leq K_L \delta
\end{align*}
for $\VEC{Z}$ and $\tilde{\VEC{Z}}$ in
$S_\delta(\VEC{Y})$ and $h<h_0$.  Thus, if $\delta$ is small enough to have
\[
  \| A(\VEC{Z},\tilde{\VEC{Z}}) - A(\VEC{Y}) \|_\infty\,
\left\|(A(\VEC{Y}))^{-1}\right\|_\infty
\leq \delta K_L K < 1 \ ,
\]
then it follows from the Banach Lemma that
$\displaystyle \left(A(\VEC{Z},\tilde{\VEC{Z}})\right)^{-1}$
exists and, as we have shown above for $\left(A(\VEC{Z})\right)^{-1}$,
\[
\| \left(A(\VEC{Z},\tilde{\VEC{Z}})\right)^{-1} \|_\infty \leq
\frac{\|\left(A(\VEC{Y})\right)^{-1}\|_\infty}
{1-\|\left(A(\VEC{Y})\right)^{-1}\|_\infty\,
\|A(\VEC{Y}) - A(\VEC{Z},\tilde{\VEC{Z}})\|_\infty}
\leq \frac{K}{1-\delta K_L K}
\]
for $\VEC{Z}$ and $\tilde{\VEC{Z}}$ in $S_\delta(\VEC{Y})$
with $\delta$ and $h$ small enough.

The stability of (\ref{GFFDM}) follows from
\[
\left(\VEC{Z} - \tilde{\VEC{Z}} \right) =
\left(A(\VEC{Z},\tilde{\VEC{Z}})\right)^{-1}
\left( \Psi(\VEC{Z}) - \Psi(\tilde{\VEC{Z}}) \right)
\]
by taking the norm on both sides and using the uniform upper bound on
$\left(A(\VEC{Z},\tilde{\VEC{Z}})\right)^{-1}$ for 
$\VEC{Z}$ and $\tilde{\VEC{Z}}$ in $S_\delta(\VEC{Y})$
with $\delta$ and $h$ small enough.
\end{proof}

We now show how we can use the Newton Method to find an approximation
of the solution of (\ref{NLBVPreapeat}) if
$\VEC{Z}^{[0]} \in S_\delta(\VEC{Y})$ is chosen appropriately, where
$\delta$ is given in the previous theorem.
More precisely, we show that if $h$ and $\delta_0 < \delta$ are small
enough and $\VEC{Z}^{[0]} \in S_{\delta_0}(\VEC{Y})$, then the sequence
$\{ \VEC{Z}^{[k]} \}_{k=0}^\infty$ defined by
\begin{equation} \label{FDM_Newton}
A(\VEC{Z}^{[k]}) \left( \VEC{Z}^{[k+1]}- \VEC{Z}^{[k]}\right)
= - \Psi(\VEC{Z}^{[k]}) \quad , \quad k=0, 1, 2, \ldots
\end{equation}
stays in $S_{\delta_0}(\VEC{Y})$ and converges toward a solution
$\VEC{W}$ of (\ref{GFFDM}).

We can rewrite (\ref{FDM_Newton}) as
\begin{align*}
L^{[\VEC{Z}^{[k]}]}_{i,h}\left( \VEC{Z}^{[k+1]}- \VEC{Z}^{[k]}\right)
&= - P_{i,h}(\VEC{Z}^{[k]}) \quad , \quad 0 \leq i < N \\
B_a(\VEC{Z}^{[k]}) \left( \VEC{z}_0^{[k+1]}- \VEC{z}_0^{[k]}\right)
+ B_b(\VEC{Z}^{[k]}) \left( \VEC{z}_N^{[k+1]} - \VEC{z}_N^{[k]}\right)
&= -g(\VEC{z}_0^{[k]}, \VEC{z}_N^{[k]})
\end{align*}
for $k=0$, $1$, $2$, \ldots

The following theorem will be useful shortly.  A proof of this theorem
can be found in \cite{O}.

\begin{theorem}[Newton-Kantorovich]
Suppose that $\phi:\RR^n \rightarrow \RR^n$ is a sufficiently
differentiable function and let
$Q(\VEC{x}) = \diff_{\VEC{x}}\phi(\VEC{x})$.
Suppose that there exists $\gamma$ such that
\begin{equation} \label{inequ_3}
\| Q(\VEC{x}) - Q(\tilde{\VEC{x}}) \|_\infty \leq
\gamma \| \VEC{x} - \tilde{\VEC{x}} \|_\infty  
\end{equation}
for all $\displaystyle \VEC{x}, \tilde{\VEC{x}}$ in an open convex set
$D \subset \RR^n$.  Suppose also that, for some $\VEC{x}_0 \in D$,
there exist constants $\alpha$ and $\beta$ such that
\begin{align}
\| Q^{-1}(\VEC{x}_0) \|_\infty &\leq \beta \ , \label{inequ_2} \\
\| Q^{-1}(\VEC{x}_0) \phi(\VEC{x}_0) \|_\infty &\leq \alpha \ ,\label{inequ_1}
\intertext{and}
\alpha\beta\gamma &< \frac{1}{2} \label{inequ_4} \ .
\end{align}
let
\[
  \delta_\pm = \frac{1 \pm \sqrt{1-2\alpha\beta\gamma}}{\beta\gamma} \ .
\]
If $S_{\delta_-}(\VEC{x}_0)
= \{ \VEC{x} : \|\VEC{x} - \VEC{x_0}\|_\infty < \delta_- \} \subset D$,
then, the sequence $\{ \VEC{x}_k \}_{k=0}^\infty$ generated by
\[
Q(\VEC{x}_k)\left( \VEC{x}_{k+1} - \VEC{x}_k \right) =
- \phi(\VEC{x}_k) \quad , \quad k=0, 1, 2, \ldots
\]
remains in $S_{\delta_-}(\VEC{x}_0)$ for all $k$ and converges 
quadratically to the unique root of $\phi$ in
$S_{\delta_+}(\VEC{x}_0) \cap D$.  \label{NewtKantTheorem}
\end{theorem}

If $D$ is very large so that $S_{\delta_-}(\VEC{x}_0) \subset D$ is
``almost'' always satisfied, then the previous theorem does
not require the explicit knowledge of the exact root of $\phi$ to
determine conditions to get a converging sequence
$\displaystyle \{ \VEC{x}^{[k]}\}_{k=0}^\infty$ to a root of $\phi$.

\begin{theorem}
Suppose that all the hypothesis of Theorem~\ref{FDM_stable} are
satisfied.  Suppose that the local truncation error of (\ref{GFFDM})
with respect to (\ref{NLBVPreapeat}) is of order $p>0$.  Then, there exist
$0 <\delta_0 < \delta$ ($\delta$ given Theorem~\ref{FDM_stable})
and $h_0 >0$ such that (\ref{GFFDM}) has a solution $\VEC{W}$ in
$S_\delta(\VEC{Y})$ if $h\leq h_0$.  The Newton Method
(\ref{FDM_Newton}) with $\VEC{Z}^{[0]}$ such that
$\displaystyle \VEC{Z}^{[0]} \in S_{\delta_0}(\VEC{Y})$
can be used to approximate this solution.  The convergence is quadratic.
\end{theorem}

\begin{proof}
We prove that the hypotheses of Newton-Kantorovich Theorem are
satisfied.  We replace $\VEC{x}_k$ by $\VEC{Z}^{[k]}$, $\phi$
by $\Psi$, $Q(\VEC{Z})$ by $\displaystyle A(\VEC{Z})
= \diff_{\VEC{W}} \Psi(\VEC{W})\big|_{\VEC{W} = \VEC{Z}}$ and
$D$ by
$S_\delta(\VEC{Y}) = \{ \VEC{Z} : \|\VEC{Z} - \VEC{Y}\|_\infty < \delta \}$
in Newton-Kantorovich Theorem, where $\VEC{Y}$ and $\delta$ are given
Theorem~\ref{FDM_stable}.

From (\ref{LipFDM}) and (\ref{Cont_BC}), we have that
\[
\| A(\VEC{W}) - A(\tilde{\VEC{W}}) \| \leq K_L
\| \VEC{W} - \tilde{\VEC{W}} \|_\infty
\]
for all $\VEC{W}$ and $\tilde{\VEC{W}}$ in $S_\delta(\VEC{Y})$
for $\delta$ given in the statement of Theorem~\ref{FDM_stable}.
So (\ref{inequ_3}) is satisfied with with $\gamma = K_L$.

Suppose that $\delta_0 < \delta$.  We will precise the value of
$\delta_0$ later.  Let $\VEC{Z}^{[0]}$ be any element in 
$S_{\delta_0}(\VEC{Y})$.

Proceeding as in the proof of Theorem~\ref{FDM_stable}, we have that
$\| A_h^{-1}(\VEC{Z},\tilde{\VEC{Z}}) \|_\infty \leq K/(1-\delta_0 K_L K)$
for $\VEC{Z} , \tilde{\VEC{Z}} \in S_{\delta_0}(\VEC{Y})$ if
$h$ is small enough and $\delta_0 < \delta$.  Recall that
$\delta K_L K < 1$.  If we take
$\VEC{Z} = \tilde{\VEC{Z}} = \VEC{Z}^{[0]}$, we get that 
(\ref{inequ_2}) is satisfies with $\beta = K/(1-\delta_0 K_L K)$;
namely,  $\| A^{-1}(\VEC{Z}^{[0]}) \|_\infty \leq \beta$.

Since
\[
A^{-1}(\VEC{Z}^{[0]}) A(\VEC{Z}^{[0]}, \VEC{Y}) = Id
+ A^{-1}(\VEC{Z}^{[0]}) \left( A(\VEC{Z}^{[0]},\VEC{Y})
- A(\VEC{Z}^{[0]})\right) \ ,
\]
we get
\[
\| A^{-1}(\VEC{Z}^{[0]}) A(\VEC{Z}^{[0]},\VEC{Z}) \|_\infty
\leq 1 + \left(\frac{K}{1-\delta_0 K_L K}\right) K_L \delta_0
\]
for $\VEC{Z}^{[0]} \in S_{\delta_0}(\VEC{Y})$.
Moreover, from (\ref{FDM_int}), we get
\[
A^{-1}(\VEC{Z}^{[0]}) \Psi(\VEC{Z}^{[0]}) =
A^{-1}(\VEC{Z}^{[0]}) \left( \Psi(\VEC{Y}) +
A(\VEC{Z}^{[0]},\VEC{Y}) \left( \VEC{Z}^{[0]}
- \VEC{Y} \right) \right) \ .
\]
Thus,
\[
\| A^{-1}(\VEC{Z}^{[0]}) \Psi(\VEC{Z}^{[0]}) \|_\infty
\leq \left(\frac{K}{1-\delta_0 K_L K}\right) K_0 h^p
+ \left( 1 + \left(\frac{K}{1-\delta_0 K_L K}\right) K_L\delta_0
\right) \delta_0
\]
for some constant $K_0$ and $h$ small enough.  The factor
$K_0h^p$ comes from the assumption that the method is of order $p$.
Thus (\ref{inequ_1}) is satisfied with 
$\displaystyle \alpha = \left(\frac{K}{1-\delta_0 K_L K} \right) K_0 h^p
+ \left( 1 + \left(\frac{K}{1-\delta_0 K_L K}\right)K_L\delta_0
\right) \delta_0$.

We need to choose $\delta_0$ and $h$ small enough to satisfy
(\ref{inequ_4}); namely,
\[
\alpha\beta\gamma = \left( \left(\frac{K}{1-\delta_0 K_L K} \right) K_0 h^p
+ \left( 1 + \left(\frac{K}{1-\delta_0 K_L K}\right) K_L \delta_0
\right) \delta_0 \right) \left(\frac{K}{1-\delta_0 K_L K}\right) K_L
< \frac{1}{2}
\]
We also need to choose $\delta_0$ small enough to have
$S_{\delta_-}(\VEC{Z}^{[0]}) \subset B_\delta(\VEC{Y})$ to be able to
apply Newton-Kantorovich Theorem.

First, we may assume that $K K_L$ is large enough to have
$1/(K K_L) < \delta/4$.  Hence,
\begin{equation} \label{NKappl2}
\delta_- < \delta_+ = \frac{1 + \sqrt{1 -2\alpha\beta\gamma}}{\beta\gamma}
< \frac{2}{\beta \gamma} < \frac{2}{K K_L} < \frac{\delta}{2} \ .
\end{equation}

Moreover, we may assume that $\delta_0$ is small enough to have
$\delta_0 K_L K < 1/2$.  Then, we select $h$ such that
\begin{equation} \label{NKappl1}
\left(\frac{K}{1-\delta_\delta K_L K} \right)^2 K_L K_0 h^p
< 4 K^2 K_L K_0 h^p < \frac{1}{4} \ .
\end{equation}
We choose $\delta_o$ small enough to have
\[
\delta_0 \left( 1 + \left(\frac{K}{1-\delta_0 K_L K}\right) K_L \delta_0
\right) \left(\frac{K}{1-\delta_0 K_L K}\right) K_L
< 2 \delta_0 \left( 1 + 2 K K_l\delta_0\right) K K_L
  < \frac{1}{4} \ .
\]
Combine with (\ref{NKappl1}), this implies that (\ref{inequ_4}) is
satisfied.

Finally, we choose $\delta_0$ small enough to have
\[
  2 \delta_0 K K_L < 1 - \sqrt{\displaystyle 1 - 2 K^2 K_L K_0 h^p} \ .
\]
We then have that
\[
\delta_0 < \frac{1 - \sqrt{\displaystyle 1 - 2 K^2 K_L K_0 h^p}}{2 K K_L}
\leq \frac{1 - \sqrt{1 - 2\alpha\beta\gamma}}{\beta\gamma} = \delta_-\ .
\]
If follow from (\ref{NKappl2}) that $\delta_0 < \delta_- < \delta/2$.
Thus for any $\displaystyle \VEC{Z}^{[0]} \in S_{\delta_0}(\VEC{Y})$, we have
$\displaystyle S_{\delta_-}\left(\VEC{Z}^{[0]}\right) \subset
S_\delta(\VEC{Y})$ as required.
\end{proof}

\subsection{Collocation and Implicit Runge-Kutta}

We consider a simple case to illustrate how collocation and Runge-Kutta
methods can be use to develop method to solve boundary value problems.

In this subsection, we consider the partition
$a = t_0 < t_1 < \ldots < t_N = b$ of the interval $[a,b]$ with
$t_{i+1}-t_i = h_i$ for $i=0$, $1$,\ldots,$N-1$.  Let
$0 \leq \theta_0 < \theta_1 < \ldots < \theta_{J-1} < \theta_J
\leq 1$.  We subdivide each interval
$[t_i,t_{i+1}]$ with a partition
$t_i \leq t_{i,0} < t_{i,1} < \ldots < t_{i,J-1} < t_{i,J} \leq t_{i+1}$
where $t_{i,j} = t_i + \theta_jh_i$ for $j=0$, $1$, \ldots, $J$.

From now on, we assume that $\theta_0=0$ and $\theta_J=1$ to
simplify the presentation.

We approximate the solution $\VEC{y}$ of (\ref{NLBVPreapeat}) on
$[t_i,t_{i+1}]$ by a polynomial mapping $\VEC{p}_i(t)$ of degree
$J+1$ such that
\begin{align}
\VEC{p}_i'(t_{i,j}) &= f(t_{i,j},\VEC{p}_i(t_{i,j})) \quad ,
\quad 0 \leq i < N \ \text{and}\ 0 \leq j \leq J \label{coll_1} \\
\VEC{0} &= g(\VEC{p}_0(t_{0,0}),\VEC{p}_{N-1}(t_{N-1,J})) \label{coll_2} \\
\VEC{p}_i(t_{i,0}) &= \VEC{p}_{i-1}(t_{i-1,J}) \quad , \quad
0 < i < N \label{coll_3}
\end{align}

Condition (\ref{coll_3}) implies that $\VEC{p}:[a,b]\to \RR^n$ defined by
$\VEC{p}(t) = \VEC{p}_i(t)$ for $t_i \leq t \leq t_{i+1}$ is a
piecewise continuous polynomial mapping.

(\ref{coll_1}) and (\ref{coll_3}) are exactly the conditions that we
have used with the collocation method to derive implicit Runge-Kutta
Method in Section~\ref{DeRKMCollMeth}.

If we use Proposition~\ref{entries_Butcher}, in particular
(\ref{polColRK}), we get
\[
\VEC{p}(t_{i,j}) = \VEC{p}(t_{i,0}) + h_i\,\sum_{m=0}^J\, \beta_{j,m}\,K_{i,m}
\quad , \quad 0 \leq i \leq N \ \text{and} \ 0 \leq j \leq J \ ,
\]
where
\[
K_{i,m} = f(t_{i,m}, \VEC{p}(t_{i,m})) \quad , \quad
0 \leq i \leq N \ \text{and} \ 0 \leq m \leq J \ ,  
\]
and
\[
\beta_{j,m} = \int_{\theta_0}^{\theta_j}\,\ell_m(\theta)\dx{\theta}
= \int_{\theta_0}^{\theta_j}
\left(\prod_{\substack{k=0\\k\neq m}}^J\frac{\theta -
\theta_k}{\theta_m-\theta_k}\right) \dx{\theta}
\quad , \quad  0\leq j,m \leq J \ .
\]

The solution $\VEC{y}$ of (\ref{NLBVPreapeat}) may therefore be
approximated by the scheme
\begin{align}
\VEC{w}_{i,j} &= \VEC{w}_{i,0} + h_i \, \sum_{m=0}^J\,
\beta_{j,m}f(t_{i,m},\VEC{w}_{i,m}) \quad , \quad
0 \leq i < N \ \text{and} \  0 \leq j \leq J
\label{FDM_RK_1} \\
\VEC{0} &= g(\VEC{w}_{0,0} ,\VEC{w}_{N-1,J}) \label{FDM_RK_2}
\end{align}
We hope that $\VEC{w}_{i,j}\approx y(t_{i,j})$ for all $i$ and
$j$.

\begin{rmkList}
\begin{enumerate}
\item Note that (\ref{FDM_RK_1}) is one step method, from $t_i$ to
$t_{i+1}$, of an implicit Runge-Kutta method.  Since we assume that
$\theta_0=0$, we have that $\beta_{0,m}=0$ for all $m$.
Since we assume that $\theta_J=1$, we have that
$\beta_{J,m} = \gamma_m$ for all $m$.  Thus (\ref{FDM_RK_1}) with
$j=J$ yields (\ref{coll_bs}).
\item The Runge-Kutta method (\ref{FDM_RK_1}) is stable for the
initial value problem
\begin{align*}
\VEC{y}'(t) &= f(t,\VEC{y}(t)) \\
\VEC{y}(a) &= \VEC{y}_c \in \RR^n
\end{align*}
\item The local truncation error of the Runge-Kutta method
(\ref{FDM_RK_1}) is at least of order $J$.
\end{enumerate}
\end{rmkList}

\begin{theorem}
\begin{enumerate}
\item Suppose that the polynomial mappings $\VEC{p}_i$ satisfy
(\ref{coll_1}), (\ref{coll_2}) and (\ref{coll_3}).  Then,
$\VEC{w}_{i,j} = \VEC{p}(t_{i,j})$ satisfy (\ref{FDM_RK_1})
and (\ref{FDM_RK_2}).
\item Suppose that the $\VEC{w}_{i,j}$ satisfy (\ref{FDM_RK_1}) and
(\ref{FDM_RK_2}).  For $0\leq i < N$, let $\VEC{p}_i$ be
the unique interpolating polynomial mapping of degree $J+1$ at the
points $(t_{i,j},\VEC{w}_{i,j})$ for $0\leq j \leq J$ that satisfies
$\VEC{p}_i'(t_{i,0}) = f(t_{i,0},\VEC{p}_i(t_{i,0}))$.  Then,
the $\VEC{p}_i$ satisfy (\ref{coll_1}), (\ref{coll_2}) and
(\ref{coll_3}).
\end{enumerate}
\end{theorem}

\begin{proof}
\stage{1}  Since $\VEC{p}_i$ is a polynomial mapping of degree $J+1$,
$\VEC{p}_i'$ is a polynomial mapping of degree $J$.  Since the
quadrature formula
\begin{equation}\label{colRKorderJp1}
  \int_{\theta_0}^{\theta_j} q(\theta) \dx{\theta}
  = \sum_{m=0}^J \beta_{j,m} q(\theta_m)
\end{equation}
with $0\leq m \leq J$ is true for polynomial $q$ of degree up to at
least $J$ by construction, we have
\begin{align*}
\VEC{p}_i(t_{i,j}) &= \VEC{p}_i(t_{i,0}) +
\int_{t_{i,0}}^{t_{i,j}} \VEC{p}_i'(t) \dx{t}
= \VEC{p}_i(t_{i,0}) +
h_i \int_{\theta_0}^{\theta_j} \VEC{p}_i'(t_i+\theta h_i) \dx{\theta} \\
&= \VEC{p}_i(t_{i,0}) +
h_i\,\sum_{m=0}^J\,\beta_{j,m}\,\VEC{p}_i'(t_{i,m}) \\
&= \VEC{p}_i(t_{i,0}) +
h_i\,\sum_{m=0}^J\,\beta_{j,m}\,f(t_{i,m},\VEC{p}_i(t_{i,m})) \quad ,
\quad 0 \leq j \leq J \ , 
\end{align*}
where the last equality comes from
(\ref{coll_1}).  So, we get (\ref{FDM_RK_1}) with
$\VEC{w}_{i,j} = \VEC{p}_i(t_{i,j})$
for all $0\leq i < N$ and $0\leq j \leq J$.  Obviously,
(\ref{coll_2}) implies (\ref{FDM_RK_2}).

\stage{2}
Again, since $\VEC{p}_i$ is a polynomial of degree $J+1$, $\VEC{p}_i'$
is a polynomial of degree $J$.  Since (\ref{colRKorderJp1}) 
is true for polynomial $q$ of degree up to at least $J$ by construction,
we get
\begin{align*}
\VEC{w}_{i,j} - \VEC{w}_{i,0} &= \VEC{p}(t_{i,j}) - \VEC{p}(t_{i,0})
= \int_{t_{i,0}}^{t_{i,j}} \VEC{p}_i'(t) \dx{t} \\
&= h_i \int_{\theta_0}^{\theta_j} \VEC{p}_i'(t_i+\theta h_i) \dx{\theta}
= h_i \sum_{m=0}^J \beta_{j,m}\VEC{p}_i'(t_{i,m}) \quad , \quad
0 \leq j \leq J \ .
\end{align*}
Moreover, from (\ref{FDM_RK_1}), we have that
\[
\VEC{w}_{i,j}- \VEC{w}_{i,0} =
h_i\,\sum_{k=0}^J\,\beta_{j,k}\,f(t_{i,k},\VEC{w}_{i,k})
= h_i\,\sum_{m=0}^J\,\beta_{j,m}\,f(t_{i,m},\VEC{p}_i(t_{i,m}))
\quad , \quad  0 \leq j\leq J \ .
\]
Thus,
\[
\sum_{m=0}^J\,\beta_{j,m}\left( \VEC{p}_i'(t_{i,m}) -
f(t_{i,m},\VEC{p}_i(t_{i,m})) \right) = \VEC{0}
\quad , \quad  0 \leq j \leq J \ .
\]
Since we assume that
\[
\VEC{p}_i'(t_{i,0}) = f(t_{i,0},\VEC{p}_i(t_{i,0})) \quad , \quad
0 \leq i < N \ ,
\]
namely that (\ref{coll_1}) with $j=0$ is satisfied, we have
\[
\sum_{m=1}^J\,\beta_{j,m}\left( \VEC{p}_i'(t_{i,m}) -
f(t_{i,m},\VEC{p}(t_{i,m})) \right) = \VEC{0}
\quad , \quad  1 \leq j \leq J \ .
\]
This can be rewritten as a linear system of the
form $B\VEC{X}=\VEC{0}$, where
\[
B = \begin{pmatrix}
\beta_{1,1} & \beta_{1,2} & \ldots & \beta_{1,J} \\
\beta_{2,1} & \beta_{2,2} & \ldots & \beta_{2,J} \\
 \vdots & \vdots & \ddots & \vdots \\
\beta_{J,1} & \beta_{J,2} & \ldots & \beta_{J,J}
\end{pmatrix}
\quad \text{and} \quad
\VEC{X} = \begin{pmatrix}
  \VEC{p}_i'(t_{i,1}) - f(t_{i,1},\VEC{p}(t_{i,1})) \\
  \VEC{p}_i'(t_{i,2}) - f(t_{i,2},\VEC{p}(t_{i,2})) \\
  \vdots \\
  \VEC{p}_i'(t_{i,J}) - f(t_{i,J},\VEC{p}(t_{i,J}))
\end{pmatrix} \ .
\]
Since $B$ is an invertible matrix\footnote{Use (\ref{colRKorderJp1}) with
$q(\theta) = \theta^m$ for $1\leq m \leq J$ to show that $B$ is
an invertible Vandermonde matrix.}, the only solution is $\VEC{X}= \VEC{0}$.
Thus (\ref{coll_1}) with $1\leq j \leq J$ must also be satisfied.

(\ref{coll_3}) is satisfied because $\VEC{p}_i(t_{i,0}) =
\VEC{w}_{i,0} = \VEC{w}_{i-1,J} = \VEC{p}_{i-1}(t_{i-1,J})$ for
$1 < i < N$.  Finally, (\ref{coll_2}) is satisfied because
$g(\VEC{p}_0(t_{0,0}),\VEC{p}(t_{N-1,J})) =
g(\VEC{w}_{0,0},\VEC{w}_{N-1,J}) = \VEC{0}$.
\end{proof}

\begin{rmk}
There exist collocation methods with smoother polynomial mappings than
the piecewise continuous polynomial mappings that we have considered
here.  These methods are more efficient.
\end{rmk}

\section{Analytic Eigenvalue Problems}

This section is based on Keller's lectures \cite{K} and Ascher et
al.'s book \cite{AMR}.

Eigenvalue problems are the major source of boundary value problems.
It is therefore important to say a few words about eigenvalues
problems.

We consider the generalised eigenvalue problem
\begin{equation}  \label{GEVP}
\begin{split}
\VEC{y}'(t) - A(t,\lambda)\VEC{y}(t) &= \VEC{0}
\quad , \quad a \leq t \leq b\\
B_a(\lambda)\VEC{y}(a) + B_b(\lambda)\VEC{y}(b) &= \VEC{0}
\end{split}
\end{equation}
where $B_a(\lambda)$ and $B_b(\lambda)$ are analytic in $\lambda$, and
$A(t,\lambda)$ is analytic in $\lambda$ uniformly in $t \in [a,b]$.
Moreover, we assume that
$\rank \begin{pmatrix} B_a(\lambda) & B_b(\lambda) \end{pmatrix}= n$ for
all $\lambda$.  This is a necessary condition for the existence of a
solution for (\ref{GEVP}).

\begin{rmk}
The eigenvalue problem (\ref{GEVP}) has partially separated boundary
conditions if
\[
B_a(\lambda) = \begin{pmatrix}
B_a^{[a]}(\lambda) \\ B_a^{[b]}(\lambda)
\end{pmatrix} \quad \text{and} \quad
B_b(\lambda) = \begin{pmatrix}
0 \\ B_b^{[b]}(\lambda)
\end{pmatrix} \ ,
\]
where $B_a^{[a]}(\lambda)$ is a $(n-q)\times n$ matrix, and
$B_a^{[b]}(\lambda)$ and $B_b^{[b]}(\lambda)$ are $q\times n$ matrices.
\label{PSBC_GEVP}
\end{rmk}

The {\bfseries fundamental solution}\index{Finite Difference
Methods!Fundamental Solution} associated to (\ref{GEVP}) is the 
solution of
\[
\begin{split}
Y'(t,\lambda) - A(t,\lambda)Y(t,\lambda) &= 0
\quad , \quad a \leq t \leq b\\
Y(a,\lambda) &= \Id
\end{split}
\]
for all $\lambda$.  It can be shown that $Y(t,\lambda)$ is analytic
in $\lambda$ uniformly in $t \in [a,b]$.

Every solution of (\ref{GEVP}) is of the form
\[
y(t,\lambda) = Y(t,\lambda) \VEC{y}_c  \ ,
\]
where $\VEC{y}_c$ is a solution of
\begin{equation} \label{BC_GEVP}
Q(\lambda) \VEC{y}_c \equiv \left( B_a(\lambda) + B_b(\lambda)Y(b,\lambda)
\right) \VEC{y}_c = \VEC{0} \ .
\end{equation}

\begin{theorem}
For the generalised eigenvalue problem (\ref{GEVP}), only one of the
following two cases is possible.
\begin{enumerate}
\item Every $\lambda$ is an eigenvalue of (\ref{GEVP}).
\item There are at most a countable number of distinct eigenvalues
$\lambda_k$ with no accumulation point.
\end{enumerate}
In the second case, $\lambda_k$ has geometric multiplicity
\[
r_k = \dim\,\ker(Q(\lambda_k)) \leq n \ .
\]
If we have partially separated boundary conditions as in
Remark~\ref{PSBC_GEVP}, then $r_k \leq q$.
\end{theorem}

\begin{proof}
$\lambda$ is an eigenvalue of (\ref{GEVP}) if (\ref{BC_GEVP}) has a
non-trivial solution, and (\ref{BC_GEVP}) has a non-trivial solution
if and only if $\det(Q(\lambda))=0$.  Thus, the geometric multiplicity
of $\lambda$ is the dimension of the solution space of
(\ref{BC_GEVP}); namely, the dimension of the kernal of $Q(\lambda)$.

Since $\det(Q(\lambda))$ is an analytic function of $\lambda$,
$\det(Q(\lambda)) = 0$ for all $\lambda$ if and only if there is an
accumulation point for the zeros of $\det(Q(\lambda))$; namely, the
eigenvalues of (\ref{GEVP}).  Thus, we have either (1) or (2).

For the partially separated boundary conditions case
\[
Q(\lambda_k) \VEC{y}_c = \begin{pmatrix}
B_a^{[a]}(\lambda_k) \\ B_a^{[b]}(\lambda_k) + B_b^{[b]}(\lambda_k)
Y(b,\lambda_k)
\end{pmatrix} \VEC{y}_c = \VEC{0} \; ,
\]
where $B_a^{[a]}(\lambda_k)$ has full rank $n-q$.
Thus $r_k = \dim\ \ker(Q(\lambda_k)) \leq q$.
\end{proof}

\section{Exercises}

\begin{question}
Show that the midpoint scheme of Example~\ref{eggmidpointscheme} is
consistent and stable for the linear boundary value problem
(\ref{LinBVPrepeat}), and therefore convergent.
\label{boundQ1}
\end{question}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
