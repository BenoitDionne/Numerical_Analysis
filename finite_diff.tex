\chapter{Finite Difference Methods}\label{FiniteDiffMeth}

Compare to solving partial differential equations numerically,
solving ordinary differential equations is very simple.  All the
numerical methods behave similarly with all types of ordinary
differential equations.  The only exception is with stiff ordinary
differential equations.

The situation for partial differential equations is a lot more
complex.  There are no numerical methods that can be used for all
types of partial differential equations to generate an accurate
numerical solution.  This is even true for the three types of linear
partial differential equations of order two with constant
coefficients; namely the parabolic, elliptic and hyperbolic equations.
In fact, as we will show, hyperbolic partial differential equations
cannot be solved accurately with finite difference schemes without
imposing strict constrains on the step sizes.  Some other methods,
like finite element methods, need to be used with such partial
differential equations.

Suppose that $u$ is the solution of a partial differential equation
\[
P\left(u, \pdydx{u}{x}, \pdydx{u}{y}, \pdydxn{u}{x}{2},
\pdydxnm{u}{x}{y}{2}{}{},  \ldots \right)= F(x,y)
\]
on a domain
\[
R= [a,b] \times [c,d] = \{ (x,y) : a \leq x \leq b \ \text{and}
\ c \leq y \leq d \} \ ,
\]
where $P$ and $F$ are ``nice'' functions.  Choose $N$ and
$M$, two positive integers, and let $\dtx{x} = (b-a)/N$ and
$\dtx{y} = (d-c)/M$.  The set
\[
R_\Delta =
\left\{ (x_i,y_j) : x_i = a+i\dtx{x} \ \text{for} \ 0\leq i \leq N
\ \text{and} \ y_j = c + j\dtx{y} \ \text{for} \ 0\leq j \leq
M \right\}
\]
forms a {\bfseries grid}\index{Finite Difference Methods!Grid} of the domain
$D$.  Each point $(x_i,y_j)$ is called a
{\bfseries mesh point}\index{Finite Difference Methods!Mesh Point}.  The
{\bfseries step sizes}\index{Finite Difference Methods!Step Sizes} are the
values of $\dtx{x}$ and $\dtx{y}$ \footnote{In our presentation, we
assume that the distance between the $x_i$ and the distance between
the $y_j$ are constant.  Finite difference schemes could be developed
for non-constant step sizes but this is for a more advanced text.}.
A {\bfseries numerical solution}\index{Finite Difference Methods!Numerical
Solution} of the partial differential equation is a set
\[
\left\{ w_{i,j} : 0\leq i \leq N \ \text{and} \ 0\leq j \leq M \right\}
\]
such that $w_{i,j} \approx u(x_i,y_j)$ for $0\leq i \leq N$ and
$0\leq j \leq M$.

The goal of this chapter is to develop some
\index{Finite Difference Methods}\index{Finite Difference
Schemes|seealso{Finite Difference Methods}}
{\bfseries finite difference schemes or methods};
namely, some finite difference equations to compute the values
$w_{i,j}$.   The finite difference equations are obtained from the
partial differential equations by substituting the partial derivatives
in the partial differential equations by finite difference formulae
approximating these partial derivatives.

The reader should not expect a complete listing of methods to solve
partial differential equations.  Only some basic partial differential
equations and finite difference schemes are considered.  There is
however enough material to get a good understanding of the complexity
and procedure to numerically solve partial differential equations.

\section{Finite Difference Formulae}\label{Basicfdf}

To develop finite difference schemes, we need to use finite difference
formulae to approximate the partial derivatives of sufficiently
differentiable functions.  Let $u:\RR^2 \to \RR$ be a sufficiently
differentiable function.  We use Taylor expansions of $u$ 
at $(x_i,y_j)$ to derive finite difference formulae of the
partial derivatives of $u$ at $(x_i,y_j)$.  We provide below a few
examples of the derivation of finite difference formulae.  More
finite difference formulae will be introduced later on.

\subsection{First Order Derivatives}

We begin by deriving a finite difference formula for
$\displaystyle \pdydx{u}{x}$ at the mesh point $(x_i,y_j)$.
If we assume that $u$ is of class $C^2$, we have
\[
u(x_{i+1},y_j) = u(x_i,y_j) + \pdydx{u}{x}(x_i,y_j) \dtx{x}
+ \frac{1}{2} \pdydxn{u}{x}{2}\left(\zeta_{i,j},y_j\right) (\dtx{x})^2
\]
for some $\zeta_{i,j} \in ]x_i,x_{i+1}[$.  So
\begin{equation} \label{fdm_foD}
\pdydx{u}{x}(x_i,y_j) = \frac{u(x_{i+1},y_j)- u(x_i,y_j)}{\dtx{x}}
- \frac{1}{2} \pdydxn{u}{x}{2}(\zeta_{i,j},y_j) \dtx{x} \ .
\end{equation}
Since $\displaystyle \pdydxn{u}{x}{2}$ is continuous on the close set
$R$, there exists a constant $K >0$ such that 
\[
\left| \frac{1}{2} \pdydxn{u}{x}{2}(x,y) \right| < K
\]
for $(x,y) \in R$.  Hence,
\[
\left| \frac{1}{2} \pdydxn{u}{x}{2}\left(\zeta_{i,j},y_j\right) \dtx{x} \right|
< K \dtx{x}
\]
because $\zeta_{i,j} \in ]x_i,x_{i+1}[$ and therefore $(\zeta_{i,j},y_j) \in R$.
We have shown that
\begin{equation} \label{fdm_simple}
\pdydx{u}{x}(x_i,y_j) \approx \frac{u(x_{i+1},y_j)- u(x_i,y_j)}{\dtx{x}}
\end{equation}
and the truncation error
$\displaystyle \frac{1}{2} \pdydxn{u}{x}{2}(\zeta_{i,j},y_j) \dtx{x}$ satisfies
\[
\frac{1}{2} \pdydxn{u}{x}{2}\left(\zeta_{i,j},y_j\right) \dtx{x} = O(\dtx{x})
\]
for $\dtx{x}$ near $0$.  The truncation error converges to zero as
$\dtx{x}$ converges to zero.

Instead of using the points $(x_i,y_j)$ and $(x_{i+1},y_j)$ to
derive a finite difference formula for $\displaystyle \pdydx{u}{x}$ at
the mesh point $(x_i,y_j)$, we could use $(x_i,y_j)$ and
$(x_{i-1},y_j)$.

If we assume that $u$ is of class $C^2$, we have
\[
u(x_{i-1},y_j) = u(x_i,y_j) - \pdydx{u}{x}(x_i,y_j) \dtx{x}
+ \frac{1}{2} \pdydxn{u}{x}{2}\left(\zeta_{i,j},y_j\right) (\dtx{x})^2
\]
for some $\zeta_{i,j} \in ]x_{i-1},x_i[$.  So
\begin{equation} \label{fdm_foDM}
\pdydx{u}{x}(x_i,y_j) = \frac{u(x_i,y_j)- u(x_{i-1},y_j)}{\dtx{x}}
+ \frac{1}{2} \pdydxn{u}{x}{2}(\zeta_{i,j},y_j) \dtx{x} \ .
\end{equation}
As we did above, if $\displaystyle \pdydxn{u}{x}{2}$ is continuous on
the close set $R$, we may assume that there exists a constant $K >0$
such that
\[
\left| \frac{1}{2} \pdydxn{u}{x}{2}\left(\zeta_{i,j},y_j\right) \dtx{x} \right|
< K \dtx{x} \ .
\]
Hence,
\begin{equation} \label{fdm_simpleM}
\pdydx{u}{x}(x_i,y_j) \approx \frac{u(x_i,y_j)- u(x_{i-1},y_j)}{\dtx{x}}
\end{equation}
and the truncation error
$\displaystyle \frac{1}{2} \pdydxn{u}{x}{2}(\zeta_{i,j},y_j) \dtx{x}$ satisfies
\[
\frac{1}{2} \pdydxn{u}{x}{2}\left(\zeta_{i,j},y_j\right) \dtx{x} = O(\dtx{x})
\]
for $\dtx{x}$ near $0$.

To derive finite difference formulae which are ``more accurate'' than
(\ref{fdm_simple}) (i.e.\ with a smaller truncation error when
$\dtx{x}$ approach $0$), we need to consider Taylor expansion of order
higher than two.  For instance, if we assume that $u$ is of class
$C^3$, we have that
\begin{align*}
u(x_{i+1},y_j) &= u(x_i,y_j) + \pdydx{u}{x}(x_i,y_j) \dtx{x}
+ \frac{1}{2} \pdydxn{u}{x}{2}\left(x_i,y_j\right) (\dtx{x})^2
+ \frac{1}{3!} \pdydxn{u}{x}{3}\left(\zeta_{i,j},y_j\right) (\dtx{x})^3
\intertext{and}
u(x_{i-1},y_j) &= u(x_i,y_j) - \pdydx{u}{x}(x_i,y_j) \dtx{x}
+ \frac{1}{2} \pdydxn{u}{x}{2}\left(x_i,y_j\right) (\dtx{x})^2
- \frac{1}{3!} \pdydxn{u}{x}{3}\left(\eta_{i,j},y_j\right) (\dtx{x})^3
\end{align*}
for $\zeta_{i,j} \in ]x_i,x_{i+1}[$ and $\eta_{i,j} \in ]x_{i-1},x_i[$.  If we
subtract the second equation from the first equation, we get
\[
u(x_{i+1},y_j) - u(x_{i-1},y_j) = 2 \pdydx{u}{x}(x_i,y_j) \dtx{x}
+ \frac{1}{3!} \left( \pdydxn{u}{x}{3}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{3}\left(\eta_{i,j},y_j\right)\right) (\dtx{x})^3 \ .
\]
Hence
\begin{equation} \label{fdm_soD}
\pdydx{u}{x}(x_i,y_j) = \frac{u(x_{i+1},y_j) - u(x_{i-1},y_j)}{2\dtx{x}}
- \frac{1}{12} \left( \pdydxn{u}{x}{3}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{3}\left(\eta_{i,j},y_j\right)\right) (\dtx{x})^2 \ .
\end{equation}
We have found that
\[
\pdydx{u}{x}(x_i,y_j) \approx \frac{u(x_{i+1},y_j) - u(x_{i-1},y_j)}{2\dtx{x}}
\]
and, because $\displaystyle \pdydxn{u}{x}{3}$ is continuous, we can
show as we did for the previous finite difference formulae that the
truncation error
$\displaystyle \frac{1}{12}
\left( \pdydxn{u}{x}{3}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{3}\left(\eta_{i,j},y_j\right)\right) (\dtx{x})^2$
satisfies
\[
\frac{1}{12} \left( \pdydxn{u}{x}{3}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{3}\left(\eta_{i,j},y_j\right)\right) (\dtx{x})^2
= O\left((\dtx{x})^2\right)
\]
for $\dtx{x}$ near $0$.

\subsection{Second Order Derivatives}

Using the Taylor Expansion Theorem, we may also derive finite
difference formulae for second order derivatives.  If $u$ is of class
$C^4$, we have
\begin{align*}
u(x_{i+1},y_j) &= u(x_i,y_j) + \pdydx{u}{x}(x_i,y_j) \dtx{x}
+ \frac{1}{2} \pdydxn{u}{x}{2}\left(x_i,y_j\right) (\dtx{x})^2
+ \frac{1}{3!} \pdydxn{u}{x}{3}\left(x_1,y_j\right) (\dtx{x})^3 \\
& \quad + \frac{1}{4!} \pdydxn{u}{x}{4}\left(\zeta_{i,j},y_j\right) (\dtx{x})^4
\intertext{and}
u(x_{i-1},y_j) &= u(x_i,y_j) - \pdydx{u}{x}(x_i,y_j) \dtx{x}
+ \frac{1}{2} \pdydxn{u}{x}{2}\left(x_i,y_j\right) (\dtx{x})^2
- \frac{1}{3!} \pdydxn{u}{x}{3}\left(x_i,y_j\right) (\dtx{x})^3 \\
&\quad + \frac{1}{4!} \pdydxn{u}{x}{4}\left(\eta_{i,j},y_j\right) (\dtx{x})^4
\end{align*}
for $\zeta_{i,j} \in ]x_i,x_{i+1}[$, and $\eta_{i,j} \in ]x_{i-1},x_i[$.
If we add these two equations, we get
\begin{align*}
&u(x_{i+1},y_j) + u(x_{i-1},y_j) \\
&\qquad = 2u(x_i,y_j) 
+ \pdydxn{u}{x}{2}\left(x_i,y_j\right) (\dtx{x})^2
+ \frac{1}{4!} \left( \pdydxn{u}{x}{4}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{4}\left(\eta_{i,j},y_j\right) \right) (\dtx{x})^4 \ .
\end{align*}
Solving for $\displaystyle \pdydxn{u}{x}{2}(x_i,y_j)$, we get
\begin{equation} \label{fdm_soDD}
\begin{split}
\pdydxn{u}{x}{2}\left(x_i,y_j\right) &=
\frac{u(x_{i+1},y_j) - 2u(x_i,y_j) + u(x_{i-1},y_j)}{(\dtx{x})^2} \\
& \qquad - \frac{1}{4!} \left( \pdydxn{u}{x}{4}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{4}\left(\eta_{i,j},y_j\right) \right) (\dtx{x})^2 \ .
\end{split}
\end{equation}
We have found that
\begin{equation} \label{fdm_simple2oM}
\pdydxn{u}{x}{2}\left(x_i,y_j\right) \approx
\frac{u(x_{i+1},y_j) - 2u(x_i,y_j) + u(x_{i-1},y_j)}{(\dtx{x})^2}
\end{equation}
and, because $\displaystyle \pdydxn{u}{x}{4}$ is continuous, we can
show as we did before that the truncation error 
$\displaystyle \frac{1}{4!} \left( \pdydxn{u}{x}{4}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{4}\left(\eta_{i,j},y_j\right) \right) (\dtx{x})^2$
satisfies
\[
\frac{1}{4!} \left( \pdydxn{u}{x}{4}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{4}\left(\eta_{i,j},y_j\right) \right) (\dtx{x})^2
= O\left((\dtx{x})^2\right)
\]
for $\dtx{x}$ near $0$.

We can proceed likewise to find other finite difference formulae.

\section{Explicit and Implicit Schemes}\label{EXplImplSchenes}

We develop finite difference schemes for the three types of linear
partial differential equations of order two with constant
coefficients.  More precisely, we develop finite difference schemes
for one representative of each of these types of partial differential
equations. This will be enough to understand the peculiarities of each
type.

\begin{enumerate}
\item For the parabolic equations, we consider the heat equation
$\displaystyle \pdydx{u}{t} = c^2 \pdydxn{u}{x}{2}$.
\item For the elliptic equations, we consider the Dirichlet equation
$\displaystyle \pdydxn{u}{x}{2} + \pdydxn{u}{y}{2} = f$.
\item For the hyperbolic equation, we consider the wave equation
$\displaystyle \pdydxn{u}{t}{2} = c^2 \pdydxn{u}{x}{2}$.
\end{enumerate}

\subsection{Parabolic Equations}

\subsubsection{An Explicit Scheme}

We consider the heat equation with forcing
\begin{equation} \label{HeatEqu}
\pdydx{u}{t} - c^2 \pdydxn{u}{x}{2} = f(x,t) \quad , \quad 0 < x < L \
\text{and} \ 0<t<T \ ,
\end{equation}
with the boundary conditions
\begin{equation} \label{HeatEquBC}
u(0,t)=h_0(t) \ \text{and} \ u(L,t)=h_L(t) \quad , \quad 0\leq t \leq T \ ,
\end{equation}
and the initial condition
\begin{equation} \label{HeatEquIC}
u(x,0) = g(x) \quad , \quad 0 \leq x \leq L \ ,
\end{equation}
where $g(0)=h_0(0)$ and $g(L)=h_L(0)$.  The forcing is provided by the
function $f$.

We develop a finite difference scheme for the heat equation with
forcing given in (\ref{HeatEqu}), (\ref{HeatEquBC}) and
(\ref{HeatEquIC}).

Given two integers $N\geq 2$ and $M\geq 1$, we set $\dtx{x} = L/N$
$\dtx{t} = T/M$, $x_i = i \dtx{x}$, $t_j = j \dtx{t}$ and
$\displaystyle u_{i,j} = u\left(x_i,t_j\right)$
for $0\leq i \leq N$ and $0\leq j \leq M$.
From (\ref{fdm_foD}) and (\ref{fdm_soDD}), we get
\begin{equation}\label{fdm_sch1_tr}
\begin{split}
\frac{u_{i,j+1}- u_{i,j}}{\dtx{t}} &-
\frac{1}{2} \pdydxn{u}{t}{2}\left(x_i,\rho_{i,j}\right) \dtx{t} - c^2 \,
\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\dtx{x})^2} \\
& \quad\quad + \frac{c^2}{4!}
\left( \pdydxn{u}{x}{4}\left(\zeta_{i,j},t_j\right)
+ \pdydxn{u}{x}{4}\left(\eta_{i,j},t_j\right) \right) (\dtx{x})^2 = f(x_i,t_j)
\end{split}
\end{equation}
for $\rho_{i,j} \in ]t_j,t_{j+1}[$,
$\zeta_{i,j} \in ]x_{i-1},x_{i+1}[$ and
$\eta_{i,j} \in ]x_{i-1},x_{i+1}[$.  For $\dtx{t}$ and $\dtx{x}$ small,
we have
\[
\frac{u_{i,j+1}- u_{i,j}}{\dtx{t}} - c^2 \,
\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\dtx{x})^2} \approx f(x_i,t_j) \ .
\]
This suggests the following finite difference equation.
\begin{equation} \label{fdm_sch1}
\frac{w_{i,j+1}- w_{i,j}}{\dtx{t}} - c^2 \,
\frac{w_{i+1,j} - 2w_{i,j} + w_{i-1,j}}{(\dtx{x})^2} = f(x_i,t_j)
\end{equation}
for $0 < i <N$ and $0 \leq j < M$.  The boundary conditions impose
$w_{0,j} = h_0(t_j)$ and $w_{N,j} =  h_L(t_j)$ for $0\leq j \leq M$.
The initial condition imposes $w_{i,0} = g(x_i)$ for $0 \leq i \leq N$.

Following some simple algebra, we get the following finite difference
scheme to approximate the solution of the heat equation with forcing
in (\ref{HeatEqu}).

\begin{algo} \label{fdm_sch1S}
\[
w_{i,j+1} - w_{i,j} - \alpha \left( w_{i+1,j} - 2w_{i,j} +
w_{i-1,j}\right) = f(x_i,t_j) \dtx{t}
\]
for $1 < i <N$ and $0 \leq j < M$, where
$\displaystyle \alpha = \frac{c^2\dtx{t}}{(\dtx{x})^2}$,
$w_{0,j} = h_0(t_j)$ and $w_{N,j} = h_L(t_j)$ for $0\leq j \leq M$, and
$w_{i,0} = g(x_i)$ for $0 \leq i \leq N$.
\end{algo}

\pdfF{finite_diff/fdm_fig1}{Finite difference scheme for the heat
equation with forcing}{Schematic representation of the finite
difference scheme given in Algorithm~\ref{fdm_sch1S}.}{fdm_fig1}

This scheme is illustrated in Figure~\ref{fdm_fig1}.  It can be
expressed as a linear system $A \VEC{w} = \VEC{B}$.  The (column)
vector $\VEC{w}$ is defined by
\[ \VEC{w} = \begin{pmatrix} \VEC{w}_1 \\ \VEC{w}_2 \\ \vdots \\
\VEC{w}_{M}
\end{pmatrix} \quad \text{with} \quad \VEC{w}_j =
\begin{pmatrix} w_{1,j} \\ w_{2,j} \\ \vdots \\
w_{N-1,j} \end{pmatrix}
\]
for $0 <j \leq M$. The matrix $A$ is
a \nm{(N-1)M}{(N-1)M} matrix of the form 
\[
A = \begin{pmatrix}
\Id & 0 & 0 & \ldots & 0 & 0 \\
K & \Id & 0 & \ldots & 0 & 0 \\
0 & K & \Id & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & K & \Id
\end{pmatrix} \ ,
\]
where $\Id$ is the \nm{(N-1)}{(N-1)} identity matrix and
\begin{equation} \label{fdm_K}
K = \begin{pmatrix}
-1+2\alpha & -\alpha & 0 & 0 & 0 & \ldots & 0 & 0 \\
-\alpha & -1 +2\alpha & -\alpha & 0 & 0 & \ldots & 0 & 0  \\
0 & -\alpha & -1 +2\alpha & -\alpha & 0 & \ldots & 0 & 0 \\
0 & 0 & -\alpha & -1 +2\alpha & -\alpha & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & \cdots & -\alpha & -1+2\alpha
\end{pmatrix}
\end{equation}
is a \nm{(N-1)}{(N-1)} matrix.  The (column) vector $\VEC{B}$ is defined by
\[
\VEC{B} = \begin{pmatrix} \VEC{B}_1 \\ \VEC{B}_2 \\ \vdots \\
\VEC{B}_M \end{pmatrix} \ , \ \text{where} \quad
\VEC{B}_1 = \begin{pmatrix} 
w_{1,0} + \alpha \left( w_{0,0} - 2 w_{1,0} + w_{2,0}\right)
+ f(x_1,t_0) \dtx{t} \\
w_{2,0} + \alpha \left( w_{1,0} - 2 w_{2,0} + w_{3,0}\right)
+ f(x_2,t_0) \dtx{t} \\
w_{3,0} + \alpha \left( w_{2,0} - 2 w_{3,0} + w_{4,0}\right)
+ f(x_3,t_0) \dtx{t} \\
\vdots \\
w_{N-1,0} + \alpha \left( w_{N-2,0} - 2 w_{N-1,0} + w_{N,0}\right)
+ f(x_{N-1},t_0) \dtx{t}
\end{pmatrix}
\]
and
\[
\VEC{B}_j = \begin{pmatrix}
\alpha w_{0,j-1} + f(x_1,t_{j-1}) \dtx{t} \\
f(x_2,t_{j-1}) \dtx{t} \\
\vdots \\
f(x_{N-2},t_{j-1}) \dtx{t} \\
\alpha w_{N,j-1} + f(x_{N-1},t_{j-1}) \dtx{t}
\end{pmatrix}
\]
for $2\leq j \leq M$.

\subsubsection{An Implicit Scheme,  Crank-Nicolson Scheme}

We will see in Section~\ref{fdm_CCS} that the finite difference scheme
in Algorithm~\ref{fdm_sch1S} is not really good.  Another scheme often used
to numerically solve the heat equation with forcing is due to Crank
and Nicolson.  Before introducing this scheme, we need to introduce
the following finite difference scheme.

Using (\ref{fdm_simpleM}) and (\ref{fdm_simple2oM}) at
$(x_i,t_{j+1})$, we may write
\begin{align*}
\frac{u_{i,j+1}- u_{i,j}}{\dtx{t}} &+
\frac{1}{2} \pdydxn{u}{t}{2}\left(x_i,\rho_{i,j}\right) \dtx{t} - c^2 \,
\frac{u_{i+1,j+1} - 2u_{i,j+1} + u_{i-1,j+1}}{(\dtx{x})^2} \\
& \quad\quad + \frac{c^2}{4!}
\left( \pdydxn{u}{x}{4}\left(\zeta_{i,j},t_{j+1}\right)
+ \pdydxn{u}{x}{4}\left(\eta_{i,j},t_{j+1}\right) \right) (\dtx{x})^2
= f(x_i,t_{j+1})
\end{align*}
for $\rho_{i,j} \in ]t_j,t_{j+1}[$, $\zeta_{i,j} \in ]x_{i-1},x_{i+1}[$ and
$\eta_{i,j} \in ]x_{i-1},x_{i+1}[$.  For $\dtx{t}$ and $\dtx{x}$ small,
we have
\[
\frac{u_{i,j+1}- u_{i,j}}{\dtx{t}} - c^2 \,
\frac{u_{i+1,j+1} - 2u_{i,j+1} + u_{i-1,j+1}}{(\dtx{x})^2}
\approx f(x_i,t_{j+1}) \ .
\]
This suggests the following finite difference equation.
\begin{equation} \label{fdm_sch1M}
\frac{w_{i,j+1}- w_{i,j}}{\dtx{t}} - c^2 \,
\frac{w_{i+1,j+1} - 2w_{i,j+1} + w_{i-1,j+1}}{(\dtx{x})^2} = f(x_i,t_{j+1})
\end{equation}
for $0 < i <N$ and $0 \leq j < M$.

The Crank-Nicolson scheme comes from adding $1/2$ times (\ref{fdm_sch1})
and $1/2$ times (\ref{fdm_sch1M}) to get the finite difference
equation\footnote{more generally, we could
have added $\lambda$ times (\ref{fdm_sch1}) and $1-\lambda$ times
(\ref{fdm_sch1M}) to get a family of finite difference scheme for
$0\leq \lambda \leq 1$.}
\begin{equation} \label{fdm_sch11}
\begin{split}
&\frac{w_{i,j+1}- w_{i,j}}{\dtx{t}} - \frac{c^2}{2} \,
\left( \frac{w_{i+1,j} - 2w_{i,j} + w_{i-1,j}}{(\dtx{x})^2}
+ \frac{w_{i+1,j+1} - 2w_{i,j+1} + w_{i-1,j+1}}{(\dtx{x})^2} \right) \\
& \qquad \qquad = \frac{1}{2} \big( f(x_i,t_j) + f(x_i,t_{j+1})\big)
\end{split}
\end{equation}
for $0 < i  < N$ and $0 \leq j < M$.  The boundary conditions and
initial condition still give $w_{0,j} = h_0(t_j)$ and $w_{N,j} = h_L(t_j)$
for $0\leq j \leq M$, and $w_{i,0} = g(x_i)$ for $0 \leq i \leq N$
respectively.

Following some simple algebra, we find the following finite difference
scheme for the heat equation with forcing in (\ref{HeatEqu}).

\begin{algo}[Crank-Nicholson] \label{fdm_sch11S}
\begin{align*}
& w_{i,j+1} - w_{i,j} - \alpha \,
\left( w_{i+1,j} - 2w_{i,j} + w_{i-1,j}
  + w_{i+1,j+1} - 2w_{i,j+1} + w_{i-1,j+1} \right) \\
&\qquad \qquad = \frac{1}{2}\big( f(x_i,t_j) + f(x_i,t_{j+1}) \big) \dtx{t}
\end{align*}
for $0 < i <N$ and $0 \leq j < M$, where 
$\displaystyle \alpha = \frac{c^2\dtx{t}}{2 (\dtx{x})^2}$,
$w_{0,j} = h_0(t_j)$ and $w_{N,j} = h_L(t_j)$ for $0\leq j \leq M$, and
$w_{i,0} = g(x_i)$ for $0 \leq i \leq N$.
\end{algo}

\pdfF{finite_diff/fdm_fig3}{The Crank-Nicolson scheme for the heat
equation with forcing}{Schematic representation of the Crank-Nicolson scheme
given in Algorithm~\ref{fdm_sch11S}.}{fdm_fig3}

This scheme is illustrated in Figure~\ref{fdm_fig3}.  This is an
implicit scheme because the value of $u$ at $(x_i,t_{j+1})$ is
approximated using values of $u$ at $(x_{i-1},t_{j+1})$ and
$(x_{i+1},t_{j+1})$, two values for $t = t_{j+1}$ that are not explicitly
known.

As with the finite difference scheme in Algorithm~\ref{fdm_sch1S}, the
Crank-Nicolson scheme can be expressed as a linear system
$A \VEC{w} = \VEC{B}$.  The (column) vector $\VEC{w}$ is again defined by
\[
\VEC{w} = \begin{pmatrix}
\VEC{w}_1 \\ \VEC{w}_2 \\ \vdots \\ \VEC{w}_M
\end{pmatrix}
\quad \text{with} \quad
\VEC{w}_j =
\begin{pmatrix} w_{1,j} \\ w_{2,j} \\ \vdots \\ w_{N-1,j} \end{pmatrix}
\]
for $0 < j \leq M$. The matrix $A$ is a \nm{(N-1)M}{(N-1)M} matrix of
the form
\[
A = \begin{pmatrix}
J & 0 & 0 & \ldots & 0 & 0 \\
K & J & 0 & \ldots & 0 & 0 \\
0 & K & J & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & K & J
\end{pmatrix} \ ,
\]
where
\begin{equation} \label{fdm_J}
J = \begin{pmatrix}
1+2\alpha & -\alpha & 0 & 0 & 0 & \ldots & 0 & 0 \\
-\alpha & 1 +2\alpha & -\alpha & 0 & 0 & \ldots & 0 & 0  \\
0 & -\alpha & 1 +2\alpha & -\alpha & 0 & \ldots & 0 & 0 \\
0 & 0 & -\alpha & 1 +2\alpha & -\alpha & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & \cdots & -\alpha & 1+2\alpha
\end{pmatrix}
\end{equation}
is a \nm{(N-1)}{(N-1)} matrix and $K$ is defined in (\ref{fdm_K}).
The (column) vector $\VEC{B}$ is the column matrix defined by
\[
\VEC{B} = \begin{pmatrix} \VEC{B}_1 \\ \VEC{B}_2 \\ \vdots \\
\VEC{B}_M \end{pmatrix} \ ,
\]
where
\[
\VEC{B}_1 = \begin{pmatrix} 
w_{1,0} + \alpha \left( w_{0,0} - 2 w_{1,0} + w_{2,0} + w_{0,1}\right)
+ \big(f(x_1,t_0) +f(x_1,t_1)\big) \dtx{t}/2 \\
w_{2,0} + \alpha \left( w_{1,0} - 2 w_{2,0} + w_{3,0}\right)
+ \big( f(x_2,t_0) + f(x_2,t_1)\big) \dtx{t}/2  \\
w_{3,0} + \alpha \left( w_{2,0} - 2 w_{3,0} + w_{4,0}\right)
+ \big( f(x_3,t_0) +f(x_3,t_1)\big) \dtx{t}/2 \\
\vdots \\
w_{N-1,0} + \alpha \left( w_{N-2,0} - 2 w_{N-1,0} + w_{N,0} + w_{N,1} \right)
+ \big( f(x_{N-1},t_0) +f(x_{N-1},t_1)\big) \dtx{t}/2
\end{pmatrix}
\]
and
\[
\VEC{B}_j = \begin{pmatrix}
\alpha \left( w_{0,j-1} + w_{0,j} \right)
+ \big( f(x_1,t_{j-1}) +f(x_1,t_j\big) \dtx{t}/2 \\
\big( f(x_2,t_{j-1}) +f(x_2,t_j)\big) \dtx{t}/2 \\
\vdots \\
\big( f(x_{N-2},t_{j-1}) +f(x_{N-2},t_j)\big) \dtx{t}/2 \\
\alpha \left( w_{N,j-1} + w_{N,j} \right)
+ \big( f(x_{N-1},t_{j-1})+f(x_{N-1},t_j)\big) \dtx{t}/2
\end{pmatrix}
\]
for $2 \leq j \leq M$.

\begin{code}[Crank-Nicholson]
To approximate the solution of the heat equation with forcing
\[
  \pdydx{u}{t} = c^2 \pdydxn{u}{y}{2} + f(x,y)
\]
on the region $R = [a,b]\times[t_1,t_2]$ with the initial condition
$u(x,t_1) = g_b(x)$ for $a \leq x \leq b$, and the boundary conditions
$u(a,t) = g_l(t)$ and $u(b,t) = g_r(t)$ for $t_1 \leq t \leq t_2$.

\subI{Input} The right hand side $f$.\\
The initial condition $g_b(x)$ when $t = t_1$.\\
The boundary condition $g_l(t)$ when $x = a$.\\
The boundary condition $g_r(t)$ when $x = b$.\\
The number of partitions $N$ of $[a,b]$ with $\dtx{x} = (b-a)/N$.\\
The number of partitions $M$ of $[t_1,t_2]$ with $\dtx{t} = (t_2-t_1)/M$.  \\
The endpoints $a < b$ of the $x$-interval for the domain $R$.\\
The endpoints $t_1 < t_2$ of the $t$-interval for the domain $R$.\\
\subI{Output}
X contains the $x$-coordinates $x_0$, $x_1$, \ldots, $x_N$ of the
mesh points in the domain $R$.\\
T contains the $t$-coordinates $t_0$, $t_1$, \ldots, $t_M$ of the
mesh points in the domain $R$.\\
U contains the approximations of $u$ at the mesh points.
$U_{i,j} \approx u(x_{i-1},t_{j-1})$ for $1\leq i \leq N+1$ and
$1 \leq j \leq M+1$.
\small
\begin{verbatim}
function [X,T,U] = crank_nicolson(f,gb,gl,gr,c,N,M,a,b,t1,t2)
  np1 = N + 1;
  mp1 = M + 1;
  U = repmat(NaN,np1,mp1);
  X = linspace(a,b,np1);
  T = linspace(t1,t2,mp1);

  % Initial condition
  for i=1:1:np1
      U(i,1) = gb(X(i));
  end
  % Boundary conditions
  for j=1:1:mp1
      U(1,j) = gl(T(j));
      U(np1,j) = gr(T(j));
  end

  deltax = (b-a)/N;
  deltat = (t2-t1)/M;
  alpha = deltat*(c/deltax)^2/2;

  nm1 = N - 1;
  J = repmat(0,nm1,nm1);
  K = repmat(0,nm1,nm1);
  for i=1:1:nm1
      for j=i-1:1:i+1
          if (i == j)
              K(i,j) = -1 + 2 * alpha;
              J(i,j) = 1 + 2 * alpha;
          elseif ( j > 0 && j < N )
              K(i,j) = -alpha;
              J(i,j) = -alpha;
          end
      end
  end

  % We use the fact that the matrix Q is block lower triangular,
  % and only the diagonal and lower diagonal contain non-trivial blocks.
  nm2 = N - 2;
  B = repmat(NaN,nm1,1);
  B(1,1) = U(2,1) + alpha*(U(1,1) -2*U(2,1) + U(3,1) + U(1,2)) ...
           + (f(X(2),T(1)) + f(X(2),T(2)))*deltat/2;
  for k=2:1:nm2
      B(k,1) = U(k+1,1) + alpha*(U(k,1) -2*U(k+1,1) + U(k+2,1)) ...
               + (f(X(k),T(1)) + f(X(k),T(2)))*deltat/2;
  end
  B(nm1,1) = U(N,1) + alpha*(U(nm1,1) -2*U(N,1) + U(np1,1) +U(np1,2)) ...
      + (f(X(N),T(1)) + f(X(N),T(2)))*deltat/2;
  % Solve the system J U_2 = B_1 with matlab library
  U(2:N,2) = linsolve(J,B);
  % Solve the system J U_2 = B_1 with gauss()
  % U(2:N,2) = gauss(J,B,1);

  for j=2:1:M
      jp1 = j + 1;
      B(1,1) = alpha*(U(1,j) + U(1,jp1)) ...
               + (f(X(2),T(j)) + f(X(2),T(jp1)))*deltat/2;
      for k=2:1:nm2
          B(k,1) = (f(X(k),T(j)) + f(X(k),T(jp1)))*deltat/2;
      end
      B(nm1,1) = alpha*(U(np1,j) + U(np1,jp1)) ...
          + (f(X(N),T(j)) + f(X(N),T(jp1)))*deltat/2;
      % Solve the system J U_{j+1} = B_j - K U_j with matlab library
      U(2:N,jp1) = linsolve(J,B-K*U(2:N,j));
      % Solve the system J U_{j+1} = B_j - K U_j with gauss()
      % U(2:N,jp1) = gauss(J,B-K*U(2:N,j),1);
  end
end
\end{verbatim}
\end{code}

The comment in Remark~\ref{SpecialLinSyst} below is very
relevant for the previous code because the matrix $J$ can still be
large.

\begin{egg}
Use the Crank-Nicolson scheme to approximate the solution to
the following heat equation with forcing.
\[
\pdydx{u}{t} = 0.5^2 \pdydx{u}{x}{2} + xy;
\]
on the domain $R=[0,2] \times [0,4]$ with the initial condition
$u(x,0) = x(2-x)$ for $0 \leq x \leq 2$, and the boundary conditions
$u(0,t) = t(4-t)$ and $u(2,t) = t(4-t)^2$ for $0 \leq t \leq 4$.

With the matlab code below, we got the following graph for the
approximation of the solution $u$.
\figbox{finite_diff/crank_nicolson}{7cm}

\begin{code}
\small
\begin{verbatim}
f = @(x,y) x.*y;
gb = @(x) x.*(2-x);
gl = @(y) y.*(4-y);
gr = @(y) y.*(4-y).^2;
c = 0.5;
a = 0;
b = 2;
t1 = 0;
t2 = 4;
N = 20;
M = 40;

[X,T,U] = crank_nicolson(f,gb,gl,gr,c,N,M,a,b,t1,t2);
[XX,TT] = meshgrid(X,T);

% We need to transpose the matrix U because meshgrid()
% transposes the coordinates.
surf(XX,TT,U');
xlabel('x')
ylabel('t')
zlabel('u')
\end{verbatim}
\end{code}
\end{egg}

\subsection{Elliptic Equations} \label{DirEquSection}

We consider the Dirichlet equation
\begin{equation} \label{DirEqu}
\Delta u \equiv \pdydxn{u}{x}{2} + \pdydxn{u}{y}{2} = f
\end{equation}
on the set $R = \left\{ (x,y) : 0 \leq x \leq a \ , \ 0 \leq y \leq b \right\}$
with the boundary conditions
\[
u\big|_{\partial R} = g \ .
\]
We assume that $f:R \rightarrow \RR$ and $g:\partial R \rightarrow \RR$
are continuous functions.

Given two integers $N\geq 2$ and $M\geq 2$, we set $\dtx{x} = a/N$,
$\dtx{y} = b/M$, $x_i = i \dtx{x}$, $y_j = j \dtx{y}$ and
$\displaystyle u_{i,j} = u\left(x_i,y_j\right)$
for $0\leq i \leq N$ and $0\leq j \leq M$.
From (\ref{fdm_soDD}) in terms of $x$ and $y$, we get
\begin{equation} \label{fdm_sch2S_exp}
\begin{split}
&\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\dtx{x})^2}
- \frac{1}{4!} 
\left( \pdydxn{u}{x}{4}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{u}{x}{4}\left(\eta_{i,j},y_j\right) \right) (\dtx{x})^2 \\
&\qquad + \frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\dtx{y})^2}
- \frac{1}{4!} \left( \pdydxn{u}{y}{4}\left(x_i,\mu_{i,j} \right)
+ \pdydxn{u}{y}{4}\left(x_i,\nu_{i,j} \right) \right) (\dtx{y})^2 =
f(x_i,y_j)
\end{split}
\end{equation}
for $\zeta_{i,j}, \eta_{i,j} \in ]x_{i-1},x_{i+1}[$ and
$\mu_{i,j}, \nu_{i,j} \in ]y_{j-1},y_{j+1}[$.  For
$\dtx{x}$ and $\dtx{y}$ small, we have
\[
\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\dtx{x})^2}
+ \frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\dtx{y})^2} \approx
f(x_i,y_j) \ .
\]
This suggests the following finite difference equation.
\begin{equation} \label{fdm_sch2}
\frac{w_{i+1,j} - 2w_{i,j} + w_{i-1,j}}{(\dtx{x})^2}
+ \frac{w_{i,j+1} - 2w_{i,j} + w_{i,j-1}}{(\dtx{y})^2} =
f(x_i,y_j)
\end{equation}
for $0 < i< N$ and $0 < j < M$.  The boundary conditions impose
\begin{align*}
w_{i,0} &= g(x_i,c) \ \text{and} \ w_{i,M} = g(x_i,d) \ \text{for}
\ 0\leq i \leq N \ ,
\intertext{and}
w_{0,j} &= g(a,y_j) \ \text{and} \ w_{a,j} = g(b,y_j) \ \text{for}
\ 0\leq j \leq M \ .
\end{align*}

We get the following finite difference scheme for the Dirichlet
equation (\ref{DirEqu}).

\begin{algo} \label{fdm_sch2S}
\[
w_{i,j+1} - 2w_{i,j} + w_{i,j-1}
+ \alpha \, \left( w_{i+1,j} - 2w_{i,j} + w_{i-1,j}  \right) =
f(x_i,y_j)(\dtx{y})^2
\]
for $0 < i < N$ and $0 < j < M$, where
$\displaystyle \alpha = \frac{(\dtx{y})^2}{(\dtx{x})^2}$,
$w_{i,0} = g(x_i,c)$ and $w_{i,M} = g(x_i,d)$ for $0\leq i \leq N$, and
$w_{0,j} = g(a,x_j)$ and $w_{N,j} = g(b,x_j)$ for $0\leq j \leq M$.
\end{algo}

\pdfF{finite_diff/fdm_fig2}{Finite difference scheme for the
Dirichlet equation}{Schematic representation of the finite difference
scheme given in Algorithm~\ref{fdm_sch2S}.}{fdm_fig2}

This finite difference scheme is illustrated in Figure~\ref{fdm_fig2}.

It can be expressed as a linear system $A\VEC{w} = \VEC{B}$.
The (column) vector $\VEC{w}$ is defined by
\[
\VEC{w} = \begin{pmatrix}
\VEC{w}_1 \\ \VEC{w}_2 \\ \vdots \\ \VEC{w}_{M-1}
\end{pmatrix}
\quad \text{with} \quad
\VEC{w}_j =
\begin{pmatrix} w_{1,j} \\ w_{2,j} \\ \vdots \\ w_{N-1,j} \end{pmatrix}
\]
for $0 < j < M$.  The matrix $A$ is a \nm{(N-1)(M-1)}{(N-1)(M-1)} matrix of
the form
\[
A = \begin{pmatrix}
J & \Id & 0 & 0 & \ldots & 0 & 0 & 0 \\
\Id & J & \Id & 0 & \ldots & 0 & 0 & 0 \\
0 & \Id & J & \Id & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & \Id & J & \Id \\
0 & 0 & 0 & 0 & \ldots & 0 & \Id & J
\end{pmatrix} \ ,
\]
where $\Id$ is the \nm{(N-1)}{(N-1)} identity matrix and
\begin{equation} \label{fdm_JE}
J = \begin{pmatrix}
-2-2\alpha & \alpha & 0 & 0 & 0 & \ldots & 0 & 0 \\
\alpha & -2-2\alpha & \alpha & 0 & 0 & \ldots & 0 & 0  \\
0 & \alpha & -2-2\alpha & \alpha & 0 & \ldots & 0 & 0 \\
0 & 0 & \alpha & -2-2\alpha & \alpha & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & \cdots & \alpha & -2-2\alpha
\end{pmatrix}    
\end{equation}
is a \nm{(N-1)}{(N-1)} matrix.
The vector $\VEC{B}$ is defined by
\[
\VEC{B} = \begin{pmatrix} \VEC{B}_1 \\ \VEC{B}_2 \\ \vdots \\
\VEC{B}_{M-1} \end{pmatrix} \ ,
\text{where} \quad
\VEC{B}_1 = \begin{pmatrix}
-w_{1,0} - \alpha w_{0,1} + f(x_1,y_1)(\dtx{y})^2\\
-w_{2,0} + f(x_2,y_1)(\dtx{y})^2 \\
-w_{3,0} + f(x_3,y_1)(\dtx{y})^2 \\
\vdots \\
-w_{N-1,0} -\alpha w_{N,1}+ f(x_{N-1},y_1)(\dtx{y})^2\\
\end{pmatrix}
\ ,
\]
\[
\VEC{B}_j = \begin{pmatrix}
-\alpha w_{0,j} + f(x_1,y_j)(\dtx{y})^2\\
f(x_2,y_j)(\dtx{y})^2 \\
f(x_3,y_j)(\dtx{y})^2 \\
\vdots \\
-\alpha w_{N,j} + f(x_{N-1},y_j)(\dtx{y})^2
\end{pmatrix}
\]
for $1 <j < M-1$, and
\[\VEC{B}_{M-1} = \begin{pmatrix}
-w_{1,M} - \alpha w_{0,M-1} + f(x_1,y_{M-1})(\dtx{y})^2\\
-w_{2,M} + f(x_2,y_{M-1})(\dtx{y})^2 \\
-w_{3,M} + f(x_3,y_{M-1})(\dtx{y})^2 \\
\vdots \\
-w_{N-1,M} -\alpha w_{N,M-1} + f(x_{N-1},y_{M-1})(\dtx{y})^2
\end{pmatrix} \ .
\]

\begin{code}
To approximate the solution of the Dirichlet equation
\[
  \pdydxn{u}{x}{2} + \pdydxn{u}{y}{2} = f(x,y)
\]
on the region $R = [a,b]\times[c,d]$ with the boundary conditions
$u(x,c) = g_b(x)$ and $u(x,d) = g_t(x)$ for $a \leq x \leq b$, and
$u(a,y) = g_l(y)$ and $u(b,y) = g_r(y)$ for $c \leq y \leq d$.

\subI{Input} The right hand side $f$.\\
The boundary condition $g_b(x)$ when $y = c$.\\
The boundary condition $g_t(x)$ when $y = d$.\\
The boundary condition $g_l(y)$ when $x = a$.\\
The boundary condition $g_r(y)$ when $x = b$.\\
The number of partitions $N$ of $[a,b]$ with $\dtx{x} = (b-a)/N$.\\
The number of partitions $M$ of $[c,d]$ with $\dtx{y} = (d-c)/M$.  \\
The endpoints $a < b$ of the $x$-interval for the domain $R$.\\
The endpoints $c < d$ of the $y$-interval for the domain $R$.\\
\subI{Output}
X contains the $x$-coordinates $x_0$, $x_1$, \ldots, $x_N$ of the
mesh points in the domain $R$.\\
Y contains the $y$-coordinates $y_0$, $y_1$, \ldots, $y_M$ of the
mesh points in the domain $R$.\\
U contains the approximations of $u$ at the mesh points.
$U_{i,j} \approx u(x_{i-1},y_{j-1})$ for $1\leq i \leq N+1$ and
$1 \leq j \leq M+1$.
\small
\begin{verbatim}
function [X,Y,U] = dirichletS1(f,gb,gt,gl,gr,N,M,a,b,c,d)
  np1 = N + 1;
  mp1 = M + 1;
  U = repmat(NaN,np1,mp1);
  X = linspace(a,b,np1);
  Y = linspace(c,d,mp1);

  % Boundary conditions
  for i=1:1:np1
     U(i,1) = gb(X(i));
     U(i,mp1) = gt(X(i));
  end
  for j=1:1:mp1
     U(1,j) = gl(Y(j));
     U(np1,j) = gr(Y(j));
  end

  deltax = (b-a)/N;
  deltay = (d-c)/M;
  alpha = (deltay/deltax)^2;

  nm1 = N - 1;
  J = repmat(0,nm1,nm1);
  for i=1:1:nm1
     for j=i-1:1:i+1
        if (i == j)
           J(i,j) = -2 -2 * alpha; 
        elseif ( j > 0 && j < N )
           J(i,j) = alpha;
        end
     end
  end

  mm1 = M - 1;
  nm2 = N - 2;
  MNm1 = mm1*nm1;
  deltay2 = deltay^2;
  Q = repmat(0,MNm1,MNm1);
  B = repmat(NaN,MNm1,1);
  for i=1:1:mm1
     for j=i-1:1:i+1
        if (i == j)
           Q((j-1)*nm1+1:j*nm1,(i-1)*nm1+1:i*nm1) = J; 
        elseif ( j > 0 && j < M )
           Q((j-1)*nm1+1:j*nm1,(i-1)*nm1+1:i*nm1) = eye(nm1);
        end
     end
     im1 = i - 1;
     ip1 = i + 1;
     if (i == 1)
        B(im1*nm1+1,1) = -U(2,1) -alpha*U(1,ip1) +f(X(2),Y(ip1))*deltay2;
        for k=2:1:nm2
           B(im1*nm1+k,1) = -U(k+1,1) + f(X(k+1),Y(ip1))*deltay2;
        end
        B(i*nm1,1) = -U(N,1) -alpha*U(np1,ip1) +f(X(N),Y(ip1))*deltay2;
     elseif (i == mm1)
        B(im1*nm1+1,1) = -U(2,mp1) -alpha*U(1,ip1) +f(X(2),Y(ip1))*deltay2;
        for k=2:1:nm2
           B(im1*nm1+k,1) = -U(k+1,mp1) + f(X(k+1),Y(ip1))*deltay2;
        end
        B(i*nm1,1) = -U(N,mp1) -alpha*U(np1,ip1) +f(X(N),Y(ip1))*deltay2;
     else
        B(im1*nm1+1,1) = -alpha*U(1,ip1) +f(X(2),Y(ip1))*deltay2;
        for k=2:1:nm2
           B(im1*nm1+k,1) = f(X(k+1),Y(ip1))*deltay2;
        end
        B(i*nm1,1) = -alpha*U(np1,ip1) +f(X(N),Y(ip1))*deltay2;
     end
  end

  % Solve the system Q UU = B with matlab library
  UU = linsolve(Q,B);
  % Solve the system Q UU = B with gauss()
  % UU = gauss(Q,B,1);

  % Transfer UU to U
  for i=1:1:mm1
     im1 = i - 1;
     ip1 = i + 1;
     U(2:N,ip1) = UU(im1*nm1+1:i*nm1,1);
  end
end
\end{verbatim}
\end{code}

\begin{rmk}
There is one issue with the code above when the mesh sizes are really
small.  The matrix $Q$ may be very large.  So, simple Gauss
elimination is not recommended to solve $Q U = B$.  The matrix $Q$ is
block tridiagonal as are the matrices $J$ and $K$.  It is therefore
really important to develop efficient and economical methods to solve
very large linear system of this form.  This is outside the scope of
this manuscript.  A good starting reference is \cite{GvL}.
\label{SpecialLinSyst}
\end{rmk}

\begin{egg}
Use the previous finite difference scheme to approximate the solution to
the following Dirichlet equation.
\[
\pdydxn{u}{x}{2} + \pdydxn{u}{y}{2} = xy;
\]
on the domain $R=[0,2] \times [0,4]$ with the boundary conditions
$u(x,0) = x(2-x)$ and $u(x,4) = x(2-x)^2$ for $0 \leq x \leq 2$, and
$u(0,y) = y(4-y)$ and $u(2,y) = y(4-y)^2$ for $0 \leq y \leq 4$.

With the matlab code below, we got the following graph for the
approximation of the solution $u$.
\figbox{finite_diff/dirichletS1}{7cm}

\begin{code}
\small
\begin{verbatim}
f = @(x,y) x.*y;
gb = @(x) x.*(2-x);
gt = @(x) x.*(2-x).^2;
gl = @(y) y.*(4-y);
gr = @(y) y.*(4-y).^2;
a = 0;
b = 2;
c = 0;
d = 4;
N = 10;
M = 20;

[X,Y,U] = dirichletS1(f,gb,gt,gl,gr,N,M,a,b,c,d);
[XX,YY] = meshgrid(X,Y);

% We need to transpose the matrix U because meshgrid()
% transposes the coordinates.
surf(XX,YY,U')
xlabel('x')
ylabel('y')
zlabel('u')
\end{verbatim}
\end{code}
\end{egg}

\subsection{Hyperbolic Equations} \label{DerWaveSection}

We consider the wave equation
\begin{equation}\label{WaveEqu}
  \pdydxn{u}{t}{2} = c^2 \pdydxn{u}{x}{2} \quad , \quad
  0 < x < L \ \text{and} \ 0<t<T \ ,
\end{equation}
with the boundary conditions
\begin{equation}\label{WaveEquBC}
u(0,t)=h_0(t) \ \text{and} \ u(L,t)=h_L(t) \quad , \quad 0\leq t \leq T \ ,
\end{equation}
and the initial conditions
\begin{equation} \label{WaveEquIC}
u(x,0) = g(x) \ \text{and} \ \pdydx{u}{t}(x,0) = f(x) \quad , \quad
 0 \leq x \leq L \ ,
\end{equation}
where $g:[0,L]\rightarrow \RR$ is a continuous function satisfying
$g(0)=h_0(0)$ and $g(L)=h_L(0)$, and $f:[0,L]\rightarrow \RR$ is also
continuous.

We develop a finite difference scheme for the wave equation given
in (\ref{WaveEqu}), (\ref{WaveEquBC}) and (\ref{WaveEquIC}).

Given two integers $N\geq 2$ and $M\geq 1$, we set $\dtx{x} = L/N$,
$\dtx{t} = T/M$, $x_i = i \dtx{x}$, $t_j = j \dtx{t}$ and
$\displaystyle u_{i,j} = u\left(x_i,t_j\right)$
for $0\leq i \leq N$ and $0\leq j \leq M$.
From (\ref{fdm_soDD}) for $x$ and $t$, we get
\begin{equation} \label{fdm_sch3_tr}
\begin{split} 
&\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\dtx{t})^2}
- \frac{1}{4!} \left( \pdydxn{u}{t}{4}\left(x_i,\zeta_{i,j} \right)
+ \pdydxn{u}{t}{4}\left(x_i,\eta_{i,j} \right) \right) (\dtx{t})^2 \\
&\qquad = c^2 \, \frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\dtx{x})^2}
- \frac{c^2}{4!} \left( \pdydxn{u}{x}{4}\left(\mu_{i,j},t_j\right)
+ \pdydxn{u}{x}{4}\left(\nu_{i,j},t_j\right) \right) (\dtx{x})^2
\end{split}
\end{equation}
for $\zeta_{i,j}, \eta_{i,j} \in ]t_{j-1},t_{j+1}[$ and
$\mu_{i,j}, \nu_{i,j} \in ]x_{i-1},x_{i+1}[$.
For $\dtx{t}$ and $\dtx{x}$ small, we have
\[
\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{(\dtx{t})^2} \approx c^2 \,
\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{(\dtx{x})^2} \ .
\]
This suggests the following finite difference equation.
\begin{equation} \label{fdm_sch3}
\frac{w_{i,j+1} - 2w_{i,j} + w_{i,j-1}}{(\dtx{t})^2} = c^2 \,
\frac{w_{i+1,j} - 2w_{i,j} + w_{i-1,j}}{(\dtx{x})^2}
\end{equation}
for $0 < i <N$ and $0 < j < M$.

Since $w_{i,-1}$ is not defined for $0<i<N$, (\ref{fdm_sch2}) cannot
be used for $j=0$.  Thus, the value of $w_{i,1}$ for $0<i<N$ cannot
be computed with (\ref{fdm_sch2}).  The initial condition of
$\displaystyle \pdydx{u}{t}$ is useful here.  The initial condition of
$\displaystyle \pdydx{u}{t}$ may be evaluated with the formula
(\ref{fdm_soD}).  If we assume that $u$ is defined for $t<0$, we may
write
\[
\pdydx{u}{t}(x_i,0) = \frac{u_{i,1}- u_{i,-1}}{2\dtx{t}}
- \frac{1}{12} \left( \pdydxn{u}{t}{3}\left(x_i,\tilde{\zeta}_i\right)
+ \pdydxn{u}{t}{3}\left(x_i,\tilde{\eta}_i\right)\right) (\dtx{t})^2
\]
for some $\tilde{\zeta}_i, \tilde{\eta}_i \in ]t_{-1},t_1[$.  We choose this
finite difference formula to approximate $\displaystyle \pdydx{u}{t}$
because, for $\dtx{t}$ near $0$, its local truncation error
\[
- \frac{1}{12} \left( \pdydxn{u}{t}{3}\left(x_i,\tilde{\zeta}_i\right)
+ \pdydxn{u}{t}{3}\left(x_i,\tilde{\eta}_i\right)\right) (\dtx{t})^2
= O\left((\dtx{t})^2\right)
\]
is comparable to the local truncation error
\[
- \frac{1}{4!} \left( \pdydxn{u}{t}{4}\left(x_i,\zeta_{i,j} \right)
+ \pdydxn{u}{t}{4}\left(x_i,\eta_{i,j} \right) \right) (\dtx{x})^2
= O\left((\dtx{t})^2\right)
\]
of the finite difference formula that has been
used to approximate $\displaystyle \pdydxn{u}{t}{2}$.  We have for
$\dtx{t}$ small enough that
\[
\pdydx{u}{t}(x_i,0) \approx \frac{u_{i,1}- u_{i,-1}}{2\dtx{t}}
\quad , \quad 0 \leq i \leq N \ .
\]
Thus
\[
u_{i,-1} \approx u_{i,1} - 2 \pdydx{u}{t}(x_i,0) \dtx{t} \quad ,
\quad 0 \leq i \leq N \ .
\]
This suggests the following formula for $w_{i,-1}$.
\[
w_{i,-1} = w_{i,1} - 2 \,w_{i,0}'\dtx{t} \quad , \quad 0 \leq i \leq N \ ,
\]
where
\[
w_{i,0}' = \pdydx{u}{t}(x_i,0) = f(x_i) \quad , \quad 0 \leq i \leq N \ .
\]
The initial condition on $u$ imposes $w_{i,0} = g(x_i)$ for $0 \leq i \leq N$.
The boundary conditions impose $w_{0,j} = h_0(t_j)$ and $w_{N,j} = h_L(t_j)$
for $0\leq j \leq M$.

We finally get the following finite difference scheme.

\begin{algo} \label{fdm_sch3S}
\[
w_{i,j+1} - 2w_{i,j} + w_{i,j-1} - \alpha \left( w_{i+1,j} - 2w_{i,j} +
w_{i-1,j}\right) = 0
\]
for $0 < i < N$ and $0 \leq j < M$, where
$\displaystyle \alpha = \frac{c^2(\dtx{t})^2}{(\dtx{x})^2}$,
$w_{0,j} = h_0(t_j)$ and $w_{N,j} = h_L(t_j)$ for $0\leq j \leq M$,
$w_{i,0} = g(x_i)$ for $0 \leq i \leq N$, and
$w_{i,-1} = w_{i,1} - 2\,f(x_i)\dtx{t}$ for $0 \leq i \leq N$.
\end{algo}

\pdfF{finite_diff/fdm_fig4}{Finite difference scheme for the
wave equation}{Schematic representation of the finite difference
scheme given in Algorithm~\ref{fdm_sch3S}.}{fdm_fig4}

This scheme is illustrated in Figure~\ref{fdm_fig4}.

As the previous schemes, it can be expressed as a linear system of the
form $A\VEC{w} = \VEC{B}$.  The (column) vector $\VEC{w}$ is defined by
\[
\VEC{w} = \begin{pmatrix}
\VEC{w}_1 \\ \VEC{w}_2 \\ \vdots \\ \VEC{w}_M
\end{pmatrix}
\quad \text{with} \quad
\VEC{w}_j =
\begin{pmatrix} w_{1,j} \\ w_{2,j} \\ \vdots \\ w_{N-1,j} \end{pmatrix}
\]
for $0 < j \leq M$.
The matrix $A$ is a \nm{(N-1)M}{(N-1)M} matrix of the form
\[
A = \begin{pmatrix}
\Id & 0 & 0 & 0 & \ldots & 0 & 0 & 0 \\
J & \Id & 0 & 0 & \ldots & 0 & 0 & 0 \\
\Id & J & \Id & 0 & \ldots & 0 & 0 & 0 \\
0 & \Id & J & \Id & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \ldots & \Id & J & \Id
\end{pmatrix} \ ,
\]
where $\Id$ is the \nm{(N-1)}{(N-1)} identity matrix and
\[
J = \begin{pmatrix}
-2+2\alpha & -\alpha & 0 & 0 & 0 & \ldots & 0 & 0 \\
-\alpha & -2+2\alpha & -\alpha & 0 & 0 & \ldots & 0 & 0  \\
0 & -\alpha & -2+2\alpha & -\alpha & 0 & \ldots & 0 & 0 \\
0 & 0 & -\alpha & -2+2\alpha & -\alpha & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & \cdots & -\alpha & -2+2\alpha
\end{pmatrix}
\]
is a \nm{(N-1)}{(N-1)} matrix.  The vector $\VEC{B}$ is defined by
\[
\VEC{B} = \begin{pmatrix} \VEC{B}_1 \\ \VEC{B}_2 \\ \vdots \\
\VEC{B}_M \end{pmatrix} \ , \quad \text{where} \ 
\VEC{B}_1 = \frac{1}{2} \begin{pmatrix}
(2-2\alpha) w_{1,0} + \alpha w_{0,0} + \alpha w_{2,0}
+ 2\,w_{1,0}' \dtx{t} \\
(2-2\alpha)w_{2,0} + \alpha w_{1,0} + \alpha w_{3,0}
+ 2\,w_{2,0}' \dtx{t} \\
(2-2\alpha)w_{3,0} + \alpha w_{2,0} + \alpha w_{4,0}
+ 2\,w_{3,0}' \dtx{t} \\
\vdots \\
(2-2\alpha)w_{N-1,0} + \alpha w_{N-2,0} + \alpha w_{N,0}
+ 2\,w_{N-1,0}' \dtx{t} \\
\end{pmatrix} \ ,
\]
\[
\VEC{B}_2 = \begin{pmatrix}
-w_{1,0} + \alpha w_{0,1}\\
-w_{2,0} \\
-w_{3,0} \\
\vdots \\
-w_{N-1,0} + \alpha w_{N,1} \\
\end{pmatrix}
\ \text{and} \quad
\VEC{B}_j = \begin{pmatrix}
\alpha w_{0,j-1}\\
0\\
0\\
\vdots \\
\alpha w_{N,j-1}
\end{pmatrix}
\]
for $3 \leq j \leq M$.

\section{Convergence, Consistency and Stability} \label{fdm_CCS}

The presentation in this section is based on \cite{IK,WH,I}.

There are three questions that come to mind when using a finite
difference scheme to numerically solve a partial differential equation.

\begin{enumerate}
\item Is there a solution to the system $A\VEC{w} = \VEC{B}$
  associated to a finite difference scheme and, if so, is this
  solution unique?
\item Since computations cannot be performed exactly on computers
  (round off errors), and since the boundary and initial conditions
  are often approximations of the true values (experimental values) or
  cannot be entered exactly on computer (round off errors), is the
  finite difference scheme ``stable?''\quad  Namely, if the computed
  value at one step of the finite difference scheme is slightly modified,
  will the computed value at the following step be close to the value
  that should have been found if the previous value had not been
  modified.  If the method is not stable, we cannot hope to get
  reliable results.
\item Is the solution to the finite difference scheme close to the
  solution of the partial differential equation from which we have
  developed the finite difference scheme?
\end{enumerate}

The first question is easy to answer positively because the matrices
$A$ obtained from the finite difference schemes that we have presented
are invertible.  As we will see when studying the stability of the
finite difference scheme, the matrices $A$ have non-zero eigenvalues
and so a non-zero determinant.  Hence there exists a unique solution
to $A\VEC{w}=\VEC{B}$.

\subsection{Uniform Theory}

We assume that the {\bfseries domain} for the partial differential
equation is
$R = \{ (x,y) : a \leq x \leq b \ \text{and} \ x \leq y \leq d \}$.
The {\bfseries boundary} of $R$, denoted $\partial R$, is the set of points
$(x,y)$ where conditions are imposed on $u$.  The {\bfseries interior}
of $R$ is defined as the set $R^o = R \setminus \partial R$. 
Be aware that the definition of boundary and interior of a set given
here may not coincide with the normal definition of border and
interior of a set in topology.

Once the step sizes $\dtx{x} = (b-a)/N$ and $\dtx{y}=(d-c)/M$ have
been selected, we define the
{\bfseries domain for the finite difference scheme}\index{Finite
Difference Methods!Domain} as
\begin{equation} \label{fdm_DomianDef}
R_\Delta = \{ (x_i,y_j) : x_i = i \dtx{x} \ \text{for} \ 0 \leq i \leq N,
\ \text{and} \ y_j = j \dtx{y} \ \text{for} \ 1 \leq j \leq M \} \ .
\end{equation}
The {\bfseries boundary}\index{Finite Difference Methods!Boundary of
the Domain} of $R_\Delta$, denoted $\partial R_\Delta$, is the set of
mesh points $(x_i,y_j) \in \partial R$.  The
{\bfseries interior}\index{Finite Difference Methods!Interior of
the Domain} of $R_\Delta$ is defined as the set
$R^o_\Delta = R_\Delta \setminus \partial R$.

\begin{egg}
For the heat and wave equations, $y$ is replaced by $t$ and the
boundary of $R$ is defined as the set
$\partial R  = \{ (x,0) : 0 \leq x \leq L \} \cup
\{ (x,t) : x = 0 \ \text{or} \ L, \ \text{and} \ 0 \leq t \leq T \}$.
The boundary of $R_\Delta$ is defined as the set
\[
\partial
R_\Delta = \{ (x_i,0) : x_i = i \dtx{x} \ \text{for} \ 0 \leq i \leq N \}
\cup 
\{ (x,t_j) : x = 0 \ \text{or} \ L ,\ \text{and} \ t_j = j \dtx{t}
\ \text{for} \ 1 \leq j \leq M \} \ .
\]
For the Dirichlet equation, the boundaries of $R$ and
$R_\delta$ have the expected definition:
$\partial R  = \{ (x,y) : y = c \ \text{or} \ d ,\ \text{and}
\ a \leq x \leq b \} \cup
\{ (x,y) : x = a \ \text{or} \ b ,\ \text{and} \ c \leq y \leq d \}$
and
\begin{align*}
\partial
R_\Delta &= \{ (x_i,y) : y = c \ \text{or} \ d ,\ \text{and}
\ x_i = i \dtx{x} \ \text{for} \ 0 \leq i \leq N \} \\
&\qquad \cup 
\{ (x,y_j) : x = a \ \text{or} \ b ,\ \text{and} \ y_j = j \dtx{y}
\ \text{for} \ 1 \leq j \leq M \} \ .
\end{align*}
\end{egg}

The partial differential equations that we are considering are of the form 
\begin{equation} \label{fdm_pde_tr}
P\left(u(x,y), \pdydx{u}{x}(x,y), \pdydx{u}{y}(x,y),
\pdydxn{u}{x}{2}(x,y), \ldots \right)= F(x,y) \ ,
\end{equation}
where $P$ is a linear mapping and $F:R\to \RR$ is a given function.
The finite difference schemes that we have deduced to numerically
solve these partial differential equations are based
on finite difference equations of the form
\begin{equation} \label{fdm_fde_tr}
P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right)
= F(x_i,y_j)
\end{equation}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$, where
$P_\Delta$ is also a linear mapping.  These schemes were deduced
in Section~\ref{EXplImplSchenes} from the expressions that we got
after substituting finite difference formulae for the partial
derivatives at $(x_i,y_j)$ into the heat, Dirichlet and wave equations.

\begin{egg}
For the finite difference scheme in Algorithm~\ref{fdm_sch1S}, we have
\[
P\left(u(x,y), \pdydx{u}{x}(x,y), \pdydx{u}{y}(x,y),
\pdydxn{u}{x}{2}(x,y), \ldots \right)
= \pdydx{u}{t} - c^2 \pdydxn{u}{x}{2}
\]
and
\begin{equation}\label{fdm_PDelta_egg}
  P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right)
= \frac{w_{i,j+1} - w_{i,j}}{\dtx{t}} - c^2 \frac{w_{i+1,j} - 2w_{i,j} +
w_{i-1,j}}{(\dtx{x})^2} \ .
\end{equation}
\end{egg}

In the definition of $P_\Delta$ given in (\ref{fdm_fde_tr}), we are
referring to $(x_i,y_j) \in R^o_\Delta$.  This is not really correct.
As for the finite difference scheme in (\ref{fdm_PDelta_egg}) above,
the formula (\ref{fdm_fde_tr}) is used to approximate the value of
$u(x_i,y_{j+1})$.  It is $(x_i,y_{j+1})$ which is really in
$R^o_\Delta$.   The point $(x_i,y_j)$ may be on the boundary as
it is the case in (\ref{fdm_PDelta_egg}) for $j=0$.  Nevertheless,
we prefer to use the formulation above because it expresses more
clearly that formula (\ref{fdm_fde_tr}) is used to approximate a value
of $u$ at an interior point.

As we had to do for the wave equation, we may also have to approximate
the boundary and/or initial conditions of $u$ on $\partial R_\Delta$.
These conditions are given by a formula of the form 
\begin{equation}\label{fdm_pde_tr_bdry}
  B\left(u(x,y), \pdydx{u}{x}(x,y), \pdydx{u}{y}(x,y), \ldots \right) = G(x,y)
\end{equation}
evaluated on $\partial R$. where $B$ is a linear mapping and
$G:\partial R \to \RR$ is a given function.
The approximation of the boundary or initial condition at each mesh
points $(x_i,y_j)$ of $\partial R_\Delta$ is given by
a formula of the form
\begin{equation}\label{fdm_fde_tr_bdry}
  B_\Delta(w_{i,j},w_{i,j+1},w_{i+1,j}, \ldots) = G(x_i,y_j)
\end{equation}
for all $(i,j)$ such that $(x_i,y_j) \in \partial R_\Delta$,
where $B_\Delta$ is a linear mapping.  This formula is also
deduced from the expressions that we got after substituting finite
difference formulae for the partial derivatives at
$(x_i,y_j)\in \partial R_\Delta$ into the boundary and initial conditions.

\begin{egg}
For the heat equation with forcing, we have
\[
B\left(u(x,t), \displaystyle \pdydx{u}{x}(x,t), \displaystyle
  \pdydx{u}{t}(x,t),\ldots \right) = u(x,t)
\]
for all $(x,t) \in \partial R$,
$B_\Delta(w_{i,j},w_{i,j+1},w_{i+1,j}, \ldots) = w_{i,j}$
for all $(i.j)$ such that $(x_i,t_j) \in \partial R_\Delta$, and
\[
G(x,t) =
\begin{cases}
h_0(t) & \text{for} \ x =0 \ \text{and} \ 0 \leq t \leq T\\
h_L(t) & \text{for} \ x = L \ \text{and} \ 0 \leq t \leq T\\
g(x) & \text{for} \ t =0 \ \text{and}\ 0 \leq x \leq L
\end{cases}
\]
for all $(x,t) \in \partial R$.

For the Dirichlet equation, we have
\[
B\left(u(x,y), \displaystyle \pdydx{u}{x}(x,y), \displaystyle
  \pdydx{u}{y}(x,y),\ldots \right) = u(x,y)
\]
for all $(x,y) \in \partial R$,
$B_\Delta(w_{i,j},w_{i,j+1},w_{i+1,j}, \ldots) = w_{i,j}$
for all $(i.j)$ such that $(x_i,y_j) \in \partial R_\Delta$, and
$G(x,y) = g(x,y)$ for all $(x,y) \in \partial R$.

For the wave equation, we may choose
\[
B\left(u(x,t), \pdydx{u}{x}(x,t), \pdydx{u}{t}(x,t), \ldots \right)
=
\begin{cases}
\begin{pmatrix} u(x,t) \\ u(x,t) \end{pmatrix}
& \text{for} \ x=0 \ \text{or} \ x=L, \ \text{and} \ 0 \leq t \leq T \\[1em]
\begin{pmatrix} u(x,t) \\ \displaystyle \pdydx{u}{t}(x,t) \end{pmatrix}
& \text{for} \ t = 0 \ \text{and} \ 0 \leq x \leq L
\end{cases}
\]
for all $(x,t) \in \partial R$,
\[
B_\Delta(w_{i,j},w_{i,j+1},w_{i+1,j}, \ldots)
\begin{cases}
\begin{pmatrix} w_{i,j} \\ w_{i,j} \end{pmatrix}
& \begin{array}{l}
\text{for} \ i=0 \ \text{or} \ i = N, \\
\text{and} \ 0 \leq j \leq M
\end{array} \\
\begin{pmatrix} 0 & 1 & 0 \\ -1/(2\dtx{t}) & 0 & 1/(2\dtx{t}) \end{pmatrix}
\begin{pmatrix} w_{i,-1} \\ w_{i,0} \\ w_{i,1}   \end{pmatrix}
& \ \text{for} \ j = 0 \ \text{and} \ 0 \leq i \leq N
\end{cases}
\]
for all $(i.j)$ such that $(x_i,y_j) \in \partial R_\Delta$, and
\[
G(x,t) =
\begin{cases}
\begin{pmatrix} h_0(t) \\ h_0(t) \end{pmatrix}
& \text{for}\ x =0 \ \text{and} \ 0 < t \leq T\\[1em]
\begin{pmatrix} h_L(t) \\ h_L(t) \end{pmatrix}
& \text{for}\ x = L \ \text{and} \ 0 < t \leq T\\[1em]
\begin{pmatrix} g(x) \\ f(x) \end{pmatrix}
& \text{for}\ t =0 \ \text{, and} \ 0 \leq x \leq L
\end{cases}
\]
for all $(x,t) \in \partial R$.
\end{egg}

To answer the third question in the introduction of this section, we
have to show that the following definition is satisfied.

\begin{defn} \label{fdm_conv_def}
The solution of a finite difference scheme associated to a finite
difference equation $P_\Delta = F$ with conditions $B_\Delta = G$
as in (\ref{fdm_fde_tr}) and (\ref{fdm_fde_tr_bdry})
{\bfseries converges}\index{Finite Difference Methods!Convergence} toward
the solution of the partial differential equation given by
$P = F$ with conditions $B = G$ as in (\ref{fdm_pde_tr}) and
(\ref{fdm_pde_tr_bdry}) if
\[
\max \left\{ \big| w_{i,j} - u_{i,j} \big| : 0\leq i \leq N \ 
\text{and} \ 0\leq j\leq M \right\} \rightarrow 0 \quad \text{as}
\quad \min \{N, M\} \rightarrow \infty \ ,
\]
where $\{w_{i,j} : 0 \leq i \leq N \ \text{and} \ 0 \leq j \leq M\}$
is the solution of the finite difference scheme and $u$ is the
solution of the partial differential equation.
As before $\displaystyle u_{i,j} = u\left(x_i,y_j\right)$ for
$0\leq i \leq N$ and $0\leq j \leq M$.
\end{defn}

\begin{rmk}
Be aware that the previous definition of convergence does not consider
any round off error or perturbation.  Therefore, a method may
theoretically converge according to the previous definition but not
give good results in practice.  Nevertheless, we must at least verify
that a method converges according to the previous definition before
using it.  To keep the presentation to a reasonable level of
sophistication, we will not consider round off error in our
presentation of the finite difference schemes except in some special
cases like when we will discuss stability of finite difference
schemes.
\end{rmk}

Unfortunately, convergence is sometime difficult to prove.  However, it
may not be necessary to prove convergence directly to prove that a
finite difference scheme is convergent as we will see later.  To
justify this approach, we will need the following concepts.

\begin{defn}\label{fdm_consist_def}
Given any sufficiently differentiable function $q:R \to \RR$, the
{\bfseries local truncation error}\index{Finite Difference Methods!Local 
Truncation Error} of the linear mapping $P_\Delta$ in
(\ref{fdm_fde_tr}) is the expression
\begin{align*}
\tau_{i,j}(\dtx{x},\dtx{y},q)
&= P_\Delta\left(q(x_i,y_j), q(x_i,y_{j+1}), q(x_{i+1},y_j), \ldots\right) \\
&\qquad - P\left(q(x_i,y_j), \pdydx{q}{x}(x_i,y_j), \pdydx{q}{y}(x_i,y_j),
\pdydxn{q}{x}{2}(x_i,y_j), \ldots \right)
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.

We also define the {\bfseries local error} for the linear mapping
$B_\Delta$ in (\ref{fdm_fde_tr_bdry}) as
\begin{align*}
\sigma_{i,j}(\dtx{x}.\dtx{y},q)
&= B_\Delta\left(q(x_i,y_j), q(x_i,y_{j+1}), q(x_{i+1},y_j),
\ldots\right) \\
&\qquad - B\left(q(x_i,y_j), \pdydx{q}{x}(x_i,y_j), \pdydx{q}{y}(x_i,y_j),
\ldots \right) \ .
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in \partial R$.

A finite difference scheme determined by the linear mappings
$P_\Delta$ and $B_\Delta$ as in (\ref{fdm_fde_tr}) and
(\ref{fdm_fde_tr_bdry}) is
{\bfseries consistent}\index{Finite Difference Methods!Consistency} if
\[
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in R^o_\Delta}}
|\tau_{i,j}(\dtx{x},\dtx{y},q)| \rightarrow 0 \quad \text{as} \quad
\min \{N, M\} \rightarrow \infty
\]
and
\[
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in \partial R_\Delta}}
\|\sigma_{i,j}(\dtx{x},\dtx{y},q)\| \rightarrow 0 \quad \text{as} \quad
\min \{N, M\} \rightarrow \infty
\]
for all sufficiently differentiable function $q:R \to \RR$.
If there are constraints on the grids used in the two previous limits,
namely on $\dtx{x}$ and $\dtx{y}$, then the finite difference scheme
is said to be {\bfseries conditionally consistent}\index{Finite
Difference Methods!Conditionally Consistent}.
\end{defn}

As mentioned previously, we are using the imprecise reference to
$\tau_{i,j}$ for $(x_i,y_j) \in R^o_\Delta$ though
$(x_i,y_j)$ may not be in $R^o_\Delta$.  The formula
(\ref{fdm_fde_tr}) is used to approximate the value of
$u(x_i,y_{j+1})$.  It is $(x_i,y_{j+1})$ which is really in
$R^o_\Delta$.   The point $(x_i,y_j)$ may be on the boundary.

\begin{rmk}[Warning]
The expression
$\displaystyle P\left(q(x_i,y_j), \pdydx{q}{x}(x_i,y_j), \pdydx{q}{y}(x_i,y_j),
\ldots \right)$ in the definition of
$\displaystyle \tau_{i,j}(\dtx{x},\dtx{y},q)$ may in fact be
a linear mapping of the form
\[
  P\left(q(x_i,y_j), \pdydx{q}{x}(x_i,y_j), \pdydx{q}{y}(x_i,y_j),
\ldots, q(x_i,y_{j+1}), \pdydx{q}{x}(x_i,y_{j+1}),
\pdydx{q}{y}(x_i,y_{j+1}), \ldots \right) \ .
\]
The Crank-Nicolson scheme is an example of this situation.
For this reason, the expression
$\displaystyle P\left(q(x_i,y_j), \pdydx{q}{x}(x_i,y_j), \pdydx{q}{y}(x_i,y_j),
\ldots \right)$ should not be interpreted literally.
For simplicity, we prefer to use the expression of the form
$\displaystyle P\left(q(x_i,y_j), \pdydx{q}{x}(x_i,y_j), \pdydx{q}{y}(x_i,y_j),
\ldots \right)$ to clearly refer to the interior point $(x_i,y_j)$ 
where we try to approximate the value of the solution $u$.
\end{rmk}

\begin{defn}\label{fdmStableDefNo1}
A finite difference scheme determined by the linear mappings
$P_\Delta$ and $B_\Delta$ as in (\ref{fdm_fde_tr}) and
(\ref{fdm_fde_tr_bdry}) is
{\bfseries stable}\index{Finite Difference Methods!Stability} if,
for all function $v: R_\Delta \to \RR$, there exists
a constant $C_\alpha$ such that
\begin{equation} \label{stabCondFDM}
\begin{split}
\max_{\substack{0\leq i\leq N\\0\leq j \leq M}} |v_{i,j}| &\leq
C_\alpha \bigg(
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j)\in R^o_\Delta}}
\left| P_\Delta\left(v_{i,j}, v_{i,j+1}, v_{i+1,j},\ldots\right) \right| \\
&\qquad
+ \max_{\substack{(i,j) \text{ such that}\\ (x_i,y_j)\in \partial R_\Delta}}
\left\| B_\Delta\left(v_{i,j},v_{i,j+1},v_{i+1,j}, \ldots\right) \right\|
\bigg) \ ,
\end{split}
\end{equation}
where $v_{i,j} = v(x_i,y_j)$ for all $i$ and $j$.

The index $\alpha$ for $C_\alpha$ is to indicate that there may be a
constraining relation on $\dtx{x}$ and $\dtx{y}$ that must be
satisfied for (\ref{stabCondFDM}) to be satisfied.  If there is no
constraining relation on $\dtx{x}$ and $\dtx{y}$ used in
(\ref{stabCondFDM}), then the finite difference scheme is said to be
{\bfseries unconditionally stable}\index{Finite Difference
Methods!Unconditionally Stable}.
If there is a constraining relation, then the finite difference scheme
is said to be {\bfseries conditionally stable}\index{Finite Difference
Methods!Conditionally Stable}.
\end{defn}

We have used the norm notation for $\|\sigma_{i,j}\|$ and
$\left\| B_\Delta(v_{i,j},v_{i,j+1},v_{i+1,j}, \ldots) \right\|$
in the previous two definitions because, as we have seen in the previous
example, $B_\Delta(v_{i,j},v_{i,j+1},v_{i+1,j}, \ldots)$ may
be a vector.

To satisfy the notion of stability introduced in the second question
in the introduction to this section, our finite difference schemes
will need to satisfy the previous definition.  To understand why, we
have to consider round off errors.  Suppose that $v_{i,j}$ is the
computed approximation of $w_{i,j}$ for all $i$ and $j$.  We may
assume that
$\{ v_{i,j} : 0 \leq i \leq N \ \text{and} \ 0 \leq j \leq M \}$ is
the exact solution of
\[
P_\Delta\left(v_{i,j}, v_{i,j+1}, v_{i+1,j}, \ldots\right)
= F(x_i,y_j) + \delta_{i,j}(\dtx{x},\dtx{y})
\]
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$, and
\[
B_\Delta(v_{i,j},v_{i,j+1},v_{i+1,j}, \ldots)
= G(x_i,y_j) + \tilde{\delta}_{i,j}(\dtx{x},\dtx{y})
\]
for all $(i,j)$ such that $(x_i,y_j) \in \partial R_\Delta$, where
the $\delta_{i,j}(\dtx{x},\dtx{y})$ and
$\tilde{\delta}_{i,j}(\dtx{x},\dtx{y})$ represent round off errors.
Thus
$\{ v_{i,j} - w_{i,j}: 0 \leq i \leq N \ \text{and} \ 0 \leq j \leq M \}$
satisfies
\[
P_\Delta\left(v_{i,j}-w_{i,j}, v_{i,j+1}-w_{i,j+1},
v_{i+1,j}-w_{i+1.j}, \ldots\right)
= \delta_{i,j}(\dtx{x},\dtx{y})
\]
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$, and
\[
B_\Delta(v_{i,j}-w_{i,j},v_{i,j+1}-w_{i,j+1},v_{i+1,j}-w_{i+1,k}, \ldots)
= \tilde{\delta}_{i,j}(\dtx{x},\dtx{y})
\]
for all $(i,j)$ such that $(x_i,y_j) \in \partial R_\Delta$.  If the
finite difference scheme is stable, there exists a constant $C_\alpha$ such
that
\[
\max_{\substack{0\leq i\leq N\\0\leq j \leq M}} |v_{i,j}-w_{i,j}|
\leq C_\alpha \bigg(
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j)\in R^o_\Delta}}
\left| \delta_{i,j}(\dtx{x},\dtx{y}) \right|
+ \max_{\substack{(i,j) \text{ such that}\\ (x_i,y_j)\in \partial R_\Delta}}
\left\| \tilde{\delta}_{i,j}(\dtx{x},\dtx{y}) \right\|
\bigg)
\]
for all $(i,j)$ such that $(x_i,y_j) \in R_\Delta$.  The error
$|v_{i,j} - w_{i,j}|$ is proportional to the round off errors in our
computations.

The following theorem is quite useful to prove the convergence of a finite
difference scheme.

\begin{theorem} \label{StabConstConv}
Consider finite difference scheme determined by the linear mappings
$P_\Delta$ and $B_\Delta$ as in (\ref{fdm_fde_tr}) and
(\ref{fdm_fde_tr_bdry}).  If this finite difference scheme is stable
and consistent, then it is convergent.
\end{theorem}

\begin{proof}
For every $(x_i,y_j) \in R^o_\Delta$, we have
\begin{align*}
0 &= F(x_i,y_j) - F(x_i,y_j) \\
&= P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right) - 
P\left(u(x_i,y_j), \pdydx{u}{x}(x_i,y_j), \pdydx{u}{y}(x_i,y_j),
\pdydxn{u}{x}{2}(x_i,y_j), \ldots \right) \\
&= P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right) - 
P_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}), u(x_{i+1},y_j),\ldots \right) \\
&\qquad + P_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}),
  u(x_{i+1},y_j), \ldots \right) \\
&\qquad \qquad -
P\left(u(x_i,y_j), \pdydx{u}{x}(x_i,y_j), \pdydx{u}{y}(x_i,y_j),
\pdydxn{u}{x}{2}(x_i,y_j), \ldots \right) \\
&= P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right) - 
P_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}), u(x_{i+1},y_j),\ldots \right)
+ \tau_{i,j}(\dtx{x},\dtx{y},u) \ .
\end{align*}
It follows from the linearity of $P_\Delta$ and the definition of the
local truncation error that
\begin{align*}
&P_\Delta\left(w_{i,j}-u(x_i,y_j), w_{i,j+1}-u(x_i,y_{j+1}),
w_{i+1,j}-u(x_{i+1},y_j), \ldots\right) \\
&\ = P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right) - 
P_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}), u(x_{i+1},y_j),
  \ldots \right)  = - \tau_{i,j}(\dtx{x},\dtx{y},u)
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.
Similarly, we have
\begin{align*}
&B_\Delta\left(w_{i,j}-u(x_i,y_j), w_{i,j+1}-u(x_i,y_{j+1}),
w_{i+1,j}-u(x_{i+1},y_j), \ldots\right) \\
&\ = B_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right) - 
B_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}), u(x_{i+1},y_j),
  \ldots \right)  = - \sigma_{i,j}(\dtx{x},\dtx{y},u)
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in \partial R_\Delta$.  Since
the finite difference is stable, there exists a constant $C_\alpha$ such that
\begin{align*}
&\max_{\substack{0\leq i\leq N\\0\leq j \leq M}} |w_{i,j} - u(x_i,y_j)| \\
&\qquad \leq C_\alpha \bigg(
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j)\in R^o_\Delta}}
\left| P_\Delta\left(w_{i,j}-u(x_i,y_j), w_{i,j+1}-u(x_i,y_{j+1}),
w_{i+1,j}- u(x_{i+1},y_j),\ldots\right) \right| \\
&\qquad
+ \max_{\substack{(i,j) \text{ such that}\\ (x_i,y_j)\in \partial R_\Delta}}
\left\| B_\Delta(w_{i,j}-u(x_i,y_j),w_{i,j+1}-u(x_i,y_{j+1}),w_{i+1,j}
  -u(x_{i+1},y_j), \ldots) \right\| \bigg) \\
& \qquad \leq C_\alpha \bigg(
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in R^o_\Delta}}
|\tau_{i,j}(\dtx{x},\dtx{y},u)|
+\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in \partial R_\Delta}}
\|\sigma_{i,j}(\dtx{x},\dtx{y},u)\| \bigg) \ .
\end{align*}
Finally, since the finite difference scheme is consistent,
\[
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in R^o_\Delta}}
|\tau_{i,j}(\dtx{x},\dtx{y},u)| \to 0 \quad \text{and} \quad
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in \partial R_\Delta}}
\|\sigma_{i,j}(\dtx{x},\dtx{y},u)\| \to 0
\]
as $\min \{N, M\} \rightarrow \infty$ imply that
\[
\max_{\substack{0\leq i\leq N\\0\leq j \leq M}} |w_{i,j} - u(x_i,y_j)|
\to 0
\]
as $\min \{N, M\} \rightarrow \infty$.
\end{proof}

Since it is generally easier to prove stability and consistency, the
previous theorem gives us a method to prove convergence without having
to prove it from the definition.  This is the approach that we
generally use later to prove convergence for some of the finite
difference schemes that we have presented in the previous section.

\begin{rmkList}
\begin{enumerate}
\item Be aware that some finite difference schemes may not converge
for all possible functions $F$ and $G$ but may be converging for some
sub-classes of functions $F$ and $G$.
\item There are finite difference scheme that may be converging but
note stable according to the definition that we have given.  This is
because the definition of stability that we have given is more
restrictive that is often necessary.  Consult \cite{IK} for more
information.
\end{enumerate}
\end{rmkList}

We can give a more precise analysis of the effect of round off error on
the numerical approximation of the solution.  As before, suppose that
$v_{i,j}$ is the computed approximation of $w_{i,j}$ for all $i$ and
$j$.  We may assume that
$\{ v_{i,j} : 0 \leq i \leq N \ \text{and} \ 0 \leq j \leq M \}$ is
the exact solution of
\begin{equation} \label{UnifErrAnal}
P_\Delta\left(v_{i,j}, v_{i,j+1}, v_{i+1,j}, \ldots\right)
= F(x_i,y_j) + \delta_{i,j}(\dtx{x},\dtx{y})
\end{equation}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$, and
\[
B_\Delta(v_{i,j},v_{i,j+1},v_{i+1,j}, \ldots)
= G(x_i,y_j) + \tilde{\delta}_{i,j}(\dtx{x},\dtx{y})
\]
for all $(i,j)$ such that $(x_i,y_j) \in \partial R_\Delta$, where
the $\delta_{i,j}(\dtx{x},\dtx{y})$ and
$\tilde{\delta}_{i,j}(\dtx{x},\dtx{y})$ represent round off errors.
Let us assume that there exists $\delta:\RR\to\RR$ such that 
$|\delta_{i,j}(\dtx{x},\dtx{y})| \leq \delta(\dtx{x},\dtx{y})$
and
$|\tilde{\delta}_{i,j}(\dtx{x},\dtx{y})| \leq \delta(\dtx{x},\dtx{y})$
for all $i$ and $j$.  So $\delta(\dtx{x},\dtx{y})$ is a bound on the
round off errors.  Proceeding as in the proof of
Theorem~\ref{StabConstConv} and using (\ref{UnifErrAnal}), we have that
\begin{align*}
0 &= F(x_i,y_j) - F(x_i,y_j)
= P_\Delta\left(v_{i,j}, v_{i,j+1}, v_{i+1,j}, \ldots\right) \\
&\qquad - P\left(u(x_i,y_j), \pdydx{u}{x}(x_i,y_j), \pdydx{u}{y}(x_i,y_j),
\pdydxn{u}{x}{2}(x_i,y_j), \ldots \right) - \delta_{i,j}(\dtx{x},\dtx{y}) \\
&= P_\Delta\left(v_{i,j}, v_{i,j+1}, v_{i+1,j}, \ldots\right) - 
  P_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}), u(x_{i+1},y_j),\ldots \right) \\
&\qquad + P_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}),
  u(x_{i+1},y_j), \ldots \right) \\
&\qquad - P\left(u(x_i,y_j), \pdydx{u}{x}(x_i,y_j), \pdydx{u}{y}(x_i,y_j),
\pdydxn{u}{x}{2}(x_i,y_j), \ldots \right) - \delta_{i,j}(\dtx{x},\dtx{y}) \\
&= P_\Delta\left(v_{i,j}, v_{i,j+1}, v_{i+1,j}, \ldots\right) - 
  P_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}), u(x_{i+1},y_j),\ldots \right) \\
&\qquad + \tau_{i,j}(\dtx{x},\dtx{y},u) - \delta_{i,j}(\dtx{x},\dtx{y})
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.
It follows from the linearity of $P_\Delta$ that
\begin{align*}
&P_\Delta\left(v_{i,j}-u(x_i,y_j), v_{i,j+1}-u(x_i,y_{j+1}),
v_{i+1,j}-u(x_{i+1},y_j), \ldots\right) \\
&\qquad = P_\Delta\left(v_{i,j}, v_{i,j+1}, v_{i+1,j}, \ldots\right) - 
P_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}), u(x_{i+1},y_j), \ldots \right) \\
&\qquad = -\tau_{i,j}(\dtx{x},\dtx{y},u) + \delta_{i,j}(\dtx{x},\dtx{y})
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.
Similarly, we have
\begin{align*}
&B_\Delta\left(v_{i,j}-u(x_i,y_j), v_{i,j+1}-u(x_i,y_{j+1}),
v_{i+1,j}-u(x_{i+1},y_j), \ldots\right) \\
&\qquad = B_\Delta\left(v_{i,j}, v_{i,j+1}, v_{i+1,j}, \ldots\right) - 
B_\Delta\left(u(x_i,y_j), u(x_i,y_{j+1}), u(x_{i+1},y_j), \ldots \right)  \\
&\qquad
= -\sigma_{i,j}(\dtx{x},\dtx{y},u) + \tilde{\delta}_{i,j}(\dtx{x},\dtx{y})
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in \partial R_\Delta$.  Since
the finite difference is stable, there exist a constant $C_\alpha$ such that
\begin{align*}
&\max_{\substack{0\leq i\leq N\\0\leq j \leq M}} |v_{i,j} - u(x_i,y_j)| \\
&\qquad \leq C_\alpha \bigg(
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j)\in R^o_\Delta}}
\left| P_\Delta\left(v_{i,j}-u(x_i,y_j), v_{i,j+1}-u(x_i,y_{j+1}),
v_{i+1,j}- u(x_{i+1},y_j),\ldots\right) \right| \\
&\qquad
+ \max_{\substack{(i,j) \text{ such that}\\ (x_i,y_j)\in \partial R_\Delta}}
\left\| B_\Delta(v_{i,j}-u(x_i,y_j),v_{i,j+1}-u(x_i,y_{j+1}),v_{i+1,j}
  -u(x_{i+1},y_j), \ldots) \right\| \bigg) \\
& \qquad \leq C_\alpha \bigg(  
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in R^o_\Delta}}
|\tau_{i,j}(\dtx{x},\dtx{y},u)|
+\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in \partial R_\Delta}}
\|\sigma_{i,j}(\dtx{x},\dtx{y},u)\| + 2 \delta(\dtx{x},\dtx{y}) \bigg) \ .
\end{align*}
If the finite difference scheme is consistent, we have that
\[
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in R^o_\Delta}}
|\tau_{i,j}| \to 0 \quad \text{and} \quad
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in \partial R_\Delta}}
\|\sigma_{i,j}\| \to 0
\]
as $\min \{N, M\} \rightarrow \infty$.  Hence, the precision of the
finite difference scheme is proportional to the round off error.

Note that there is no reason for round off errors to decrease as
$\min \{N, M\} \rightarrow \infty$; namely, as
$\max\{\dtx{x},\dtx{y}\} \to 0$.   In fact, round off errors may
start to increase for $\max\{\dtx{x},\dtx{y}\}$ small if the
computation involve divisions by small numbers.

\subsection{$\ell^2$ Theory} \label{ell2Theory}

The definitions of convergence, consistency and stability that we have
presented in the previous section are the strongest ones to be given
because they require uniform convergence on all the domain of the
boundary value problem.  Unfortunately, these definitions are too
restrictive for many of the interesting finite difference schemes.
Weaker definitions of convergence, consistency and stability are
required.

The discussion in this section is basically for the heat and wave
equation.  For the Dirichlet equation, the previous notion of
convergence, consistency and stability are fine.  We prove in
Section~\ref{CCSDirichlet}, using totally different techniques than
those presented in this section, that Algorithm~\ref{fdm_sch2S}
for the Dirichlet equation satisfies the previous definitions of
convergence, consistency and stability.

We only touch the subject of stability and convergence for finite
difference schemes to solve partial differential equations.  A good
reference on the subject and one of the principal source of
information for this section is \cite{WH}.  

The universal idea about that stability is to ensure that the errors
in our computed values do not increase as the step sizes decrease,
at least that the errors are bounded by a small value as the step sizes
decrease.

We consider a partial differential equation
\[
P\left(u(x,t), \pdydx{u}{x}(x,t), \pdydx{u}{t}(x,t),
\pdydxn{u}{x}{2}(x,t), \ldots \right)= 0
\]
on the domain $\RR \times [0,T]$ and assume that the initial
conditions are periodic with period $2\pi$.  More precisely, we assume
that $u(x,0) = g(x)$ for a periodic function $g:\RR\to \RR$ of period
$2\pi$.  For hyperbolic equation like the wave equation, we also
assume that $\displaystyle \pdydx{u}{t}(x,0) = h(x)$ for a periodic
function $h:\RR\to \RR$ of period $2\pi$.   Instead of boundary
conditions, we assume that the solution $u(x,t)$ of the partial
differential equation is periodic of period $2\pi$ with respect to
$x$.

A problem given by a partial differential equation with only initial
conditions like the problem above is called a {\bfseries Cauchy
problem}\index{Partial Differential Equations!Cauchy Problems}.

Suppose that we have a finite difference scheme of the form
$P_\Delta(w_{i,j},w_{i,j+1},w_{i+1,j}, \ldots) = F(x_i,t_j)$ for all
$(i,j)$ such that
\[
(x_i,t_j) \in R^o_\Delta = \left\{ (x_i,t_j) : x_i = i \dtx{x} \ \text{for} \
i \in \ZZ \ \text{and} \ t_j = j \dtx{t} \ \text{for} \ 0 < j \leq M \right\}
\]
and $w_{i,j} \approx u_{i,j}$.

We consider the space $\ell^2(\ZZ)$ of all functions $g:\ZZ \to \CC$
such that
\[
  \|g\|_2^2 = \sum_{k\in \ZZ} |g(k)|^2 < \infty \ .
\]
We assume that we can express this finite difference scheme as
$g_{j+1} = Q_\alpha(g_j)$ for $j \geq 0$, where
$g_j:\ZZ \to \RR$ for $j\geq 0$ is defined by
$g_j(i) = w_{i,j}$ for all $i$ and $j$, and
$Q_\alpha:\ell^2(\ZZ) \to \ell^2(\ZZ)$ is
a bounded linear mapping.  The index $\alpha$ in
$Q_\alpha$ is to indicate that the linear operator may depend on a
relation between $\dtx{x}$ and $\dtx{t}$.

\begin{egg}
For the heat equation without forcing, the Crank-Nicolson
scheme given in Algorithm~\ref{fdm_sch11S} is of the form
$g_{j+1} = Q_\alpha(g_j)$; namely,
\[
  g_{j+1}(i) = Q_\alpha(g_j)(i) = \sum_{s\in \ZZ} q_{i,s}\, g_j(s) \ ,
\]
where $q_{i,s}$ is the $(i,s)$ component of the infinite matrix
$Q_\alpha = -J^{-1} K$, where
\[
J_{r,s} =
\begin{cases}
1 + 2 \alpha & \quad \text{if} \quad r = s \\
-\alpha & \quad \text{if} \quad s = r + 1 \ \text{or} \ s = r - 1\\
0 & \quad \text{otherwise}
\end{cases}
\]
and
\[
K_{r,s} =
\begin{cases}
-1 + 2 \alpha & \quad \text{if} \quad r = s \\
-\alpha & \quad \text{if} \quad s=r+1 \ \text{or} \ s = r - 1\\
0 & \quad \text{otherwise}
\end{cases}
\]
for $r,s \in \ZZ$.  Recall that $\alpha = c \dtx{t} / (2 (\dtx{x})^2)$.

As we will see later, we do not have to worry
about computing the inverse of the ``infinite dimensional'' matrix
$J$.  Note that the $(r,s)$ component of the product of two infinite
dimensional matrices $A$ and $B$ is defined by
$\displaystyle \sum_{k\in \ZZ} A_{r,k}B_{k,s}$.
\label{ell2CNegg1}
\end{egg}

We need new definitions for convergence, consistency and stability.

\begin{defn} \label{ell2ConvDefN1}
A finite difference scheme of the form $g_{j+1} = Q_\alpha(g_j)$ with
$g_j \in \ell^2(\ZZ)$ for all $j \geq 0$ is
{\bfseries $\ell^2$-convergent}
\index{Finite Difference Methods!$\ell^2$-Convergent} if, for all
$t \in [0,T]$,
\[
\left\| g_j - u_t \right\|_2 \to 0
\quad \text{as} \quad M \to \infty \ \text{and} \ j\dtx{t} \to t \ ,
\]
where $\displaystyle u_t(i) = u(i \dtx{x}, t)$ for all $i \in \ZZ$.
\end{defn}

The previous definition is more general than what we may have
expected.  It does not only consider $t = j \dtx{t}$ but
$j\dtx{t} \to t$.

\begin{defn}
Given any sufficiently differentiable function $q:\RR\times [0,T] \to \RR$,
the {\bfseries local truncation error}\index{Finite Difference Methods!Local 
Truncation Error} of the finite difference scheme $g_{j+1} = Q_\alpha(g_j)$
is the expression
\[
\tau_t(\dtx{x},\dtx{t},q) = \frac{1}{\dtx{t}}
\big(q_{t+\dtx{t}} - Q_\alpha(q_t)\big)
\]
where $q_t(i) = q(i \dtx{x}, t)$ for all $i \in \ZZ$.

The finite difference scheme $g_{j+1} = Q_\alpha(g_j)$ is
{\bfseries consistent}\index{Finite Difference Methods!Consistency}
if, for all $t$,
\[
\sup_{0\leq t \leq T-\Delta t}\left\|\tau_t(\dtx{x},\dtx{t},q)\right\|_2
\rightarrow 0 \quad \text{as} \quad
M \rightarrow \infty
\]
for all sufficiently differentiable function $q:\RR\times [0,T] \to \RR$.
\end{defn}

We also have a new definition of stability.

\begin{defn} \label{fdmStableDefNo2}
A finite difference scheme of the form $g_{j+1} = Q_\alpha(g_j)$
with $g_j \in \ell^2(\ZZ)$ for all $j \geq 0$ is
{\bfseries $\ell^2$-stable}\index{Finite Difference
Methods!$\ell^2$-Stable} if there exists a constant $C_\alpha$ such
that $\|Q_\alpha^j\|_2 \leq C$ for $0 \leq j \leq M$ and all $M$.

The index $\alpha$ for $C_\alpha$ is to indicate that there may be a
constraining relation on $\dtx{x}$ and $\dtx{y}$ that must be
satisfied for $\|Q_\alpha^j\|_2 \leq C$ to be satisfied for
$0 \leq j \leq M$ and all $M$.  If no
constraining relation on $\dtx{x}$ and $\dtx{y}$ is imposed, then the
finite difference scheme is said to be
{\bfseries unconditionally stable}\index{Finite Difference
Methods!Unconditionally Stable}.
If there is a constraining relation, then the finite difference scheme
is said to be {\bfseries conditionally stable}\index{Finite Difference
Methods!Conditionally Stable}.
\end{defn}

This definition of stability ensures that the error of a computed
value does not increase in $\ell^2$ norm.  This can be heuristically
justified as it follows.  Suppose that $\tilde{g}_j$ is the computed
value obtained using the finite difference scheme.  We may assume that
\begin{equation}\label{FdmStabNo2QestMark}
  \tilde{g}_{j+1} = Q_\alpha(\tilde{g}_j) + \dtx{t}\, \delta_j \ ,
\end{equation}
where $\delta_j \in \ell^2(\ZZ)$ represents the round off error.
Let $r_j = \tilde{g}_j - g_j$ for $j\geq 0$.  We have
\[
r_{j+1} = \tilde{g}_{j+1} - g_{j+1}
= \left( Q_\alpha (\tilde{g}_j) + \dtx{t} \, \delta_j \right) - Q_\alpha (g_j)
= Q_\alpha \left( \tilde{g}_j - g_j\right)+ \dtx{t} \, \delta_j
= Q_\alpha \left(r_j\right) + \dtx{t} \, \delta_j
\]
for $j\geq 0$.  We get by induction that
\[
r_j = Q_\alpha^j (r_0)
+ \dtx{t} \sum_{k=0}^{j-1} Q_\alpha^k \left(\delta_{j-1-k}\right)
\]
for $j \geq 1$.  If we assume that $\|\delta_j\|_2 \leq \delta$
for all $j$, we get
\begin{align*}
\|r_j\|_2 & \leq \| Q_\alpha^j\|_2 \, \|r_0\|_2
+ \dtx{t} \sum_{k=0}^{j-1} \|Q_\alpha^k\|_2\, \left\|\delta_{j-1-k}\right\|_2
\leq \| Q_\alpha^j\|_2 \, \|r_0\|_2
+ \delta  \dtx{t} \sum_{k=0}^{j-1} \|Q_\alpha^k\|_2 \\
&\leq C \|r_0\|_2 + \delta C (j \dtx{t})
\leq C \|r_0\|_2 + \delta C T
\end{align*}
for $0 < j \leq M$.  The error is bounded in $\ell^2$ norm.  In
particular, if $r_0 = 0$, the error is bounded by a
multiple of the round off error.  This is the ideal case.

\begin{rmk}
The reader certainly wanders why we may assume that the error in
(\ref{FdmStabNo2QestMark}) is of the form $\dtx{t}\, \delta_j$.
This can be motivated by the following example.  If we consider the
finite difference equation (\ref{fdm_sch1}), then the approximate
values $v_{i,j}$ of $w_{i,j}$ are given by the exact solution of
\[
\frac{v_{i,j+1}- v_{i,j}}{\dtx{t}} - c^2 \,
\frac{v_{i+1,j} - 2v_{i,j} + v_{i-1,j}}{(\dtx{x})^2}
= f(x_i,t_j) + \delta_{i,j} \ .
\]
Thus
\[
v_{i,j+1}= v_{i,j} + \alpha \,
\left( v_{i+1,j} - 2v_{i,j} + v_{i-1,j}\right) = \dtx{t}\,f(x_i,t_j)
+ \dtx{t}\, \delta_{i,j} \ .
\]
The function $\delta_j \in \ell^2(\ZZ)$ is defined by
$\delta_j(i) = \delta_{i,j}$ for $i \in \ZZ$.
We can reach a similar conclusion with other finite difference schemes.
\end{rmk}

As the reader may know, $\ell^2(\ZZ)$ is a
{\bfseries Hilbert space}\index{Hilbert Space}.  The linear
operator $Q_\alpha$ is therefore a linear operator from a Hilbert
space into itself.  The operators that we consider behave very like
linear mapping on $\RR^n$.   Let $E$ be a Hilbert space with the norm
$\|\cdot\|$ defined by a scalar product $\ps{\cdot}{\cdot}$, and let
$P:E \to E$ be a bounded linear mapping.  As in finite dimension, we
have that $\displaystyle \|P^j\| \leq \|P\|^j$ and $\rho(P) \leq \|P\|$,
where $\rho(P)$ is the
{\bfseries spectral radius}\index{Linear Mapping!Spectral Radius} of
$P$.  Moreover, we have that
$\displaystyle \rho(P) = \lim_{k\to \infty} \|P^k\|^{1/k}$ as in
finite dimension.  The
{\bfseries adjoint}\index{Linear Mapping!!Adjoint Operator}
of $P$ is the bounded linear mapping
$P^\ast$ such that $\ps{Px}{y} = \ps{x}{P^\ast y}$ for all $x,y \in E$.
The operator $P^\ast$ is the equivalent of the transpose of a \nn
matrix.  A bounded linear operator $P$ is
{\bfseries normal}\index{Linear Mapping!!Normal Operator}
if $P P^\ast = P^\ast P$
\footnote{Symmetric operator (i.e. $P = P^\ast$) are obviously normal
operators.}.  Again, as in finite dimension, if $P$ is a normal
operator, then $\rho(P) = \|P\|$.

The following criteria will be useful to determine if a finite
difference scheme is stable.

\begin{prop}[Lax] \label{LaxN1}
Consider a finite difference scheme of the form $g_{j+1} = Q_\alpha(g_j)$
with $g_j \in \ell^2(\ZZ)$ for all $j \geq 0$.  If there exists a
constant $C_\alpha$ such that
$\displaystyle \left\|Q_\alpha \right\|_2 \leq 1 + C_\alpha \dtx{t}$ for all
$\dtx{t}$. then the finite difference scheme is $\ell^2$-stable.
\end{prop}

\begin{proof}
We have
\[
\left\|Q_\alpha^j \right\|_2 \leq \left\|Q_\alpha \right\|_2^j
\leq \left( 1 + C_\alpha \dtx{t} \right)^j
\leq  e^{j C_\alpha \dtx{t}} \leq e^{T C_\alpha}
\]
for $0 < j \leq M$ and all $M>0$.
We have used the relation $e^x \geq 1 + x$ for all $x \in \RR$.
\end{proof}

\begin{prop}[von Neumann] \label{vonNNo1}
If a finite difference scheme of the form $g_{j+1} = Q_\alpha(g_j)$
with $g_j \in \ell^2(\ZZ)$ for all $j \geq 0$ is $\ell^2$-stable, then
there exists a constant $C_\alpha$ such that
$\rho(Q_\alpha) \leq 1 + C_\alpha/M$ for all $M$.
\end{prop}

\begin{proof}
Since the finite difference scheme is $\ell^2$-stable, we have
$\|Q_\alpha^j\|_2 \leq C_\alpha$ for $0\leq j \leq M$.
Hence,
\[
  \rho^j(Q_\alpha) = \rho(Q_\alpha^j) \leq \|Q_\alpha^j\|_2 \leq C_\alpha
\]
for $0\leq j \leq M$.  For $j=M$, we get
\[
  \rho(Q_\alpha) \leq C_\alpha^{1/M} \leq 1 + \frac{C_\alpha}{M} \ .
\]
The last inequality comes from the following observation.
Consider $\displaystyle f(x) = e^{x\ln{C_\alpha}} - 1 - C_\alpha x$ for
$0 \leq x \leq 1$.  Since
$\displaystyle f'(x) = \ln(C_\alpha) e^{x\ln{C_\alpha}} - C_\alpha \leq 0$ for
$0 < C_\alpha \leq e$, we have $f(x) \leq f(0) = 0$ for $0\leq x \leq 1$.
Since $f$ reaches it absolute maximum at
$\tilde{x} = (\ln(C_\alpha) - \ln(\ln(C_\alpha)))/\ln(C_\alpha) \in [0,1]$
for $C_\alpha \geq e$, we have
$f(x) \leq f(\tilde{x}) = -1 -C_\alpha + C_\alpha/\ln(C_\alpha) + C_\alpha
\ln(\ln(C_\alpha))/\ln(C_\alpha) \leq 0$ for $0\leq x \leq 1$.
\end{proof}

\begin{rmk}
The conclusion of the previous proposition is often stated as
there exists a constant $D_\alpha$ such that
$\rho(Q_\alpha) \leq 1 + D_\alpha \dtx{t}$ for all $\dtx{t}$.
The constant $D_\alpha$ is $C_\alpha/T$ in the proposition above. 
\end{rmk}

If $Q_\alpha$ is a normal operator, we have a strong criteria for
$\ell^2$ stability.

\begin{prop}[Lax]
Consider a finite difference scheme of the form $g_{j+1} = Q_\alpha(g_j)$
with $g_j \in \ell^2(\ZZ)$ for all $j \geq 0$.  If $Q_\alpha$ is a
normal operator, then the finite difference scheme is
$\ell^2$-stable if and only if there exists a constant $C_\alpha$ such
that $\rho(Q_\alpha) \leq 1 + C_\alpha/M$ for all $M$.
\end{prop}

\begin{proof}
Since $Q_\alpha$ is normal, we have that
$\displaystyle \left\|Q_\alpha^2 \right\|_2 = \left\|Q_\alpha\right\|^2$.
The reader is asked to prove this result in Exercise~\ref{fdmQ3}.  By
induction, we have
$\displaystyle \left\|Q_\alpha^{2^k} \right\|_2 = \left\|Q_\alpha\right\|^{2^k}$
for all $k\geq 0$.

From $\displaystyle \rho\left(Q_\alpha\right)
= \lim_{k\to \infty} \|Q_\alpha^k\|^{1/k}$, we get
\[
\rho\left(Q_\alpha\right)
= \lim_{k\to \infty} \left\|Q_\alpha^{2^k}\right\|^{1/2^k}
\lim_{k\to \infty} \left(\left\|Q_\alpha\right\|^{2^k}\right)^{1/2^k}
= \|Q_\alpha\|_2
\]
It follows from Proposition~\ref{LaxN1} that
$\rho\left(Q_\alpha\right) \leq 1 + C_\alpha/M$ for a constant $C_\alpha$
and all $M$ is a sufficient condition for the $\ell^2$ stability of
the finite difference scheme.  It follows from
Proposition~\ref{vonNNo1} that
$\rho\left(Q_\alpha\right) \leq 1 + C_\alpha/M$ for a constant $C_\alpha$
and all $M$ is a necessary condition for the $\ell^2$ stability of
the finite difference scheme.
\end{proof}

The main result of this section is the following.

\begin{theorem} \label{ell2StabConstConv}
Consider a finite difference scheme of the form $g_{j+1} = Q_\alpha(g_j)$
where $g_j \in \ell^2$.   If this finite difference scheme is
$\ell^2$-stable and consistent, then it is $\ell^2$-convergent. 
\end{theorem}

A proof of this result is given in \cite{WH} (Theorem~6.22).  They
also prove that $\ell^2$-convergence implies $\ell^2$-stability.  They
provide many more criteria to determine if a finite difference scheme is
$\ell^2$-stable.

\subsection[von Neumann's Method]{Stability Analysis with Fourier
Transforms (von Neumann's Method)} \label{vonNewmannMeth}

We first review some concepts of functional analysis that will be
useful to justify the von Neumann's method.

We consider the space $L^2([0,2\pi])$ of all integrable
functions $f:[2,\pi] \to \CC$ such that
\[
  \|f\|_2^2 = \frac{1}{2\pi} \int_0^{2\pi} |f(x)|^2 \dx{x} < \infty \ .
\]
We have used the same notation for the norm in $\ell^2(\ZZ)$ and the norm
in $L^2([0,2\pi])$.  The reader should be able by the context to
determine which norm is used. 

It is well know in functional analysis that there is an
isometry\footnote{An isometry between two normed spaces $X$ and $Y$
is a one-to-one and onto mapping $F:X \to Y$ such that the norm of $x$
is equal to the norm of $F(x)$ for all $x \in X$.}
between these two spaces defined by
\begin{align*}
\Phi : L^2{[0,2\pi]} & \to \ell^2(\ZZ) \\
f & \mapsto \hat{f}
\end{align*}
where
\[
  \hat{f}(k) = \frac{1}{2\pi} \int_0^{2\pi} f(x) e^{-k x\,i} \dx{x}
\]
for $k\in \ZZ$ and $i \in \CC$ is such that $i^2 =-1$.  The function
$\hat{f}$ is the Fourier transform of $f$.  We could have
considered only real value functions but then $\cos(k \pi)$ and
$\sin(k \pi)$ would have to be used to define $\Phi$ and the notation
becomes messy.  We will therefore stick to complex valued functions
for a while.  The equation $\|f\|_2 = \|\hat{f}\|_2$ is known as
Parseval equality.

The inverse Fourier transform is define by
\begin{align*}
\Phi^{-1} : \ell^2(\ZZ) & \to L^2{[0,2\pi]} \\
g & \mapsto \check{g}
\end{align*}
where
\[
  \check{g}(x) = \sum_{k\in \ZZ} g(k) e^{k x\, i}
\]
for $x \in [0,2\pi]$.  We also have that $\|g\|_2 = \|\check{g}\|_2$.

If we apply the inverse Fourier transform on both sides of
$g_{j+1} = Q_\alpha(g_j)$, we get
\[
\Phi^{-1}(g_{j+1}) = \Phi^{-1}(Q_\alpha(g_j))
= \big(\Phi^{-1}\circ Q_\alpha \circ\Phi\big) (\Phi^{-1}(g_j)) \ .
\]
If we set $\check{Q}_\alpha = \Phi^{-1}\circ Q_\alpha \circ \Phi$, we get
$\check{g}_{j+1} = \check{Q}_\alpha\left(\check{g}_j\right)$ with
$\check{Q}_\alpha: L^2([0,2\pi]) \to L^2([0,2\pi])$ a linear mapping.

Since $\Phi : L^2([0,2\pi]) \to \ell^2(\ZZ)$ is an isometry with
inverse $\Phi^{-1}$, we have that they are both of induced norm $1$.
Hence,
\[
\left\|\check{Q}_\alpha^j\right\|_2
= \left\|\left(\Phi^{-1}\circ Q_\alpha \circ \Phi\right)^j\right\|_2
= \left\|\Phi^{-1}\circ Q_\alpha^j \circ \Phi\right\|_2
= \left\|Q_\alpha^j \right\|_2
\]
for all $j>0$.  We have proved that the finite difference scheme of
the form $g_{j+1} = Q_\alpha(g_j)$ for $j\geq 0$ is $\ell^2$-stable
if there exists a constant $C$ such that $\|\check{Q}_\alpha^j\|_2 \leq C$
for all $j\geq 0$.

To determine if a finite difference scheme of the form
$g_{j+1} = Q_\alpha(g_j)$ with $g_j \in \ell^2(\ZZ)$ is $\ell^2$-stable,
we have to prove that there exists a constant $C_\alpha$ such that
$\|Q_\alpha^j \|_2\leq C_\alpha$ or
$\|\check{Q}_\alpha^j \|_2\leq C_\alpha$ for all $j$.
This may not be easy to do.   Even if $Q_\alpha$ is normal, proving
that the eigenvalues of $Q_\alpha$ are less or equal to $1$ in absolute
value may not be trivial.

We need a detailed description of the action of $Q_\alpha$ and
$\check{Q}_\alpha$ to be able to determine the $\ell^2$ stability of
the finite difference scheme.  As we have seen in a previous example,
we can express $g_{j+1} = Q_\alpha(g_j)$ as
\[
g_{j+1}(k) = Q_\alpha(g_j)(k) = \sum_{s\in \ZZ} q_{k,s}\, g_j(s) \ ,
\]
where $q_{k,s}$ is the $(k,s)$ component of the matrix $Q_\alpha$.
We first observe that for all our finite difference schemes, we have
that $q_{k,s} = q_{k-1,s-1}$.  Thus
\[
g_{j+1}(k) = Q_\alpha(g_j)(k) = \sum_{s\in \ZZ} q_{k,s}\, g_j(s)
= \sum_{s\in \ZZ} q_{0,s-k}\, g_j(s)
= \sum_{s\in \ZZ} q_{0,s}\, g_j(s+k)
\]
for all $k \in \ZZ$.  So, for the rest of the discussion, we will
assume that $g_{j+1} = Q_\alpha(g_j)$ is given by
\begin{equation} \label{fdml2gf}
g_{j+1}(k) = \sum_{s\in \ZZ} q_s\, g_j(s+k)
\end{equation}
for all $k \in \ZZ$, where $q_s = q_{0,s}$.  We also observe that for
all our finite difference schemes, the sum in (\ref{fdml2gf}) is
finite.   There is only a finite number of $q_s$ that are non-null.

We have that
\begin{align*}
\check{g}_{j+1}(x) &= \Phi^{-1}\left(Q_\alpha(g_j)\right)(x)
= \sum_{k\in \ZZ} \left(\sum_{s\in \ZZ} q_s\, g_j(s+k) \right) e^{kx i}
= \sum_{s\in \ZZ} q_s \left( \sum_{k\in \ZZ}  g_j(s+k)  e^{kx i} \right) \\
& = \sum_{s\in \ZZ} q_s \left( \sum_{r\in \ZZ}  g_j(r)  e^{(r-s)x i} \right)
= \sum_{s\in \ZZ} q_s
\underbrace{\left( \sum_{r\in \ZZ}  g_j(r)  e^{rx i}\right)}_{=\check{g}_j(x)}
  e^{-sx i}
= \left( \sum_{s\in \ZZ} q_s e^{-sx i} \right) \check{g}_j(x) \ .
\end{align*}
Let $\displaystyle \tilde{Q}_\alpha(x) = \sum_{s\in \ZZ} q_s e^{-sx i}$ for
$x \in \RR$.  The action of $\check{Q}_\alpha$ on a function in
$f \in L^2([0,2\pi])$ is just the product $\tilde{Q}_\alpha\,f$.

Hence,
\[
\left\| \check{Q}_\alpha \right\|_2
= \sup_{\substack{f\in L^2([0,2\pi])\\ \|f\|_2=1}}
\left\| \check{Q}_\alpha(f) \right\|_2
= \sup_{\substack{f\in L^2([0,2\pi])\\ \|f\|_2=1}}
\left\| \tilde{Q}_\alpha \, f \right\|_2
= \left\|\tilde{Q}_\alpha \right\|_2 \ ,
\]
where the last norm is just the $L^2$-norm of the function
$\tilde{Q}_\alpha$.

To use Proposition~\ref{LaxN1} to show that a finite difference scheme
is $\ell^2$-stable, we may simply show that
\[
\left\| \tilde{Q}_\alpha \right\|_2 \leq
\left\| \tilde{Q}_\alpha \right\|_\infty \leq 1 + C_\alpha \dtx{t}
\]
for some constant $C_\alpha$.

\begin{egg}
For the finite difference scheme presented in
Algorithm~\ref{fdm_sch1S}, we have (\ref{fdml2gf}) with
$q_{-1} = \alpha$, $q_0 = 1 - 2\alpha$, $q_1 = \alpha$ and $q_s = 0$
otherwise, where
$\displaystyle \alpha = \frac{c^2\dtx{t}}{(\dtx{x})^2}$.  Thus
\[
  \tilde{Q}_\alpha(x) = \alpha e^{-xi} + (1-2\alpha) + \alpha e^{xi}
= 1 - 2\alpha(1 -\cos(x))
\]
Since
\[
  \left\|\tilde{Q}_\alpha(x)\right\|_\infty =
  \sup_{x \in [0,2\pi]}|1 - 2\alpha(1 -\cos(x))| \leq 1
\]
for $2\alpha \leq 1$, we have that the finite difference scheme in 
Algorithm~\ref{fdm_sch1S} is $\ell^2$-stable for
$\displaystyle \frac{c^2\dtx{t}}{(\dtx{x})^2} \leq \frac{1}{2}$.
\end{egg}

It could be trickier to use the theory developed above to prove that
an implicit finite difference scheme is $\ell^2$-stable because we may
not have an explicit formulation for $Q_\alpha$.  For instance, the
matrix $Q_\alpha$ for the Crank-Nicolson finite difference scheme is
given by $Q_\alpha = -J^{-1} K$, where $J$ and $K$ are defined in
Example~\ref{ell2CNegg1}.

We do not need to know $Q_\alpha$ to find $\check{Q}_\alpha$.
Consider an implicit finite difference scheme of the form
$\displaystyle A_\alpha(g_{j+1}) = B_\alpha(g_j)$ for $g_j \in \ell^2(\ZZ)$,
where $A_\alpha , B_\alpha:\ell^2(\ZZ) \to \ell^2(\ZZ)$ are two bounded
linear mapping.  This relation can be rewritten explicitly as
\begin{equation} \label{fdml2gfImpleA}
\sum_{s\in \ZZ} a_{k,s} g_{j+1}(s) = \sum_{s\in \ZZ} b_{k,s}\, g_j(s)
\end{equation}
for all $k \in \ZZ$, where $a_{k,s}$ is the $(k,s)$ component of the
infinite matrix $A_\alpha$ and $b_{k,s}$ is the $(k,s)$ component of the
infinite matrix $B_\alpha$.  For our finite difference schemes, we have
$a_{k,s} = a_{k-1,s-1}$ and $b_{k,s} = b_{k-1,s-1}$ for all $k$ and $s$.
As we did above for $Q_\alpha$, we can rewrite (\ref{fdml2gfImpleA}) as
\begin{equation} \label{fdml2gfImpl}
\sum_{s\in \ZZ} a_s g_{j+1}(s+k) = \sum_{s\in \ZZ} b_s\, g_j(s+k)
\end{equation}
for all $k\in \ZZ$, where $a_s = a_{0,s}$ and $b_s = b_{0,s}$.

Hence, proceeding as we have done for $Q_\alpha$, we have
\begin{align*}
\Phi^{-1}\left(A_\alpha(g_{j+1})\right) = \Phi^{-1}\left(B_\alpha(g_j)\right)
& \Rightarrow
\sum_{k\in \ZZ} \left(\sum_{s\in \ZZ} a_s\, g_{j+1}(s+k) \right) e^{kx i}
= \sum_{k\in \ZZ} \left(\sum_{s\in \ZZ} b_s\, g_j(s+k) \right) e^{kx i} \\
&\Rightarrow
\sum_{s\in \ZZ} a_s
\underbrace{\left( \sum_{r\in \ZZ}  g_{j+1}(r)
e^{rx i}\right)}_{=\check{g}_{j+1}(x)}  e^{-sx i}
= \sum_{s\in \ZZ} b_s
\underbrace{\left( \sum_{r\in \ZZ}  g_j(r)
e^{rx i}\right)}_{=\check{g}_j(x)}  e^{-sx i} \\
&\Rightarrow
\left( \sum_{s\in \ZZ} a_s e^{-sx i} \right) \check{g}_{j+1}(x)
= \left( \sum_{s\in \ZZ} b_s e^{-sx i} \right) \check{g}_j(x)
\end{align*}
for $x\in \RR$.  Hence,
\[
\check{g}_{j+1}(x)
= \left( \sum_{s\in \ZZ} a_s e^{-sx i} \right)^{-1}
\left( \sum_{s\in \ZZ} b_s e^{-sx i} \right) \check{g}_j(x)
\]
Let
\[
\tilde{Q}_\alpha(x) = \left( \sum_{s\in \ZZ} a_s e^{-sx i} \right)^{-1}
\left( \sum_{s\in \ZZ} b_s e^{-sx i} \right)
\]
for $x \in \RR$.  The action of $\check{Q}_\alpha$ on a function in
$f \in L^2([0,2\pi])$ is just the product $\tilde{Q}_\alpha\,f$.

\begin{rmk}
The $\ell^2$ theory above can be expanded to finite difference scheme
that are more then one-step schemes.   \label{ell2TheoryMsteps}

Suppose that we have a finite difference scheme of the form
\begin{equation} \label{fdml2gfImplAext}
\sum_{s\in \ZZ} a_{k,s} g_{j+1}(s) = \sum_{s\in \ZZ} b_{k,s}\, g_j(s)
+ \sum_{s\in \ZZ} c_{k,s}\, g_{j-1}(s)
\end{equation}
for all $k \in \ZZ$.  Suppose that we assume, as we did before,
that $a_{k,s} = a_{k-1,s-1}$, $b_{k,s} = b_{k-1,s-1}$ and
$c_{k,s} = c_{k-1,s-1}$ for all $k$ and $s$.  Then
(\ref{fdml2gfImplAext}) can be written as
\begin{equation} \label{fdml2gfImplext}
\sum_{s\in \ZZ} a_s g_{j+1}(s+k) = \sum_{s\in \ZZ} b_s\, g_j(s+k)
+ \sum_{s\in \ZZ} c_s\, g_{j-1}(s+k)
\end{equation}
for all $k\in \ZZ$, where $a_s = a_{0,s}$, $b_s = b_{0,s}$ and $c_s = c_{0,s}$.

Using the inverse Fourier transform as we did before, we get
\[
\left( \sum_{s\in \ZZ} a_s e^{-sx i} \right) \check{g}_{j+1}(x)
= \left( \sum_{s\in \ZZ} b_s e^{-sx i} \right) \check{g}_j(x)
+ \left( \sum_{s\in \ZZ} c_s e^{-sx i} \right) \check{g}_{j-1}(x)
\]
for $x\in \RR$.  This is a finite difference equation for
$\check{g}_j$.  The characteristic polynomial of this finite
difference equation is
\begin{equation}\label{fdml2gfImplBext}
R(x) \big(\lambda(x)\big)^2 + S(x) \lambda(x) + T(x) = 0 \ ,
\end{equation}
where 
$\displaystyle R(x) = \sum_{s\in \ZZ} a_s e^{-sx i}$,
$\displaystyle S(x) = \sum_{s\in \ZZ} b_s e^{-sx i}$ and
$\displaystyle T(x) = \sum_{s\in \ZZ} c_s e^{-sx i}$.
The solution of this finite difference equation is of the form
\[
\check{g}_j(x) = C_1(x) \big(\lambda_1(x)\big)^j
+ C_2(x) \big(\lambda_2(x)\big)^j 
\]
if the characteristic polynomial at $x$ has two distinct roots
$\displaystyle \lambda_1(x)$ and $\displaystyle \lambda_2(x)$, or of
the form
\[
\check{g}_j(x) = C_1(x) \big(\lambda_1(x)\big)^j + C_2(x)j
\big(\lambda_1(x)\big)^j 
\]
if $\displaystyle \lambda_1(x) = \lambda_2(x)$.

Doing a stability analysis using this approach seems to be a
formidable task.  However, it is often very simple in practice as
it is illustrated in Item~\ref{dfmLastRmksItem4} of
Remark~\ref{dfmLastRmks} for the finite difference
scheme in Algorithm~\ref{fdm_sch3S} used to numerically solve the wave
equation.
\end{rmk}

\begin{egg}
For the Crank-Nicolson scheme presented in
Algorithm~\ref{fdm_sch11S}, we have (\ref{fdml2gfImpl}) with
$a_{-1} = -\alpha$, $a_0 = 1 + 2\alpha$, $a_1 = -\alpha$,
$b_{-1} = \alpha$, $b_0 = 1 - 2\alpha$, $b_1 = \alpha$,
and $a_s = b_s = 0$ otherwise, where
$\displaystyle \alpha = \frac{c^2\dtx{t}}{2 (\dtx{x})^2}$.
Thus
\[
\tilde{Q}_\alpha(x) =
\frac{\alpha e^{-xi} +(1-2\alpha) + \alpha e^{xi}}
{-\alpha e^{-xi} + (1+2\alpha) - \alpha e^{xi}}
= \frac{1 - 2\alpha(1 -\cos(x))}{1+2\alpha(1-cos(x))} \ .
\]
for all $x$.  Since
$\displaystyle \left\|\tilde{Q}_\alpha(x)\right\|_\infty \leq 1$
independently of the value of $\alpha$, we have that the
Crank-Nicolson scheme is unconstrained $\ell^2$-stable.
\label{CNell2StableNo2}
\end{egg}

\subsection{$L^2$ Stability}\label{L2Stab}

There is another approach to the theory of convergence, consistency
and stability that we present briefly in this section.  It is
often presented as the von Neumann's method in many books.  The $L^2$
notation in this section is not widely used but we use it to
distinguish the definition of stability presented in this section from
the definition of stability presented in other sections.

We still consider the Cauchy problem presented in Section~\ref{ell2Theory}.
The main difference with the previous approach is that we now assume
that the approximations $w_{i,j}$ of $u_{i,j} = u(x_i,t_j)$ are given
by functions $h_j \in L^2([0,2\pi])$ for $0 \leq j \leq M$.
So, $\displaystyle w_{k,j} = h_j(x_k)$ for $0 \leq k \leq N$ and
$0\leq j \leq M$.

We now define the stability as it follows.

\begin{defn} \label{fdmStableDefNo3}
A finite difference scheme is {\bfseries $L^2$-stable}\index{Finite Difference
Methods!von Neumann's Stable or $L^2$-Stable} if there exists a
constant $C_\alpha$ such that $\|h_j\|_2 \leq C_\alpha \|h_0\|_2$ for
$0 \leq j \leq M$ and all $M$.
\end{defn}

To motivate the previous definition, we go back to our general form
for a finite difference scheme
\begin{equation} \label{fdmgfL2A}
\sum_{r\in \ZZ} a_r w_{k+r,j+1} =
\sum_{r\in \ZZ} b_r w_{k+r,j}
\end{equation}
where $m$ is a non-negative integer.  Do not forget that $a_r$ and
$b_r$ may depend on a relation between $\dtx{x}$ and $\dtx{t}$.
Also, the two summations above are in fact finite for the finite
difference schemes that we consider.

\begin{egg}
As we have seen, the Crank-Nicholson scheme is of this form
(\ref{fdmgfL2A}) with $a_{-1}=-\alpha$, $a_0 = 1+2\alpha$, $a_1=-\alpha$, 
$b_{-1} =\alpha$, $b_0 = 1-2\alpha$, $b_1 = \alpha$ and
$a_k = b_k = 0$ otherwise.
\end{egg}

We may express each $h_j$ using Fourier series to get
$\displaystyle h_j(x) = \sum_{k\in \ZZ} A_{k,j} e^{kxi}$, where $i$
is the complex number such that $i^2=-1$.  We have that
$\displaystyle g(x) = h_0(x) = \sum_{k\in \ZZ} A_{k,0} e^{kxi}$
with $\displaystyle A_{k,0} = \frac{1}{2\pi} \int_0^{2\pi} g(x) e^{-kxi}\dx{x}$.

We expand (\ref{fdmgfL2A}) to the finite difference equation
\begin{equation}\label{fdmgfL2B}
\sum_{r\in \ZZ} a_r h_{j+1}(x+r\dtx{x}) =
\sum_{r\in \ZZ} b_r h_j(x+r\dtx{x}) \ .
\end{equation}
Using the Fourier series of $h_j$, we get
\begin{align*}
&\sum_{r\in \ZZ} a_r \left( \sum_{k\in \ZZ} A_{k,j+1} e^{k(x+r\dtx{x})i}\right)
 = \sum_{r\in \ZZ} b_r \left( \sum_{k\in \ZZ} A_{k,j} e^{k(x+r\dtx{x})i}\right)  \\
& \qquad \Rightarrow
\sum_{k\in \ZZ}\left( A_{k,j+1} \sum_{r\in \ZZ} a_r e^{kr \dtx{x}\, i} \right)
e^{k x\,i}
= \sum_{k\in \ZZ}\left( A_{k,j} \sum_{r\in\ZZ} b_r  e^{k r \dtx{x}\, i} \right)
e^{k x\,i} \\
& \qquad \Rightarrow
\sum_{k\in \ZZ}\left( A_{k,j+1} \sum_{r\in \ZZ} a_r e^{k r \dtx{x}\, i}
- A_{k,j} \sum_{r\in \ZZ} b_r e^{k r \dtx{x}\, i} \right) e^{k x\,i} = 0 \ .
\end{align*}
Thus, for all $k$,
\begin{equation}\label{fdmgfL2C}
  A_{k,j+1} \alpha_k -  A_{k,j}  \beta_k = 0
\end{equation}
for $0\leq j < M$, where
\[
\alpha_k = \sum_{r\in \ZZ} a_r e^{k r \dtx{x}\, i} \quad \text{and} \quad
\beta_k = \sum_{r\in \ZZ} b_r  e^{k r \dtx{x}\, i}  \ .
\]
We therefore have that
\[
  A_{k,j} = \left(\frac{\beta_k}{\alpha_k}\right)^j A_{k,0}
\]
for $j \geq 0$.

Since $\alpha_{k+N} = \alpha_k$ and $\beta_{k+N} = \beta_k$ for all
$k$ because $\dtx{x} = 2\pi/N$, there is only a finite number of
ratios $\displaystyle \lambda_k = \beta_k/\alpha_k$ to
determine.  We only need to compute $\lambda_k$ for $0 \leq k < N$.

If there exists a constant $C_\alpha$ such that $|\lambda_k|^j \leq C_\alpha$ for
$0 \leq j \leq M$ and $0 \leq k <N$,
then $|A_{k,j}|^2 \leq C_\alpha^2 |A_{k,0}|^2$ for $0 \leq j \leq M$ and
$k \in \ZZ$.  We get from Parseval equality that
\[
\|h_j\|_2^2 = \left\|\hat{h}_j\right\|_2^2
= \sum_{k\in \ZZ} |A_{k,j}|^2 \leq C_\alpha^2 \sum_{k\in \ZZ} |A_{k,0}|^2
= C_\alpha^2 \|\hat{h}_0\|_2^2 = C_\alpha^2 \|h_0\|_2^2
\]
for $0 \leq j \leq M$.
We have shown that $\|h_j\|_2 \leq C_\alpha \|h_0\|_2$ if
$|\lambda_k|^j \leq C$ for $0 \leq j \leq M$ and $0 \leq k < N$.
This last condition is satisfied if and only if
$|\lambda_k| \leq 1$ for $0\leq k < N$ because $M$ can be as
large as we want.  Do not forget that there may be a restriction on
$N$ and $M$ because $|\lambda_k|^j \leq C$ may be true only if
a relation between $\dtx{x}$ and $\dtx{t}$ is satisfied; a relation
that is inherited from the dependence of $a_r$ and $b_r$ on the
parameter $\alpha$ that we have defined for the finite difference
schemes presented in Section~\ref{EXplImplSchenes}.

\begin{rmk}
We have considered one-step finite difference schemes in the previous
discussion but this method can be generalized to two or more step
schemes.  If instead of (\ref{fdmgfL2A}) we have
\begin{equation}\label{fdmgfL2Aext}
\sum_{r\in\ZZ} a_r w_{i+r,j+1} =
\sum_{r\in \ZZ} b_r w_{i+r,j} + \sum_{r\in \ZZ} c_r w_{i+r,j-1} \ ,
\end{equation}
then instead of (\ref{fdmgfL2B}) we get
\begin{equation}\label{fdmgfL2Bext}
\sum_{r\in\ZZ} a_r h_{j+1}(x+r\dtx{x}) =
\sum_{r\in\ZZ} b_r h_{j}(x+r\dtx{x})
+ \sum_{r\in\ZZ} c_r h_{j-1}(x+r\dtx{x})
\end{equation}
for $j\geq 0$.  We have that $h_0(x) = g(x)$ and we may assume that
$h_{-1}(x)$ is a given periodic function of period $2\pi$.  For
instance, in the case of the wave equation in
Section~\ref{DerWaveSection}, we will have
$h_{-1}(x) = h_1(x) - 2 f(x) \dtx{t}$.   \label{fdmgfL2ext}

Using the Fourier series of the $h_j$, (\ref{fdmgfL2Bext}) yields
\begin{align*}
& \sum_{r\in\ZZ} a_r \left( \sum_{k\in \ZZ} A_{k,j+1} e^{k(x+r\dtx{x}) i}\right)
= \sum_{r\in\ZZ} b_r \left( \sum_{k\in \ZZ} A_{k,j} e^{k (x+r\dtx{x}) i}\right) 
+ \sum_{r\in\ZZ} c_r \left( \sum_{k\in \ZZ} A_{k,j-1} e^{k(x+r\dtx{x})\,i}\right)
\\
& \qquad \Rightarrow
\sum_{k\in \ZZ}\left(A_{k,j+1}
\sum_{r\in\ZZ} a_r e^{k r \dtx{x}\, i}
- A_{k,j}  \sum_{r\in\ZZ} b_r  e^{k r \dtx{x}\, i}
- A_{k,j-1} \sum_{r\in\ZZ} c_r  e^{k r \dtx{x}\, i}\right) e^{k x\,i} = 0 \ .
\end{align*}
Thus, for all $k$,
\begin{equation}\label{{fdmgfL2Cext}}
  A_{k,j+1} \alpha_k -  A_{k,j}  \beta_k - A_{k,j-1} \gamma_k = 0
\end{equation}
for $0\leq j < M$, where
\begin{equation} \label{{fdmgfL2Dext}}
\alpha_k = \sum_{r\in\ZZ} a_r e^{k r \dtx{x}\, i} \quad , \quad
\beta_k = \sum_{r\in\ZZ} b_r  e^{k r \dtx{x}\, i} \quad \text{and} \quad
\gamma_k = \sum_{r\in\ZZ} c_r  e^{k r \dtx{x}\, i} \ .
\end{equation}
If the characteristic polynomial
$\alpha_k \lambda^2 - \beta_k \lambda - \gamma_k$ has two distinct
roots $\lambda_{k,1}$ and $\lambda_{k,2}$, then the general solution of
(\ref{{fdmgfL2Cext}}) is of the form
\[
A_{k,j} = C_{k,1} \lambda_{k,1}^j + C_{k,2} \lambda_{k,2}^j 
\]
for $0 \leq j < M$ and some constants $C_{k,1}$ and $C_{k,2}$.  If
$\lambda_{k,1} = \lambda_{k,2}$, then
\[
A_{k,j} = C_{k,1} \lambda_{k,1}^j + C_{k,2} j \lambda_{k,1}^j 
\]
for $0 \leq j < M$ and some constants $C_{k,1}$ and $C_{k,2}$.

Since $\alpha_{k+N} = \alpha_k$, $\beta_{k+N} = \beta_k$ and
$\gamma_{k+N} = \gamma_k$ for all $k$ because $\dtx{x} = 2\pi/N$,
there is only a finite number of roots to determine.  We
only need to compute $\lambda_{k,1}$ and $\lambda_{k,2}$ for
$0 \leq k < N$.
We could show that the finite difference scheme is ``stable'' when
$|\lambda_{k,1}| \leq 1$ and $|\lambda_{k,2}| \leq 1$.  If
$|\lambda_{k,1}| = |\lambda_{k,2}|=1$, then we also need
$\lambda_{k,1} \neq \lambda_{k,2}$.

This technique is illustrated in Item~\ref{dfmLastRmksItem1} of
Remark~\ref{dfmLastRmks} for the finite difference
scheme in Algorithm~\ref{fdm_sch3S} used to numerically solve the wave
equation.

We will not pursue on this subject.  The generalization to higher
dimension in the rest of this section can instead be used to handle
finite difference schemes that are more than one-step schemes.
\end{rmk}

Proceeding exactly as we have just done, the previous discussion can
be generalized to the finite difference scheme of the form
\begin{equation} \label{fdmgfL2D}
\sum_{r\in \ZZ} J_r \VEC{w}_{j+1} = \sum_{r\in \ZZ} K_r \VEC{w}_j \ ,
\end{equation}
where $J_r$ and $K_r$ are \nn matrices, and $\VEC{w}_j \in \RR^n$ for
$j\geq 0$.

\begin{egg}
The Crank-Nicolson scheme is of the form (\ref{fdmgfL2A}) with
$J_0 = -J$ and $K_0 = K$ with $K$ and $J$ defined in (\ref{fdm_K}) and
(\ref{fdm_J}) respectively, and $J_r=K_r= 0$ otherwise.  We also have
that $n= N-1$.
\end{egg}

If we assume that $\displaystyle \VEC{w}_j = h_j(x_k)$ for
$h_j \in L^2([0,2\pi],\RR^n)$ \footnote{The functions
$h:[0,2\pi] \to \RR^n$ such that each component is in $L^2([0,2\pi])$.},
we may expand (\ref{fdmgfL2D}) to the finite difference equation
\begin{equation}\label{fdmgfL2F}
\sum_{r\in \ZZ} J_r h_{j+1}(x+r\dtx{x}) =
\sum_{r\in \ZZ} K_r h_j(x+r\dtx{x}) \ .
\end{equation}
If we substitute the Fourier series
$\displaystyle h_j(x) = \sum_{k\in \ZZ} \VEC{A}_{k,j} e^{k x\, i}$,
where $A_{k,j} \in \RR^n$, in the previous equation, we find that
\begin{equation}\label{fdmgfL2E}
  \VEC{A}_{k,j+1} = Q_k \VEC{A}_{k,j} \ ,
\end{equation}
where
\[
  Q_k = \left( \sum_{r=-m}^m K_r e^{k r \dtx{x}\, i} \right)^{-1}
  \left( \sum_{r=-m}^m J_r  e^{k r \dtx{x}\, i} \right) \ .
\]
By induction, we get from (\ref{fdmgfL2E}) that
\begin{equation}\label{fdmgfL2G}
\VEC{A}_{k,j} = Q_k^j \VEC{A}_{k,0} \quad , \quad j \geq 0 \ .
\end{equation}
If there exists $C_\alpha$ such that $\|Q_k^j\|_2 \leq C_\alpha$ for
$0\leq j \leq M$ and $k \in \ZZ$,  we get from Parseval equality that
\[
  \|h_j\|_2^2 = \|\hat{h}_j \|_2^2
  = \sum_{k\in \ZZ} \|\VEC{A}_{k,j}\|_2^2
\leq C_\alpha^2 \sum_{k\in \ZZ} \|\VEC{A}_{k,0}\|_2^2
= C_\alpha^2 \|\hat{h}_0 \|_2^2 = C_\alpha^2\|h_0\|_2^2
\]
for $0 \leq j \leq M$.  We have shown that $\|h_j\|_2 \leq C_\alpha \|h_0\|_2$
if there exists $C_\alpha$ such that $\|Q_k^j\|_2 \leq C_\alpha$ for
$0\leq j \leq M$ and $k \in \ZZ$.   Again, because of the periodicity
of $Q_k$, we only have to consider $0 \leq k <N$.  We have the more
precise result that follows.

\begin{prop}
A finite difference scheme of the form (\ref{fdmgfL2D}) is
{\bfseries $L^2$-stable} if and only if there exists $C_\alpha$ such that
$\|Q_k^j\|_2 \leq C_\alpha$ for $0\leq j \leq M$, $k \in \ZZ$ and $N>0$.
\label{QkjCstable}
\end{prop}

\begin{proof}
We have proved above that the condition $\|Q_k^j\|_2 \leq C$ for
$0\leq j \leq M$ and $k \in \ZZ$ was sufficient.   We now prove that
it is necessary.  Suppose that $\|Q_{k_0}^{j_0}\|_2 > C_\alpha$ for
some $k_0$ and $j_0$.  Choose $\VEC{w} \in \RR^n$ such that
$\|Q_{k_0}^{j_0}\VEC{w}\|_2 > C_\alpha \|\VEC{w}\|_2$.
If we consider a boundary value problem
such that $g(x) = \VEC{w} e^{k_0 x\, i}$, then
$\displaystyle h_0(x) = \VEC{A}_{k_0,0} e^{k_0 x\, i}$ with
$\VEC{A}_{k_0,0} = \VEC{w}$.  From (\ref{fdmgfL2G}), we have
$\VEC{A}_{k_0,j} = Q_{k_0}^j\VEC{w}$.   Thus
\[
\|h_{j_0}\|_2^2 = \|\hat{h}_{j_0}\|_2^2 \geq \|\VEC{A}_{k_0,j_0}\|_2^2
= \| Q_{k_0}^{j_0} \VEC{w} \|_2^2
> C_\alpha^2 \|\VEC{w}\|_2^2 = C_\alpha^2 \|\VEC{A}_{k_0,0}\|_2^2
= C_\alpha^2 \|\hat{h}_0\|_2^2 = C_\alpha^2 \|h_0\|_2^2 \ .
\]
This contradicts $\|h_j\|_2 \leq C_\alpha \|h_0\|_2$ for
$0 \leq j \leq M$ and all $M$.
\end{proof}

We have a version of the von Neumann criteria,
Proposition~\ref{vonNNo1}, in the present context.

\begin{prop}[von Neumann]
If a finite difference scheme of the form (\ref{fdmgfL2D}) is
{\bfseries $L^2$-stable}, then there exists a constant $C_\alpha$ such that
$\rho(Q_k) \leq 1 + C_\alpha/M$ for all $k \in \ZZ$, $N$ and $M$.
\end{prop}

\begin{proof}
Since the finite difference scheme is $L^2$-stable, we get from
Proposition~\ref{QkjCstable} that
$\|Q_k^j\|_2 \leq C_\alpha$ for $0\leq j \leq M$ and $k \in \ZZ$.
Hence, from Remark~\ref{iter_LE_eig3}, we get
\[
  \rho^j(Q_k) = \rho(Q_k^j) \leq \|Q_k^j\|_2 \leq C_\alpha
\]
for $0\leq j \leq M$ and $k \in \ZZ$.  As in the proof of
Proposition~\ref{vonNNo1}, we get for $j=M$ that
\[
  \rho(Q_k) \leq C_\alpha^{1/M} \leq 1 + \frac{C_\alpha}{M} \ . \qedhere
\]
\end{proof}

We can deduce from Proposition~\ref{QkjCstable} a sufficient
condition to determine $L^2$ stability.

\begin{prop}
If the matrix $Q_k$ is normal, then the finite difference scheme of
the form (\ref{fdmgfL2D}) is $L^2$-stable if $\rho(Q_k) \leq 1$
for all $k \in \ZZ$, $N$ and $M$.
\end{prop}

\begin{proof}
Since the matrix $Q_k$ is normal, we have $\rho(Q_k) = \|Q_k\|_2$.  Hence,
$\|Q_k^j\|_2 \leq \|Q_k\|_2^j = \rho^j(Q_k) \leq 1$ for
$0\leq j \leq M$, $k \in \ZZ$ and $N>0$.
\end{proof}

\subsection{Matrix Method} \label{Moreell2Stab}

There is yet another approach to determine the convergence and stability
of a finite difference scheme.  The method presented in this section
is called the
{\bfseries Matrix Method}\index{Finite Difference Methods!Matrix Method}
because it is based on the matrix representation of the finite
difference schemes as presented in Section~\ref{EXplImplSchenes}.
We are back in finite dimension.

We only give a brief description of this method though it is widely
used in engineering for historical reasons.  We focus our discussion
on explicit one-step finite difference schemes. 

Suppose that
$P_\Delta(w_{i,j},w_{i,j+1},w_{i+1,j}, \ldots) = F(x_i,t_j)$ for all
$(i,j)$ such that $(x_i,t_j) \in R^o_\Delta$ can be expressed
in vector form as $\VEC{w}_{j+1} = Q_\alpha\VEC{w}_j + \VEC{B}_j$
for $0 \leq j < M$, where
\begin{enumerate}
\item $Q_\alpha$ is a \nm{(N-1)}{(N-1)} matrix,
\item $\displaystyle \VEC{w}_j = \begin{pmatrix}
w_{1,j} & w_{2,j} & w_{3,j} & \ldots & w_{N-1,j} \end{pmatrix}^\top$
and
\item $\VEC{B}_j
\equiv \VEC{B}_j\left(F,\{w_{r,s}:x_{r,s} \in \partial R\}\right) \in \RR^{N-1}$;
namely, $\VEC{B}_j$ is a vector valued function of $F$ evaluated at
the mesh points and the values $w_{i,j}$ on the boundary
of the domain $R$ which, for the purpose of this section, we assume
are the values of the solution $u$ on the boundary.  Note that
boundary also includes the initial values.
\end{enumerate}
The vectors $\VEC{w}_j$ for $0<j\leq M$ represent all
the values $w_{i,j}$ for $(i,j)$ such that $(x_i,t_j) \in R^o_\Delta$.

As we have done before, the index $\alpha$ in $Q_\alpha$ is to indicate
that the linear operator may depend on a relation between $\dtx{x}$
and $\dtx{t}$.

\begin{egg}
For the heat equation with forcing for instance,
the finite difference scheme given in Algorithm~\ref{fdm_sch1S} can be
expressed in the form $\VEC{w}_{j+1} = Q_\alpha\VEC{w}_j + \VEC{B}_j$ where
$Q_\alpha = -K$ for $K$ given in (\ref{fdm_K}) and
\[
\VEC{B}_j = \begin{pmatrix}
\alpha w_{0,j} + f(x_1,t_j) \dtx{t} \\
f(x_2,t_j) \dtx{t} \\
\vdots \\
f(x_{N-2},t_j) \dtx{t} \\
\alpha w_{N,j} + f(x_{N-1},t_j) \dtx{t}
\end{pmatrix} \ .
\]

The Crank-Nicolson scheme given in Algorithm~\ref{fdm_sch11S} can
be expressed in the form $\VEC{w}_{j+1} = Q_\alpha\VEC{w}_j + \VEC{B}_j$ where
$Q_\alpha = -J^{-1}K$ for $K$ given in (\ref{fdm_K}), $J$ given in
(\ref{fdm_J}) and
\[
\VEC{B}_j = J^{-1} \begin{pmatrix}
\alpha \left( w_{0,j+1} + w_{0,j} \right)
+ \big( f(x_1,t_j) + f(x_1,t_{j+1}) \big)\dtx{t}/2 \\
\big( f(x_2,t_j) + f(x_2,t_{j+1}) \big)\dtx{t}/2 \\
\vdots \\
\big( f(x_{N-2},t_j) + f(x_{N-2},t_{j+1}) \big)\dtx{t}/2 \\
\alpha \left( w_{N,j+1} + w_{N,j} \right)
+ \big( f(x_{N-1},t_j)  + f(x_{N-1},t_{j+1}) \big)\dtx{t}/2
\end{pmatrix} \ .
\]
\label{Moreell2StabEgg}
\end{egg}

We consider the norm on $\RR^{N-1}$ defined by
\[
  \| \VEC{y} \|_N = \left(\sum_{i=1}^{N-1} y_i^2\,\dtx{x}\right)^{1/2}
\]
for $\VEC{y} \in \RR^{N-1}$.  If $y_i = g(x_i)$ for all $i$, where
$g:[0,L]\to \RR$ is a continuous function, then
$\displaystyle \|\VEC{y} \|_N \to \left( \int_0^L g^2(x)\dx{x}\right)^{1/2}$
as $N \to \infty$ by definition of the Riemann integral.

We can give new, and not so new, definitions of convergence,
consistency and stability.

\begin{defn} \label{ell2ConvDefN2}
A finite difference scheme of the form
$\VEC{w}_{j+1} = Q_\alpha\VEC{w}_j + \VEC{B}_j$
with $\VEC{w}_j \in \RR^{N-1}$ is {\bfseries $\ell^2$-convergent}
\index{Finite Difference Methods!$\ell^2$-Convergent} if
\[
  \sup_{0\leq j \leq M} \left\| \VEC{w}_j - \VEC{u}_j \right\|_N \to 0
  \quad \text{as} \quad \min\{N,M\} \to \infty \ ,
\]
where $\displaystyle \VEC{u}_j = \begin{pmatrix}
u_{1,j} & u_{2,j} & u_{3,j} & \ldots & u_{N-1,j} \end{pmatrix}^\top$.
\end{defn}

\begin{defn} \label{fdm_consist_defN2}
Given any sufficiently differentiable function $q:R \to \RR$,
the {\bfseries local truncation error}\index{Finite Difference Methods!Local 
Truncation Error} of the finite difference scheme of the form
$\VEC{w}_{j+1} = Q_\alpha\VEC{w}_j + \VEC{B}_j$ is the expression
\[
\VEC{\tau}_j(\dtx{x},\dtx{t},q) = \frac{1}{\dtx{t}}
\left(\VEC{q}_{j+1} - Q_\alpha \VEC{q}_j -
\VEC{B}_j\big(F, \{ \VEC{q}(x_r,t_s) :x_{r,s} \in \partial R\} \big) \right)
\]
for $j\geq 0$, where $\VEC{q}_{i,j} = q(i\dtx{x}.j\dtx{t})$ for $0\leq i\leq N$
and $0\leq j \leq M$.

A finite difference scheme of the form $\VEC{w}_{j+1} = Q\VEC{w}_j + \VEC{B}_j$
with $\VEC{w}_j \in \RR^{N-1}$ is {\bfseries $\ell^2$-consistent}
\index{Finite Difference Methods!$\ell^2$-Consistent} if
\[
\| \VEC{\tau}_j(\dtx{x},\dtx{t},q) \|_N \to 0 \quad \text{as} \quad
\max\{N,M\} \to \infty \ ,
\]
for all function $q:R \to \RR$.
\end{defn}

\begin{rmk}
It should be pointed out that consistency according to
Definition~\ref{fdm_consist_def} automatically implies
consistency according to the previous definition because the $i^{th}$
component of $\VEC{\tau}_j(\dtx{x},\dtx{t},q)$ is
$\tau_{i,j}(\dtx{x},\dtx{t},q)$ as defined in 
Definition~\ref{fdm_consist_def}.  Hence
\begin{align*}
\| \VEC{\tau}_j(\dtx{x},\dtx{t},q) \|_N^2
&= \dtx{x} \sum_{i=1}^{N-1} \left(\tau_{i,j} (\dtx{x},\dtx{t},q)\right)^2
\leq (N-1)\dtx{x} \max_{0<i<N} \left|\tau_{i,j} (\dtx{x},\dtx{t},q)\right|^2 \\
&\leq L \max_{\substack{(i,j) \text{ such that}\\(x_iy_j) \in R^o}}
\left|\tau_{i,j} (\dtx{x},\dtx{t},q)\right|^2 \ .
\end{align*}
\label{rmk_consist_defN2}
\end{rmk}

\begin{defn} \label{fdmStableDefNo4}
A finite difference scheme of the form
$\VEC{w}_{j+1} = Q_\alpha\VEC{w}_j + \VEC{B}_j$
with $\VEC{w}_j \in \RR^{N-1}$ is {\bfseries $\ell^2$-stable}
\index{Finite Difference Methods!$\ell^2$-Stable} if there
exists a constant $C_\alpha$ such that $\|Q_\alpha^j\|_N \leq C_\alpha$
for all $N>0$ and $j\geq 0$.
\end{defn}

A word of caution about the previous definitions of convergence and
stability.  Because of the dependence of $Q_\alpha$ on $\alpha$, a
relation between $\dtx{x}$ and $\dtx{t}$ may need to be satisfied to
ensure convergence and stability.  So, $N$ and 
$M$ may not be totally independent of each other.

Definition~\ref{fdmStableDefNo4} is a weaker
definition of stability than Definition~\ref{fdmStableDefNo1} in the
sense that it ensures that round off errors do not increase in
$\ell^2$-norm instead of in the uniform norm as $M$ increases.

It follows from Theorem~\ref{spectral} that
$\rho(Q_\alpha) \leq \|Q_\alpha\|_N$
for all norms on the space of \nm{(N-1)}{(N-1)} matrices where
$\rho(Q_\alpha)$ is the spectral radius of $Q_\alpha$.  So, if
$\|Q_\alpha\|_N\leq 1$, then $\rho(Q_\alpha)\leq 1$; namely, all the
eigenvalues of $Q_\alpha$ are less than or equal to $1$ in absolute
value.   We have a partial converse to this statement.  If $Q_\alpha$
is a normal matrix (i.e.\ $Q_\alpha Q_\alpha^\top = Q_\alpha^\top Q_\alpha$)
and the induced norm on the \nm{(N-1)}{(N-1)} matrices is
the Euclidean norm $\|\cdot\|_2$ on $\RR^{N-1}$, then
$\|Q_\alpha\|_2 = \rho(Q_\alpha)$.
This is also true if the Euclidean norm is replaced by a multiple of
the Euclidean norm as we have for $\|\cdot\|_N$.  So,
$\|Q_\alpha\|_N = \rho(Q_\alpha)$.
Thus $\rho(Q_\alpha) \leq 1$ if and only if $\|Q_\alpha\|_N \leq 1$.  We also get
that $\|Q_\alpha^j\|_N \leq \|Q_\alpha\|_N^j \leq 1$.

We get the following result.

\begin{prop}\label{MatrixStabProp}
A finite difference scheme of the form
$\VEC{w}_{j+1} = Q_\alpha\VEC{w}_j + \VEC{B}_j$,
where $\VEC{w}_j \in \RR^{N-1}$ and $Q_\alpha$ is normal, is
$\ell^2$-stable according to Definition~\ref{fdmStableDefNo4}
if all the eigenvalues of $Q_\alpha$ are less than or equal to $1$ in
absolute value, independently of the value of $N$.
\end{prop}

\begin{rmk}
As we saw in Section~\ref{EXplImplSchenes}, we may represent the
finite difference scheme formed of (\ref{fdm_fde_tr}) and
(\ref{fdm_fde_tr_bdry}) as a linear system $A\VEC{w} = \VEC{B}$.

There is yet another definition of stability that is frequently used in
this situation.  A linear system of the form $A \VEC{w} = \VEC{B}$ is
{\bfseries stable}\index{Finite Difference Methods!Stable} if there
exists a constant $K_\alpha$ such that
$\| \VEC{w} \| \leq K_\alpha \| A \VEC{w}\|$
for all $\VEC{w}$.  This definition is reminiscent of the property
that well-conditioned linear systems of equations have.  If $\VEC{w}_1$
and $\VEC{w}_2$ are two solutions of $A \VEC{w} = \VEC{B}$, then
$\displaystyle \left\| \VEC{w}_1- \VEC{w}_2 \right\| \leq
K_\alpha \left\| A \left(\VEC{w}_1 - \VEC{w}_2\right)\right\|$.  So, if the
difference between $A \VEC{w}_1$ and $A \VEC{w}_2$ is small, then the
difference between $\VEC{w}_1$ and $\VEC{w}_2$ should also be
proportionally small.  This is the property that we have associated to
well-conditioned systems in Section~\ref{ErrorEstAxb}.  We will not
treat this subject.
\end{rmk}

\begin{rmk}
There is a link between definition of stability given in
Definition~\ref{fdmStableDefNo3} and Definition~\ref{fdmStableDefNo4}.

Suppose that $w_{k,j} = g_j(k \dtx{x})$ for a continuous function
$g_j:[0,2\pi]\to \RR$ for all $j$.  Then
\[
 \|\VEC{w}_j\|_N^2 = \sum_{k=1}^{N-1} |w_{k,j}|^2 \dtx{x}
= \sum_{k=1}^{N-1} |g_j(k\dtx{x})|^2 \dtx{x}
\to \int_0^{2\pi} |g_j|^2 \dx{x} = \|g_j\|_2^2
\]
as $N \to \infty$ and so $\dtx{x} \to 0$.

Given two real numbers $0 < S_1 < 1 < S_2$, there exists $N_M$ large
enough such that
\[
  S_1 \leq \frac{\|\VEC{w}_j\|_N}{\|g_j\|_2} \leq S_2
\]
for $0\leq j \leq M$ and $N \geq N_M$.  We assume that
$\|g_j\|_2 >0$ for $0 \leq j \leq M$ and leave the case
$\|g_j\|_2 = 0$ for some $j$ to the reader.  Hence,
\[
\|\VEC{w}_j\|_N \leq \frac{C_\alpha S_2}{S_1} \|\VEC{w}_0\|_N
\iff \|g_j\|_2 \leq C_\alpha \|g_0\|_2
\]
for $0\leq j \leq M$ and $N \geq N_M$.
\end{rmk}

\subsection{Conclusion}

We have seen several definitions of convergence and stability.  Some
of them were based on a Cauchy problem and so were ignoring the
boundary conditions that some partial differential equations may have
to satisfied.  Which definitions should we use?  This depends on the problem
and on what we want to achieve.

Uniform approximation of the solution seems to be the ultimate
goal to achieve but this is not possible for all finite difference
schemes.  Some finite difference schemes may have very desirable
features other than uniform convergence.  So $\ell^2$ convergence and
stability may be preferable.

There is also the issue of proving stability.  Proving stability for
uniform approximation is far from trivial when possible.
Using Definition~\ref{fdmStableDefNo4} to determine $\ell^2$ stability
may seem to be the next reasonable option but that requires a
nice matrix (usually ``near'' diagonal) to be able to determine the
eigenvalues of $Q_\alpha$.  Using Definitions~\ref{fdmStableDefNo1}
and \ref{fdmStableDefNo2} may be the best options.  They may not be
considering partial differential equation with boundary conditions but
may be good enough to ensure stability.  That is our ultimate goal to
avoid propagation of round off errors.

The reader may have noticed that we did not mention consistency in
this conclusion.  The reason is simple.  Must of the finite difference
schemes (at least those that we have presented) are developed from
finite difference formulae as those in Section~\ref{Basicfdf} to
ensure that the local truncation error is of order greater than one
in $\dtx{x}$ and $\dtx{t}$.  This is enough to ensure consistency.

\section{Preliminaries of Linear Algebra}

We take a little pause to review some notions of linear algebra that
will be needed later on.

\begin{prop}
Consider the tri-diagonal matrix
\[
Q = \begin{pmatrix}
a & b & 0 & 0 & 0 & \ldots & 0 & 0 \\
c & a & b & 0 & 0 & \ldots & 0 & 0  \\
0 & c & a & b & 0 & \ldots & 0 & 0 \\
0 & 0 & c & a & b & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & \cdots & c & a
\end{pmatrix}
\]
of dimension \nn.  The eigenvalues of $Q$ are
\[
\lambda_k = a + 2 b \sqrt{\frac{c}{b}} \cos\left(\frac{k\pi}{n+1}\right)
\quad , \quad 0 < k < n+1 \; .
\]
A possible eigenvector associated to $\lambda_k$ is
\[
\VEC{v}_k =
\begin{pmatrix}
\left(c/b\right)^{1/2} \sin\left(k\pi/(n+1)\right) \\
\left(c/b\right)^{2/2} \sin\left(2k\pi/(n+1)\right) \\
\vdots \\
\left(c/b\right)^{n/2} \sin\left(nk\pi/(n+1)\right)
\end{pmatrix} \ .
\]
\label{fdm_eigR}
\end{prop}

\begin{proof}
Let $\VEC{v}$ be an eigenvector of $Q$ associated to the eigenvalue
$\lambda$.  If we set $v_0 = v_{n+1} = 0$, the equation
$A\VEC{v} = \lambda \VEC{v}$ can be written as
\begin{equation} \label{fdm_DE1}
c v_{j-1} + (a-\lambda) v_j + b v_{j+1} = 0 \quad ,
\quad 1 \leq j \leq n \ .
\end{equation}
To find the solution of this difference equation, we have to find the
roots of the characteristic equation
\begin{equation} \label{fdm_DE2}
b \rho^2 + (a-\lambda) \rho + c = 0 \; .
\end{equation}
Let $\rho_1$ and $\rho_2$ be the roots of (\ref{fdm_DE2}).
Since $\rho_1\rho_2 = c/b \neq 0$, none of the roots is null.

If $\rho_1 = \rho_2$, the solution of (\ref{fdm_DE1}) is of the form 
$\displaystyle v_j = \alpha \rho_1^j + \beta j \rho_1^j$ for
$0 \leq j \leq n+1$. Since $v_0 = 0$ implies $\alpha=0$, we get that
$v_{n+1} = 0$ implies $\beta (n+1) \rho_1^{n+1} = 0$.  It follows that
$\beta=0$.  We find $v_j=0$ for all $j$ which is not possible for an
eigenvector.

We may assume that $\rho_1$ and $\rho_2$ are two distinct and non null
roots.  In this case, the solution of (\ref{fdm_DE1}) is of the form
\begin{equation} \label{fdm_DE4}
v_j = \alpha \rho_1^j + \beta \rho_2^j \quad , \quad 0 \leq j \leq n+1 \ . 
\end{equation}
From $v_0=0$, we get $0=\alpha+\beta$.  Hence $\beta=-\alpha$.  From
$v_{n+1} = 0$, we get
\[
0 = \alpha \rho_1^{n+1} + \beta \rho_2^{n+1}
= \alpha \left( \rho_1^{n+1} -\rho_2^{n+1}\right) \ .
\]
Thus $\displaystyle \left(\rho_1/\rho_2\right)^{n+1} = 1$.
It follows that $\rho_1/\rho_2$ is a $(n+1)$ root of the unity;
namely,
\begin{equation} \label{fdm_DE3}
\frac{\rho_1}{\rho_2} = e^{2k\pi\,i/(n+1)} \quad ,
\quad 0 \leq k < n+1 \ .
\end{equation}
Hence,
\[
\frac{c}{b} = \rho_1\rho_2 = \left(\rho_2 e^{2k\pi\,i/(n+1)}\right)
\rho_2 = \rho_2^2 e^{2k\pi\,i/(n+1)}
\]
yields
\[
\rho_2 =  \sqrt{\frac{c}{b}} e^{-k\pi\,i/(n+1)} \quad ,
\quad 0 \leq k < n+1 \ .
\]
We also get from (\ref{fdm_DE3}) that
\[
\rho_1 =  \sqrt{\frac{c}{b}} e^{k\pi\,i/(n+1)} \quad ,
\quad 0 \leq k < n+1 \ .
\]
We have to ignore $k=0$ because this gives
$\rho_1 = \rho_2 = \sqrt{c/b}$ which is impossible as we have shown before.

Finally,
\[
- \frac{a-\lambda}{b} = \rho_1+\rho_2 = 
\sqrt{\frac{c}{b}} e^{k\pi\,i/(n+1)} + 
\sqrt{\frac{c}{b}} e^{-k\pi\,i/(n+1)} =
2 \sqrt{\frac{c}{b}} \cos\left(\frac{k\pi}{n+1}\right)
\quad , \quad 0 < k < n+1 \ .
\]
Thus, the eigenvalues of $Q$ are
\[
\lambda_k = a + 2 b \sqrt{\frac{c}{b}} \cos\left(\frac{k\pi}{n+1}\right)
\quad , \quad 0 < k < n+1 \ .
\]

Since $\beta = \alpha$ in (\ref{fdm_DE4}), the eigenvectors
$\VEC{v}_k$ of $Q$ associated to the eigenvalue $\lambda_k$ will have
the components
\[
v_{j,k} = \alpha \left(\left(\frac{c}{b}\right)^{j/2} e^{jk\pi\,i/(n+1)}
- \left(\frac{c}{b}\right)^{j/2} e^{-jk\pi\,i/(n+1)}\right)
= 2 \alpha \,i \left(\frac{c}{b}\right)^{j/2}
\sin\left(\frac{jk\pi}{n+1}\right) \quad, \quad
0 < j < n+1 \ .
\]
Since $\alpha$ can be any non-null complex number, we may
take $\alpha =-i/2$ to get real eigenvectors
\[
\VEC{v}_k =
\begin{pmatrix}
\left(c/b\right)^{1/2} \sin\left(k\pi/(n+1)\right) \\
\left(c/b\right)^{2/2} \sin\left(2k\pi/(n+1)\right) \\
\vdots \\
\left(c/b\right)^{n/2} \sin\left(nk\pi/(n+1)\right)
\end{pmatrix} \ .  \qedhere
\]
\end{proof}

\begin{prop}\label{fdm_eig_RR}
Consider the matrix
\[
Q =
\begin{pmatrix}
Q_{1,1} & Q_{1,2} & \ldots & Q_{1,s} \\
Q_{2,1} & Q_{2,2} & \ldots & Q_{2,s} \\
\vdots & \vdots & \ddots & \vdots \\
Q_{s,1} & Q_{s,2} & \ldots & Q_{s,s} \\
\end{pmatrix} \ ,
\]
where each sub-matrix $Q_{i,j}$ is a matrix of dimension \nn.
Suppose that $\VEC{v}_1$, $\VEC{v}_2$, \ldots, $\VEC{v}_n$ in $\RR^n$
are $n$ linearly independent eigenvectors for each matrix $Q_{i,j}$.
Let $\displaystyle \lambda_{i,j,k}$ be the eigenvalue of $Q_{i,j}$
associated to the eigenvector $\VEC{v}_k$ for $1\leq i, j \leq s$ and
$1\leq k \leq n$.  Then the eigenvalues of the \nm{s}{s} matrices
\[
P_k =
\begin{pmatrix}
\lambda_{1,1,k} & \lambda_{1,2,k} & \ldots & \lambda_{1,s,k} \\
\lambda_{2,1,k} & \lambda_{2,2,k} & \ldots & \lambda_{2,s,k} \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_{s,1,k} & \lambda_{s,2,k} & \ldots & \lambda_{s,s,k} \\
\end{pmatrix}
\]
for $1 \leq k \leq n$ are eigenvalues of $Q$.
\end{prop}

\begin{proof}
Suppose that $\lambda$ is an eigenvalue of $P_k$ for some $k$ fixed.  We
will show that there exist $a_1$, $a_2$, \ldots, $a_s$ in $\RR$ such
that
\[
\VEC{v} =
\begin{pmatrix}
a_1 \VEC{v}_k \\ a_2 \VEC{v}_k \\ \vdots \\ a_s \VEC{v}_k
\end{pmatrix}
\]
is an eigenvector of $Q$ associated to the eigenvalue $\lambda$.

The following statement about $\lambda$ and the vector $\VEC{v}$ above
are equivalent:
{
\renewcommand{\labelenumi}{\roman{enumi}.}
\begin{enumerate}
\item $\VEC{v}$ is an eigenvector of $Q$ associated to the
  eigenvalue $\lambda$; namely, $Q\VEC{v} = \lambda \VEC{v}$.
\item
\[
\sum_{j=1}^s a_j Q_{i,j}\VEC{v}_k =
\sum_{j=1}^s a_j \lambda_{i,j,k}\VEC{v}_k = \lambda\, a_i\, \VEC{v}_k \quad ,
\quad 1 \leq i \leq s \ .
\]
\item
\[
\sum_{\substack{j=1\\j\neq i}}^s a_j \lambda_{i,j,k}\VEC{v}_k +
a_i \left(\lambda_{i,i,k} - \lambda \right) \VEC{v}_k = 0 \quad ,
\quad 1 \leq i \leq s \ .
\]
\item $R \VEC{a} = \VEC{0}$, where
\[
R =
\begin{pmatrix}
\lambda_{1,1,k} -\lambda & \lambda_{1,2,k} & \ldots & \lambda_{1,s,k} \\
\lambda_{2,1,k} & \lambda_{2,2.k}-\lambda & \ldots & \lambda_{2,s,k} \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_{s,1,k} & \lambda_{s,2,k} & \ldots & \lambda_{s,s,k}-\lambda \\
\end{pmatrix} \ , \quad
\VEC{a} =
\begin{pmatrix}
a_1 \\ a_2 \\ \vdots \\ a_s
\end{pmatrix}
\ \text{and} \ 
\VEC{0} =
\begin{pmatrix}
0 \\0 \\ \vdots \\ 0
\end{pmatrix} \ .
\]
\end{enumerate}
}
Since $\lambda$ is an eigenvalue of $P_k$.  The matrix $R$ is not
invertible and, therefore, there exists a non-trivial solution $\VEC{a}$
of $R \VEC{a} = \VEC{0}$.  This solution yields the eigenvector
$\VEC{v}$ of $Q$ associated to $\lambda$.
\end{proof}

\section{Heat Equation}

\subsection{Algorithm~\ref{fdm_sch1S}}

We study the uniform convergence of Algorithm~\ref{fdm_sch1S} which is
used to approximate the solution of the heat equation with forcing.

\begin{prop}
The scheme in Algorithm~\ref{fdm_sch1S} is consistent.
\end{prop}

\begin{proof}
Using the notation introduced in Section~\ref{fdm_CCS}, we have that
\[
P\left(u(x,t), \pdydx{u}{x}(x,t), \pdydx{u}{y}(x,t),
  \pdydxn{u}{x}{2}(x,t), \ldots \right)
= \pdydx{u}{t}(x,t) - c^2 \pdydxn{u}{x}{2}(x,t)
\]
and
\[
P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right)
= \frac{w_{i,j+1} - w_{i,j}}{\dtx{t}} - c^2 \frac{w_{i+1,j} - 2w_{i,j} +
  w_{i-1,j}}{(\dtx{x})^2}
\]
for the finite difference scheme in Algorithm~\ref{fdm_sch1S}.

The local truncation error of the finite difference scheme in
Algorithm~\ref{fdm_sch1S} is deduced from (\ref{fdm_sch1_tr}) with the
function $u$ replaced by the function $q$.  We have
\begin{align*}
\tau_{i,j}(\dtx{x},\dtx{t},q) &= 
P\left(q(x_i,t_j), \pdydx{q}{x}(x_i,t_j), \pdydx{q}{y}(x_i,t_j),
  \pdydxn{q}{x}{2}(x_i,t_j), \ldots \right) \\
&\qquad\quad
- P_\Delta\left(q(x_i,t_j), q(x_i,y_{j+1}), q(x_{i+1},t_j), \ldots\right) \\
&= -\frac{1}{2} \pdydxn{q}{t}{2}\left(x_i,\rho_{i,j}\right) \dtx{t}
+ \frac{c^2}{4!} \left( \pdydxn{q}{x}{4}\left(\zeta_{i,j},t_j\right)
+ \pdydxn{q}{x}{4}\left(\eta_{i,j},t_j\right) \right) (\dtx{x})^2
\end{align*}
for some $\rho_{i,j} \in ]t_j,t_{j+1}[$, and
$\zeta_{i,j}, \eta_{i,j} \in ]x_{i-1},x_{i+1}[$.  If the partial
derivatives of order up to four of $q$ are continuous on the compact set
\begin{equation} \label{fdm_sch1_dom}
R = \left\{ (x,t) : 0\leq x \leq L \ \text{and}\ 0 \leq t \leq T \right\} \ ,
\end{equation}
then there exists $H$ such that
\[
\max_{(x,t)\in R}\, \left|\pdydxn{q}{t}{2}\left(x,t\right) \right| \leq
H \ \text{and} \
\max_{(x,t)\in R}\, \left|\pdydxn{q}{x}{4}\left(x,t\right) \right| \leq
H \ .
\]
Hence,
\begin{equation}\label{fdm_sch1Trunc}
\left| \tau_{i,j}(\dtx{x},\dtx{t},q) \right | \leq
\frac{H}{2} \dtx{t} + \frac{c^2 H}{12} (\dtx{x})^2
\end{equation}
for $i$ and $j$.  We conclude that
\begin{equation} \label{sch1SCons}
\max \left\{ \left|\tau_{i,j}(\dtx{x},\dtx{t},q)\right| :
0<i<N \ \text{and} \ 0\leq j < M
\right\} \rightarrow 0 \quad \text{as} \quad
\min \{N, M\} \rightarrow \infty
\end{equation}
since $\dtx{x} = L/N$ and $\dtx{t} = T/M$ converge to $0$ as
$\min\{N,N\}$ converges to infinity.

Since $\displaystyle B\left(u(x,y), \ldots \right) = u(x,y)$,
$\displaystyle B_\Delta(w_{i,j},\ldots) = w_{i,j}$ and $w_{i,j} = u(x_i,t_j)$
for $(i,j)$ such that $(x_i,t_j) \in \partial R_\Delta$, we get from 
(\ref{sch1SCons}) that Definition~\ref{fdm_consist_def} is satisfied.
\end{proof}

\begin{prop}
The finite difference scheme in Algorithm~\ref{fdm_sch1S} is stable
as defined in Definition~\ref{fdmStableDefNo1} if
\[
\frac{c^2\dtx{t}}{(\dtx{x})^2} \leq \frac{1}{2} \ .
\]
\end{prop}

\begin{proof}
Consider a function $v:R_\Delta \to \RR$.  Let $v_{i,j} = v(x_i,t_j)$
for all $(x_i,t_j) \in R_\Delta$ and let
$f(x_i,t_j) = P_\Delta(v_{i,j},v_{i,j+1},v_{i+1,j},\ldots)$.

We have
\begin{align*}
v_{i,j+1} &= v_{i,j} +\alpha \left(v_{i+1,j} - 2v_{i,j} +
  v_{i-1,j}\right) + f(x_i,t_j) \dtx{t} \\
&= (1-2\alpha) v_{i,j} +\alpha v_{i+1,j} + \alpha v_{i-1,j}
+ f(x_i,t_j) \dtx{t}
\end{align*}
for $0<i<N$ and $0\leq j < M$,
where $\displaystyle \alpha = \frac{c^2\dtx{t}}{(\dtx{x})^2}$.

Let
\[
v_j = \max\bigg\{ \max_{0\leq i \leq N} |v_{i,j}|, 
\max_{\substack{i=0,N \text{ and}\\0\leq j \leq M}} |v_{i,j}|
\bigg\}
\]
and
$\displaystyle F = \max_{\substack{0<i<N\\0\leq j<M}} |f(x_i,y_j)|$.

If $\alpha < 1/2$, we get
\begin{align*}
|v_{i,j+1}| &\leq (1-2\alpha) |v_{i,j}| +\alpha |v_{i+1,j}| + \alpha |v_{i-1,j}|
+ |f(x_i,y_i)| \dtx{t} \\
&\leq (1-2\alpha) v_j +\alpha v_j + \alpha v_j + F \dtx{t}
  = v_j + F \dtx{t}
\end{align*}
for $0<i<N$ and $0\leq j < M$.  Thus
$\displaystyle v_{j+1} \leq v_j + F \dtx{t}$ for $0\leq j < M$.
By induction, we get
\[
  v_j \leq v_0 + (j \dtx{t}) F \leq v_0 + T F 
\]
for $0\leq j \leq M$.  Hence,
\[
  |v_{i,j}| \leq v_j \leq v_0 + T F
\]
for $0\leq j \leq M$ and $0\leq i \leq N$.  Since
$f(x_i,y_j) = P_\Delta(v_{i,j},v_{i,j+1},v_{i+1,j},\ldots)$
for $(i,j)$ such that $(x_i,t_j) \in R_\Delta^o$
and $B_\Delta(v_{i,j},v_{i,j+1},v_{i+1,j},\ldots) = v_{i,j}$ for the
$(i,j)$ such that $(x_i,t_j) \in \partial R_\Delta$.  We can
rewrite the previous inequality as
\begin{align*}
|v_{i,j}| &\leq C \bigg\{
\max_{\substack{(i,j)\ \text{such that}\\(x_i,t_j) \in \partial R_\Delta}}
|B_\Delta(v_{i,0},v_{i,1},v_{i+1,0},\ldots)|
+ \max_{\substack{(i,j)\ \text{such that}\\(x_i,t_j) \in R_\Delta^o}}
|P_\Delta(v_{i,j},v_{i,j+1},v_{i+1,j},\ldots)| \bigg\}
\end{align*}
for $0\leq j \leq M$ and $0\leq i \leq N$, and $C = \max \{1, T\}$.
We get (\ref{stabCondFDM}).
\end{proof}

In Question~\ref{fdmQ3} of the exercise section below, the reader is
asked to prove that the finite difference scheme in
Algorithm~\ref{fdm_sch1S} is $\ell^2$-stable if
$\displaystyle c^2\dtx{t}/(\dtx{x})^2 \leq 1/2$.

The next proposition follows from Theorem~\ref{StabConstConv}.

\begin{prop}\label{fdm_sch1S_convTh}
The finite difference scheme in Algorithm~\ref{fdm_sch1S} is
convergent if
$\displaystyle \frac{c^2\dtx{t}}{(\dtx{x})^2} \leq \frac{1}{2}$.
\end{prop}

This scheme is fairly restrictive because $\dtx{t}$ is forced
to be very small if $\dtx{x}$ is small.  If $\dtx{x} < 10^{-2}$, then
$\dtx{t} <  10^{-4}/(2c^2)$.  Thus, a lot of computations
are required to advance moderately in time.  For this reason, this
finite difference scheme is not recommended.

Though we have proved Proposition~\ref{fdm_sch1S_convTh} about
the convergence of the finite difference scheme in
Algorithm~\ref{fdm_sch1S}. it is instructive to prove it again from the
definition of convergence.

\begin{proof}
Let $r_{i,j} = w_{i,j} - u(x_i,t_j)$ for $0\leq i \leq N$ and
$0 \leq j \leq M$, where $u$ is the solution of (\ref{HeatEqu})
with the associated boundary and initial conditions given in
(\ref{HeatEquBC}) and (\ref{HeatEquBC}) respectively.
If we subtract
\[
\frac{u(x_i,t_{j+1}) - u(x_i,t_j)}{\dtx{t}}
- c^2\, \frac{u(x_{i+1},t_j) - 2u(x_i,t_j) + u(x_{i-1},t_j)}{(\dtx{x})^2} = 
f(x_i,t_j) - \tau_{i,j}(\dtx{x},\dtx{t},u) \ ,
\]
from
\[
\frac{w_{i,j+1} - w_{i,j}}{\dtx{t}}  - c^2\, \frac{w_{i+1,j} - 2w_{i,j} +
w_{i-1,j}}{(\dtx{x})^2} = f(x_i,t_j) \ ,
\]
we get
\[
r_{i,j+1} - r_{i,j} - \alpha \left( r_{i+1,j} - 2 r_{i,j}  + r_{i-1,j} \right)
= \tau_{i,j}(\dtx{x},\dtx{t},u) \, \dtx{t} \ .
\]
where $\displaystyle \alpha = \frac{c^2\dtx{t}}{(\dtx{x})^2}$,  Let
$\displaystyle R_j = \max_{0 < i < N}|r_{i,j}|$.  Since we assume that
$w_{i,j} = u(x_i,t_j)$ for $i=0$ and $i=N$, we have in fact that
$\displaystyle R_j = \max_{0 \leq i \leq N}|r_{i,j}|$.

If we assume that $1 - 2\alpha >0$ and use (\ref{fdm_sch1Trunc}), we
get
\begin{align*}
|r_{i,j+1}| &= \big| (1-2\alpha) r_{i,j} + \alpha r_{i+1,j} + \alpha
  r_{i-1,j} + \tau_{i,j}(\dtx{x},\dtx{t},u) \, \dtx{t} \big| \\
&\leq (1-2\alpha) |r_{i,j}| + \alpha |r_{i+1,j}| + \alpha
  |r_{i-1,j}| + |\tau_{i,j}(\dtx{x},\dtx{t},u)| \, \dtx{t} \\
&\leq R_j + \left(\frac{H}{2} \dtx{t} + \frac{c^2 H}{12} (\dtx{x})^2 \right)
\dtx{t}
\end{align*}
for $0 < i < N$.  Thus,
\[
R_{j+1} \leq R_j + \left(\frac{H}{2} \dtx{t} + \frac{c^2 H}{12}
(\dtx{x})^2 \right) \dtx{t} \ .
\]
It follows by induction that
\[
R_j \leq R_0 + \left(\frac{H}{2} \dtx{t} + \frac{c^2 H}{12}
  (\dtx{x})^2 \right) \, j \dtx{t}
= R_0 + \left(\frac{H}{2} \dtx{t} + \frac{c^2 H}{12}
  (\dtx{x})^2 \right) t_j
\]
for $0\leq j \leq M$.
Since we assume that $w_{i,0} = u(x_i,0) = g(x_i)$ for $0\leq i \leq N$,
we have that $R_0 = \VEC{0}$.  Thus,
\[
\max_{\substack{0\leq i\leq N\\0 \leq j <M}} |w_{i,j} - u(x_i,t_j) |
\leq \max_{0 \leq j \leq M} R_j
\leq \left(\frac{H}{2} \dtx{t} + \frac{c^2 H}{12}
(\dtx{x})^2 \right) L \to 0
\]
as $\min(N,M) \to \infty$ as long as
\begin{equation} \label{fdm_sch1Cond}
\alpha = \frac{c^2\dtx{t}}{(\dtx{x})^2} \leq \frac{1}{2} \ .
\end{equation}
We have proved Proposition~\ref{fdm_sch1S_convTh}.
\end{proof}

In fact, the condition (\ref{fdm_sch1Cond}) is
necessary as can be seen from the following example.

\begin{egg}
This example comes from \cite{IK}.  We consider the heat equation
\[
\pdydx{u}{t} = c^2 \pdydxn{u}{x}{2} \quad , \quad
-\infty < x < \infty  \ \text{and} \ 0<t<T \ ,
\]
with the initial condition
\[
u(x,0) = g(x) = \sum_{m=0}^\infty \beta_m
\cos\left(\frac{2^m \pi x}{L}\right)
\]
for $0 \leq x \leq L$. where we assume that
$\displaystyle \sum_{m=1}^\infty|\beta_m|$ and
$\displaystyle \sum_{m=1}^\infty\beta_m^2$ converge.  This ensure that
$g(x)$ is a differentiable function which satisfy $g(0) = g(L)$.

Since the initial condition is a periodic function of period $L$, we
may assume that the solution $u(x,t)$ is periodic of period $L$
with respect to $x$.

We consider the finite difference scheme
\begin{equation}\label{heatfdmegg}
\frac{w_{i,j+1} - w_{i,j}}{\dtx{t}}  - c^2\, \frac{w_{i+1,j} - 2w_{i,j} +
w_{i-1,j}}{(\dtx{x})^2} = 0
\end{equation}
with $\dtx{x} = L/N$ and $\dtx{t} = T/M$.  The domain of the finite
difference scheme is
\[
R_\Delta = \{ (x_i,y_j) : x_i = i \dtx{x} \ \text{for} \ i \in \ZZ,
\ \text{and} \ y_j = j \dtx{y} \ \text{for} \ 1 \leq j \leq M \}
\]
with
\[
\partial R_\Delta =  \{ (x_i,0) : x_i = i \dtx{x} \ \text{for} \ i \in \ZZ \}
\ .
\]

As is done to solve the heat equation using separation of variables,
we seek solutions of (\ref{heatfdmegg}) of the form
$w_{i,j} = e^{a t_j}\cos(b x_i)$ for $0\leq i \leq N$ and
$0 \leq j \leq M$, and some constants $a$ and $b$ to be determined.
If we substitute this expression of $w_{i,j}$ in (\ref{heatfdmegg}),
we get
\begin{align*}
0 &= \frac{e^{at_{j+1}}\cos(bx_i) - e^{at_j}\cos(bx_i)}{\dtx{t}}  -
c^2\, \frac{e^{at_j}\cos(bx_{i+1})  - 2 e^{at_j}\cos(bx_i) +
e^{at_j}\cos(bx_{i-1})}{(\dtx{x})^2} \\
&=\frac{e^{at_j}e^{a\dtx{t}}\cos(bx_i) - e^{at_j}\cos(bx_i)}{\dtx{t}}\\
&\qquad\qquad
- c^2\, \frac{e^{at_j}\cos(bx_i)\cos(b\dtx{x}) - 2 e^{at_j}\cos(bx_i) +
e^{at_j}\cos(bx_i)\cos(b\dtx{x})}{(\dtx{x})^2} \\
&= \frac{e^{at_j}\cos(bx_i)}{\dtx{t}} \left( e^{a\dtx{t}} - 1 + 2\alpha
(1 - \cos(b\dtx{x})) \right)
= \frac{e^{at_j}\cos(bx_i)}{\dtx{t}} \left( e^{a\dtx{t}} - 1 + 4\alpha
\sin^2\left(\frac{b\dtx{x}}{2}\right) \right)
\end{align*}
for all $i$ and $j$, where $\alpha$ is defined in
(\ref{fdm_sch1Cond}).  Thus, we must have that
\[
  e^{a\dtx{t}} = 1 - 4\alpha \sin^2\left(\frac{b\dtx{x}}{2}\right) \ .
\]
We have found that
\[
w_{i,j} = e^{at_j}\cos(bx_i) = \cos(b x_i) (e^{a\dtx{t}})^{t_j/\dtx{t}}
= \cos(b x_i) \left( 1 - 4\alpha \sin^2\left(\frac{b\dtx{x}}{2}\right)
\right)^{t_j/\dtx{t}} \ .
\]

Since the solution $u(x,t)$ is periodic of period $L$ with respect to $x$, we
must have that $w_{i+N,j} = w_{i,j}$ for all $i$.  This is certainly
true for $j=0$ because we have that
\[
  w_{i+N,0} = g(x_{i+N}) = g(x_i + N\dtx{x}) = g(x_i+L) = g(x_i)
\]
for all $i$.  For this periodic condition to be satisfied, we must
have that $\displaystyle b = b_n = 2n\pi/L$ for an integer $n$ that we
may assume positive.

Let
\[
w_{i,j}^{[n]} = e^{at_j}\cos(b_n x_i)
= \cos(b_n x_i) (e^{a\dtx{t}})^{t_j/\dtx{t}} = \cos(b_n x_i)
\left(1 - 4\alpha \sin^2\left(\frac{b_n\dtx{x}}{2}\right)\right)^{t_j/\dtx{t}}
\]
for $n>0$.  We seek a solution $w_{i,j}$ of the form
\[
w_{i,j} = \sum_{n=0}^\infty \gamma_n w_{i,j}^{[n]}
\]
for $i \in \ZZ$ and $0 \leq j \leq M$.  The periodicity with respect
to $i$ is still satisfied.

The initial condition $w_{i,0} = g(x_i)$ for all $i$ yields
\[
\sum_{n=0}^\infty \gamma_n \cos(b_n x_i) =
\sum_{m=0}^\infty \beta_m \cos\left(\frac{2^m \pi x_i}{L}\right) \ .
\]
Thus,
\[
\gamma_n = \begin{cases} \beta_m & \quad \text{for} \ n = 2^{m-1} \\
0 & \quad \text{otherwise}
\end{cases}
\]
We have just matched the coefficients of two Fourier cosine series.
We get
\[
w_{i,j} = \sum_{m=1}^\infty \beta_m
\cos\left(\frac{2^m\pi x_i}{L}\right)
\left(1  - 4\alpha \sin^2\left(\frac{2^m \pi \dtx{x}}{2L}\right)
\right)^{t_j/\dtx{t}} \ .
\]

We choose a positive integer $S$ such that
$\displaystyle c^2T/L^2\leq S < 2c^2T/L^2$, where we
assume that $\displaystyle 2c^2 T/ L^2 > 1$.
If $N = 2^k$ and $M= 2^{2k}S$ for $k>0$ arbitrary, then
$\dtx{x} = L/2^k$ and $\dtx{t} = T/(S2^{2k})$.
We have that
\[
\alpha = \frac{c^2\dtx{t}}{(\dtx{x})^2}
= \frac{ c^2 T /(S2^{2k})}{L^2/2^{2k}}
= \frac{T c^2}{S L^2} 
\]
with
$\displaystyle \frac{1}{2} <  \frac{T c^2}{SL^2} \leq 1$.
We get
\[
w_{i,j} = \sum_{m=1}^\infty \beta_m
\cos\left( \frac{ 2^m i \pi}{2^k}\right)
\left(1 - 4\alpha \sin^2\left( \frac{2^m\pi}{2^{k+1}} \right)
\right)^{2^{2k} S t_j/T} \ .
\]

For $m > k$, we have
$\displaystyle \sin^2\left( \frac{2^m\pi}{2^{k+1}} \right)
= \sin^2\left( 2^{m-k-1}\pi\right) = 0$
because $m - k -1 \geq 0$.  Thus
\[
  1 - 4\alpha \sin^2\left( \frac{2^m\pi}{2^{k+1}}\right) =1
\]
for $m > k$.

For $m = k$, we have
$\displaystyle  \sin^2\left( \frac{2^m\pi}{2^{k+1}} \right)
= \sin^2\left( \frac{\pi}{2}\right) = 1$.  Thus
\[
-3 \leq  1 - 4 \alpha = 1 - 4\alpha \sin^2\left(
    \frac{2^m\pi}{2^{k+1}}\right) < -1
\]
for $m = k$.

For $m = k - 1$, we have
$\displaystyle \sin^2\left(\frac{2^m\pi}{2^{k+1}} \right)
= \sin^2\left(\frac{\pi}{4}\right) = \frac{1}{2}$.  Thus
\[
  -1 \leq 1 - 2\alpha = 1 - 4\alpha \sin^2\left(\frac{2^m\pi}{2^{k+1}}
  \right) <  0
\]
for $m = k -1$.

For $m < k - 1$, we have
$\displaystyle \sin^2\left(\frac{\pi}{2^{k+1-m}} \right) \leq \frac{1}{4}$
because
$\displaystyle 0 \leq \frac{2^m\pi}{2^{k+1}} = \frac{\pi}{2^{k+1-m}}
\leq \frac{\pi}{8}$
for $k-m +1 > 2$.  Thus
\[
  0 \leq 1 - \alpha \leq 1 - 4\alpha \sin^2\left(\frac{2^m\pi}{2^{k+1}}
  \right) <  1
\]
for $m < k-1$.

We have that
\begin{align*}
\left|w_{i,j}\right|
&\geq |\beta_k|\, \left|1 - 4\alpha \right|^{2^{2k} S t_j/T}
- \sum_{m=0}^{k-1} |\beta_m|
\left| \cos\left( \frac{ 2^m i \pi}{2^k}\right) \right|
\, \left|1 - 4\alpha \sin^2\left( \frac{2^m\pi}{2^{k+1}} \right)
\right|^{2^{2k} S t_j/T}
- \sum_{m=k+1}^\infty |\beta_m| \\
&\geq |\beta_k|\, \left|1 - 4\alpha \right|^{2^{2k} S t_j/T}
- \sum_{m=0}^{k-1} |\beta_m| - \sum_{m=k+1}^\infty |\beta_m|
\geq |\beta_k|\, \left|1 - 4\alpha \right|^{2^{2k} S t_j/T}
- \sum_{m=0}^\infty |\beta_m| \ .
\end{align*}

Let us now be a little more specific and assume for instance that
$\beta_m = e^{-2^m} >0$ \footnote{Any sequence
$\displaystyle \{\beta_m\}_{m=0}^\infty$ that preserves the convergence of
$\displaystyle \sum_{m=1}^\infty|\beta_m|$ and
$\displaystyle \sum_{m=1}^\infty\beta_m^2$, and such that
$|\beta_k| e^{2^{2k} S t_j/T} \to \infty$ as $k \to \infty$ can be
used.}.  We have that
$\displaystyle \sum_{m=1}^\infty|\beta_m|$ and
$\displaystyle \sum_{m=1}^\infty\beta_m^2$ converge as required.

Hence
\[
\left|w_{i,j}\right|
\geq  e^{-2^k} \left|1 - 4\alpha \right|^{2^{2k} S t_j/T}
- \sum_{m=0}^\infty \beta_m
= e^{-2^k + 2^{2k} S t_j \ln|1-4\alpha|/T} - g(0) 
\geq e^{-2^k + 2^{2k} S t_j/T} - g(0) 
\to \infty
\]
as $k\to \infty$ because $|1 - 4\alpha|>1$.  Thus $w_{i,j}$ cannot
approach $u(x_i,y_j)$ as $\min \{N,M\} \to \infty$.
\end{egg}

\subsection{Crank-Nicolson Scheme}

We study the $\ell^2$ convergence according to
Definition~\ref{ell2ConvDefN2} of Algorithm~\ref{fdm_sch11S} which is
used to approximate the solution of the heat equation with forcing.

As was mentioned in Remark~\ref{rmk_consist_defN2}, we will get
$\ell^2$ consistency according to Definition~\ref{fdm_consist_defN2}
if we prove consistency according to Definition~\ref{fdm_consist_def}.
This is what we now do.

\begin{prop}
The Crank-Nicolson scheme in Algorithm~\ref{fdm_sch11S} is consistent.
\end{prop}

\begin{proof}
Using the notation introduced in Section~\ref{fdm_CCS}, we have that
\[
P\left(u(x,t), \pdydx{u}{x}(x,t), \pdydx{u}{y}(x,t),
  \pdydxn{u}{x}{2}(x,t), \ldots \right)
= \pdydx{u}{t}(x,t) - c^2 \pdydxn{u}{x}{2}(x,t)
\]
and
\begin{align*}
P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right)
&= \frac{w_{i,j+1} - w_{i,j}}{\dtx{t}} \\
& \qquad - c^2 \left( \frac{w_{i+1,j} - 2w_{i,j} + w_{i-1,j}}{(\dtx{x})^2}
+ \frac{w_{i+1,j+1} - 2w_{i,j+1} + w_{i-1,j+1}}{(\dtx{x})^2} \right)
\end{align*}
for the Crank-Nicolson scheme.  For this scheme, the local truncation is
\begin{align*}
\tau_{i,j}(\dtx{x},\dtx{t},q)
&= P_\Delta\left(q(x_i,t_j), q(x_i,t_{j+1}), q(x_{i+1},t_j), \ldots\right) \\
&\qquad -
\frac{1}{2} \left(
P\left(q(x_i,t_j), \pdydx{q}{x}(x_i,t_j), \pdydx{q}{y}(x_i,t_j),
\pdydxn{q}{x}{2}(x_i,t_j), \ldots \right) \right. \\
& \left.
\qquad\qquad + P\left(q(x_i,t_{j+1}), \pdydx{q}{x}(x_i,t_{j+1}),
\pdydx{q}{y}(x_i,t_{j+1}), \pdydxn{q}{x}{2}(x_i,t_{j+1}), \ldots \right)
\right) \ .
\end{align*}
To find the local truncation error of the Crank-Nicolson scheme, we
note that
\begin{align*}
&\frac{q(x_i,t_{j+1})-q(x_i,t_j)}{\dtx{t}} -
\frac{1}{2} \left( \pdydx{q}{t}(x_i,t_j) + \pdydx{q}{t}(x_i,t_{j+1}) \right) \\
&\ = \frac{1}{2}  \left( \frac{q(x_i,t_{j+1})-q(x_i,t_j)}{\dtx{t}} -
\pdydx{q}{t}(x_i,t_j) \right)
+ \frac{1}{2} \left( \frac{q(x_i,t_{j+1})-q(x_i,t_j)}{\dtx{t}}
- \pdydx{q}{t}(x_i,t_{j+1}) \right) \\
& \ = \frac{1}{2} \left( \frac{1}{2}\, \pdydxn{q}{t}{2}(x_i,t_j) \dtx{t}
+ \frac{1}{6}\, \pdydxn{q}{t}{3}(x_i,\xi_{i,j}) (\dtx{t})^2 \right)
+\frac{1}{2} \left( - \frac{1}{2}\, \pdydxn{q}{t}{2}(x_i,t_{j+1}) \dtx{t}
+ \frac{1}{6}\, \pdydxn{q}{t}{3}(x_i,\tilde{\xi}_{i,j}) (\dtx{t})^2 \right) \\
& \ = \frac{1}{4} \left( \pdydxn{q}{t}{2}(x_i,t_j) \dtx{t}
- \pdydxn{q}{t}{2}(x_i,t_{j+1}) \dtx{t} \right)
+ \frac{1}{12}\, \pdydxn{q}{t}{3}(x_i,\xi_{i,j}) (\dtx{t})^2
+ \frac{1}{12}\, \pdydxn{q}{t}{3}(x_i,\tilde{\xi}_{i,j}) (\dtx{t})^2 \\
&\ = -\frac{1}{4}\, \pdydxn{q}{t}{3}(x_i,\breve{\xi}_j) (\dtx{t})^2
+ \frac{1}{12}\, \pdydxn{q}{t}{3}(x_i,\xi_{i,j}) (\dtx{t})^2
+ \frac{1}{12}\, \pdydxn{q}{t}{3}(x_i,\tilde{\xi}_{i,j}) (\dtx{t})^2 \\
&\ = \left( -\frac{1}{4}\, \pdydxn{q}{t}{3}(x_i,\breve{\xi}_j)
+ \frac{1}{12}\, \pdydxn{q}{t}{3}(x_i,\xi_{i,j})
+ \frac{1}{12}\, \pdydxn{q}{t}{3}(x_i,\tilde{\xi}_{i,j}) \right) (\dtx{t})^2
\end{align*}
for some $\xi_{i,j}, \tilde{\xi}_{i,j},\breve{\xi}_{i,j} \in ]t_i,t_{i+1}[$, and
\begin{align*}
& \frac{1}{2} \, \left( \frac{q(x_{i+1},t_j) - 2 q(x_i,t_j) +
q(x_{i-1},t_j)}{(\dtx{x})^2}
+ \frac{q(x_{i+1},t_{j+1}) - 2q(x_i,t_{j+1}) + q(x_{i-1},t_{j+1})}
  {(\dtx{x})^2} \right) \\
&\qquad - \frac{1}{2} \left( \pdydxn{q}{x}{2}(x_i,t_j) +
\pdydxn{q}{x}{2}(x_i,t_{j+1}) \right) \\
&\quad = \frac{1}{2} \, \left( \frac{q(x_{i+1},t_j) - 2 q(x_i,t_j) +
q(x_{i-1},t_j)}{(\dtx{x})^2} - \pdydxn{q}{x}{2}(x_i,t_j) \right) \\
& \qquad + \frac{1}{2}\, \left(
\frac{q(x_{i+1},t_{j+1}) - 2q(x_i,t_{j+1}) + q(x_{i-1},t_{j+1})}
{(\dtx{x})^2} - \pdydxn{q}{x}{2}(x_i,t_{j+1}) \right) \\
&\quad = \frac{1}{2} \, \left(
\frac{1}{4!} \left( \pdydxn{q}{x}{4}\left(\zeta_{i,j},t_j\right)
+ \pdydxn{q}{x}{4}\left(\eta_{i,j},t_j\right) \right) (\dtx{x})^2 \right)\\
&\qquad + \frac{1}{2} \, \left(
\frac{1}{4!} \left( \pdydxn{q}{x}{4}\left(\nu_{i,j},t_{j+1}\right)
+ \pdydxn{q}{x}{4}\left(\mu_{i,j},t_{j+1}\right)\right) (\dtx{x})^2 \right) \\
&\quad = \frac{1}{48} \left( \pdydxn{q}{x}{4}\left(\zeta_{i,j},t_j\right)
+ \pdydxn{q}{x}{4}\left(\eta_{i,j},t_j\right)
+ \pdydxn{q}{x}{4}\left(\nu_{i,j},t_{j+1}\right)
+ \pdydxn{q}{x}{4}\left(\mu_{i,j},t_{j+1}\right)\right) (\dtx{x})^2
\end{align*}
for $\zeta_{i,j}, \eta_{i,j},\nu_{i,j},\mu_{i,j} \in ]x_{i-1},x_{i+1}[$.
Thus, the local truncation error of the finite difference equation
(\ref{fdm_sch11}) is
\begin{align*}
\tau_{i,j}(\dtx{x},\dtx{t},q)
&= P_\Delta\left(q(x_i,t_j), q(x_i,t_{j+1}), q(x_{i+1},t_j), \ldots\right) \\
&\quad -
\frac{1}{2} \left(
P\left(q(x_i,t_j), \pdydx{q}{x}(x_i,t_j), \pdydx{q}{y}(x_i,t_j),
\pdydxn{q}{x}{2}(x_i,t_j), \ldots \right) \right. \\
& \left.
\quad\qquad + P\left(q(x_i,t_{j+1}), \pdydx{q}{x}(x_i,t_{j+1}),
\pdydx{q}{y}(x_i,t_{j+1}), \pdydxn{q}{x}{2}(x_i,t_{j+1}), \ldots \right)
\right) \\
&= \left( -\frac{1}{4}\, \pdydxn{q}{t}{3}(x_i,\breve{\xi}_j)
+ \frac{1}{12}\, \pdydxn{q}{t}{3}(x_i,\xi_{i,j})
+ \frac{1}{12}\, \pdydxn{q}{t}{3}(x_i,\tilde{\xi}_{i,j}) \right) (\dtx{t})^2 \\
&\quad
- \frac{c^2}{48} \left( \pdydxn{q}{x}{4}\left(\zeta_{i,j},t_j\right)
+ \pdydxn{q}{x}{4}\left(\eta_{i,j},t_j\right)
+ \pdydxn{q}{x}{4}\left(\nu_{i,j},t_{j+1}\right)
+ \pdydxn{q}{x}{4}\left(\mu_{i,j},t_{j+1}\right)\right) (\dtx{x})^2
\end{align*}
for some $\xi_{i,j}, \tilde{\xi}_{i,j},\breve{\xi}_{i,j} \in ]t_i,t_{i+1}[$ and
$\zeta_{i,j}, \eta_{i,j},\nu_{i,j},\mu_{i,j} \in ]x_{i-1},x_{i+1}[$.
If the partial derivatives of order up to four of $q$ are continuous on the
compact set $R$ defined in (\ref{fdm_sch1_dom}), then
there exists $H$ such that
\[
\max_{(x,t)\in R}\, \left|\pdydxn{q}{t}{3}\left(x,t\right) \right| \leq H
\quad \text{and} \quad
\max_{(x,t)\in R}\, \left|\pdydxn{q}{x}{4}\left(x,t\right) \right| \leq H \ .
\]
Hence,
\begin{equation} \label{fdm_sch11STrunc}
\left| \tau_{i,j}(\dtx{x},\dtx{t},q) \right | \leq
\frac{5H}{12} (\dtx{t})^2 + \frac{c^2 H}{12} (\dtx{x})^2
\end{equation}
for $i$ and $j$.  We conclude that
\begin{equation} \label{sch11SCons}
\max \left\{ \left|\tau_{i,j}(\dtx{x},\dtx{t},q)\right| :
0<i<N \ \text{and} \ 0\leq j < M
\right\} \rightarrow 0 \quad \text{as} \quad \min \{N, M\} \rightarrow \infty
\end{equation}
since $\dtx{x} = L/N$ and $\dtx{t} = T/M$ converge to $0$ as $N$ and
$M$ converge to infinity.

Since $\displaystyle B\left(u(x,y), \ldots \right) = u(x,y)$,
$\displaystyle B_\Delta(w_{i,j},\ldots) = w_{i,j}$ and $w_{i,j} = u(x_i,t_j)$
for $(i,j)$ such that $(x_i,t_j) \in \partial R_\Delta$, we get from 
(\ref{sch11SCons}) that Definition~\ref{fdm_consist_def} is satisfied.
\end{proof}

A direct proof that the Crank-Nicolson scheme satisfies the stability
condition in Definition~\ref{fdmStableDefNo1} is not simple.
The reader is asked to proof a limited version of this stability in
Question~\ref{fdmQ1} at the end of this chapter.

We have proved in Example~\ref{CNell2StableNo2} that the
Crank-Nicolson scheme is $\ell^2$-stable according to
Definition~\ref{fdmStableDefNo2}.  Hence, it follows from
Theorem~\ref{ell2StabConstConv}, that the Crank-Nicolson scheme is
$\ell^2$-convergent according to Definition~\ref{ell2ConvDefN1}.

We could stop here but, since we want to demonstrate how to use
several definition of stability, we will prove that the Crank-Nicolson
scheme is $\ell^2$-stable according to Definition~\ref{fdmStableDefNo4}.

\begin{prop}
The Crank-Nicolson scheme in Algorithm~\ref{fdm_sch11S} is
$\ell^2$-stable without constraints on $\dtx{t}$ and $\dtx{x}$.
\end{prop}

\begin{proof}
As we have seen in Section~\ref{Moreell2Stab}, the finite difference
scheme given by Algorithm~\ref{fdm_sch1S} can be expressed as
$\VEC{U}_{j+1} = Q_\beta\VEC{U}_j + \VEC{B}_j$ for $j\geq 0$, where
$Q_\beta = -J^{-1}K$ for $K$ given in (\ref{fdm_K}) and $J$ given in
(\ref{fdm_J}), and 
\[
\VEC{B}_j = J^{-1} \begin{pmatrix}
\alpha \left( w_{0,j+1} + w_{0,j} \right)
+ \big( f(x_1,t_j) + f(x_1,t_{j+1}) \big)\dtx{t}/2 \\
\big( f(x_2,t_j) + f(x_2,t_{j+1}) \big)\dtx{t}/2 \\
\vdots \\
\big( f(x_{N-2},t_j) + f(x_{N-2},t_{j+1}) \big)\dtx{t}/2 \\
\alpha \left( w_{N,j+1} + w_{N,j} \right)
+ \big( f(x_{N-1},t_j)  + f(x_{N-1},t_{j+1}) \big)\dtx{t}/2
\end{pmatrix} \ .
\]
The matrix $K$ can be written as $K = \Id + \alpha A$, where
\[
A = \begin{pmatrix}
-2 & 1 & 0 & 0 & 0 & \ldots & 0 & 0 \\
1 & -2 & 1 & 0 & 0 & \ldots & 0 & 0  \\
0 & 1 & -2 & 1 & 0 & \ldots & 0 & 0 \\
0 & 0 & 1 & -2 & 1 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & \cdots & 1 & -2
\end{pmatrix}
\]
is a \nm{(N-1)}{(N-1)} matrix.  It follows from
Proposition~\ref{fdm_eigR} that the eigenvalues of $A$ are
\[
\lambda_k = -2 + 2 \cos\left(k\,\pi/N\right)
= -4 \sin^2\left(k \,\pi/(2N)\right)
\]
for $0 < k < N$.  Thus, the eigenvalues of $K$ are
\[
  1+\alpha \lambda_k = 1 - 4 \alpha \sin^2\left(k\,\pi/(2N)\right)
\]
for $0 < k < N$.  Proceeding as we did for $K$, we find that the
eigenvalues of $J$ are
$\displaystyle 1 + 4 \alpha \sin^2\left(k\,\pi/(2N)\right)$
for $0 < k < N$.

It follows from Proposition~\ref{fdm_eigR} that the eigenvectors
of $J$ associated to the eigenvalue
$\displaystyle 1 + 4 \alpha \sin^2\left(k\,\pi/(2N)\right)$ are
also the eigenvectors of $K$ associated to the eigenvalue
$\displaystyle 1 - 4 \alpha \sin^2\left(k\,\pi/(2N)\right)$.
Thus the eigenvalues of $J^{-1} K$ are
\[
\frac{1 - 4 \alpha \sin^2\left(k\,\pi/(2N)\right)}
{1 + 4 \alpha \sin^2\left(k\,\pi/(2N)\right)}
\]
for $0 < k < N$.  These values are all smaller than one in absolute
value for every $\alpha>0$.  We have shown that $\|Q_\beta\|_N < 1$ for all
$N$.
\end{proof}

Since we have not stated a theorem equivalent to
Theorem~\ref{StabConstConv} for $\ell^2$-convergence
according to Definition~\ref{ell2ConvDefN2}, though one exists
\footnote{The proof of Proposition~\ref{CKell2NConv} can be slightly
modified to prove this result for a large class of finite difference
schemes.  The requirement is that the linear operator in front of
$\VEC{\tau}_j$ be uniformly bounded for all $N$ as it is the case
for $J^{-1}$ in the proof of Proposition~\ref{CKell2NConv}}, we
will prove the following proposition.

\begin{prop} \label{CKell2NConv}
The Crank-Nicolson scheme given in Algorithm~\ref{fdm_sch11S}
is $\ell^2$-convergent according to Definition~\ref{ell2ConvDefN2}
without any constrain on $\dtx{x}$ and $\dtx{t}$.
\end{prop}

\begin{proof}
Let $u$ be the solution of the partial differential equation
\[
P\left(u(x,t), \pdydx{u}{x}(x,t), \pdydx{u}{t}(x,t), \ldots\right) =
\pdydx{u}{t}(x,t) - c^2 \pdydxn{u}{x}{2}(x,t) = f(x,t)
\]
on $R=[0,L]\times [0,T]$ with $u(x,0) = g(x)$ for $0\leq x \leq L$, and
$u(0,t) = h_0(t)$ and $u(L,t) = h_L(t)$ for $0\leq t \leq T$.

Suppose that the scheme $\VEC{w}_{j+1} = Q\VEC{w}_j + \VEC{B}_j$ is
the matrix representation of the Crank-Nicolson scheme given in
Algorithm~\ref{fdm_sch11S}.  We have that $Q = -J^{-1}K$ for $K$ given
in (\ref{fdm_K}) and $J$ given in (\ref{fdm_J}), and 
\[
\VEC{B}_j = J^{-1} \begin{pmatrix}
\alpha (w_{0,j+1} + w_{0,j})
+ \big( f(x_1,t_j) + f(x_1,t_{j+1})\big)\dtx{t}/2 \\
\big( f(x_2,t_j)  + f(x_2,t_{j+1})\big)\dtx{t}/2 \\
\vdots \\
\big( f(x_{N-2},t_j) + f(x_{N-2},t_{j+1})\big)\dtx{t}/2 \\
\alpha (w_{N,j+1} + w_{N,j})
+ \big( f(x_{N-1},t_j) + f(x_{N-1},t_{j+1})\big)\dtx{t}/2
\end{pmatrix}
\]
for $j\geq 0$.

In vector form, we get from
\begin{align*}
\tau_{i,j}(\dtx{x},\dtx{t},u)
&=  P_\Delta\left(u(x_i,t_j), q(u_i,t_{j+1}),
u(x_{i+1},t_j), \ldots\right) \\ 
&\quad - \frac{1}{2} \left(
P\left(u(x_i,t_j), \pdydx{u}{x}(x_i,t_j), \pdydx{u}{y}(x_i,t_j),
\pdydxn{u}{x}{2}(x_i,t_j), \ldots \right) \right. \\
&\qquad \left. - P\left(u(x_i,t_{j+1}), \pdydx{u}{x}(x_i,t_{j+1}),
\pdydx{u}{y}(x_i,t_{j+1}), \pdydxn{u}{x}{2}(x_i,t_{j+1}), \ldots \right)
\right)
\end{align*}
that
$\VEC{u}_{j+1} = Q \VEC{u}_j + \VEC{b}_j
+ \dtx{t}\, J^{-1} \VEC{\tau}_j(\dtx{x},\dtx{t},u)$, where
\[
\VEC{u}_j = \begin{pmatrix}
u(x_1,t_j) \\ u(x_2,t_j) \\ \vdots \\ u(x_{N-1},t_j)
\end{pmatrix}
\ , \quad 
\VEC{\tau}_j(\dtx{x},\dtx{t},u)
= \begin{pmatrix}
\tau_{1,j}(\dtx{x},\dtx{t},u) \\ \tau_{2,j}(\dtx{x},\dtx{t},u) \\
\vdots \\ \tau_{N-1,j}(\dtx{x},\dtx{t},u) 
\end{pmatrix}
\]
and
\[
\VEC{b}_j = J^{-1} \begin{pmatrix}
\alpha (u(x_0,t_{j+1}) + u(x_0,t_j))
+ \big( f(x_1,t_j) + f(x_1,t_{j+1})\big)\dtx{t}/2 \\
\big( f(x_2,t_j)  + f(x_2,t_{j+1})\big)\dtx{t}/2 \\
\vdots \\
\big( f(x_{N-2},t_j) + f(x_{N-2},t_{j+1})\big)\dtx{t}/2 \\
\alpha ( u(x_N,t_{j+1}) + u(x_N,t_j))
+ \big( f(x_{N-1},t_j) + f(x_{N-1},t_{j+1})\big)\dtx{t}/2
\end{pmatrix}
\]
for $j\geq 0$.  Since we assume that $w_{0,j} = u(0,t_j)$ and
$w_{N,j} = u(L,t_j)$ for all $j$, we have that $\VEC{B}_j = \VEC{b}_j$
for all $j$.

We have that $\VEC{w}_{j+1} = Q\VEC{w}_j + \VEC{B}_j$ satisfies
Definition~\ref{fdmStableDefNo2} with $C=1$ because $\|Q\|_N < 1$ as
we have shown in the proof of the previous proposition.  Moreover, we
have also shown that
\[
\|J^{-1}\|_N = \rho(J^{-1}) = \max_{0<k<N}
\{\, 1/(1 + 4 \alpha \sin^2\left(k\,\pi/(2N)\right)) \, \} < 1 \ .
\]

Since
\[
  \VEC{w}_{j+1} - \VEC{u}_{j+1}
=  \left( Q\VEC{w}_j + \VEC{B}_j \right)
- \left( Q \VEC{u}_j + \VEC{b}_j + \dtx{t}\,J^{-1} \VEC{\tau}_j \right)
=  Q\left( \VEC{w}_j - \VEC{u}_j\right) - \dtx{t}\, J^{-1} \VEC{\tau}_j
\]
for $0\leq j < M$, we get by induction that
\[
\VEC{w}_j - \VEC{u}_j
= Q^j\left( \VEC{w}_0 - \VEC{u}_0\right) -
\dtx{t}\, \sum_{s=0}^{j-1} Q^s J^{-1} \VEC{\tau}_{j-1-s}
\]
for $0 < j \leq M$.  Hence
\begin{align}
\| \VEC{w}_j - \VEC{u}_j \|_N
&\leq \|Q^j\|_N\,\|\VEC{w}_0 - \VEC{u}_0\|_N
+ \dtx{t}\, \|J^{-1}\|_N \, \sum_{k=0}^{j-1} \|Q^s\|_N\, \|\VEC{\tau}_{j-1-s}\|_N
\nonumber \\
& \leq \|\VEC{w}_0 - \VEC{u}_0\|_N
  + j \dtx{t} \, L^{1/2} \,\tau(\dtx{x},\dtx{t},u) \nonumber \\
&\leq \|\VEC{w}_0 - \VEC{u}_0\|_N
  + T \, L^{1/2} \,\tau(\dtx{x},\dtx{t},u) \label{NewSCC}
\end{align}
for $0 < j \leq M$, where
\[
\tau(\dtx{x},\dtx{t},u) \equiv
\max_{\substack{(i,j) \text{ such that}\\(x_i,y_j) \in R^o_\Delta}}
|\tau_{i,j}(\dtx{x},\dtx{t},u)|
\leq \frac{5H}{12} (\dtx{t})^2 + \frac{c^2 H}{12} (\dtx{x})^2
\]
because of (\ref{fdm_sch11STrunc}).  It is here that the choice of the
norm $\|\cdot\|_N$ is important because we have
\begin{align*}
\|\VEC{\tau}_j\|_N 
&= \left(\sum_{i=1}^{N-1} \left(\tau_{i,j}(\dtx{x},\dtx{t},u)\right)^2\,
\dtx{x}\right)^{1/2}
\leq \left(\sum_{i=1}^{N-1} \tau^2(\dtx{x},\dtx{t},u)\,\dtx{x}\right)^{1/2} \\
&= \tau(\dtx{x},\dtx{t},u) \,
((N-1)(L/N))^{1/2} \leq L^{1/2}\, \tau(\dtx{x},\dtx{t},u) \ .
\end{align*}

Since the Crank-Nicolson scheme is consistent, we get from
(\ref{NewSCC}) that
\[
\max_{0<j\leq M} \| \VEC{w}_j - \VEC{u}_j \|_N
\leq T \, L^{1/2} \,\tau(\dtx{x},\dtx{t},u) \to 0
\]
as $\min\{N,M\} \to \infty$ since we assume that
$w_{i,j} = u(x_i,t_j)$ for all $(i,j)$ such that
$(x_i,t_j) \in \partial R_\Delta$.  This implies that
$\VEC{w}_0 = \VEC{u}_0$.
\end{proof}

Crank-Nicolson is a really good scheme because there is no constrains
on $\dtx{x}$ and $\dtx{t}$, and the convergence is quadratic; namely,
order two in $\dtx{x}$ and $\dtx{t}$.

\section{Dirichlet Equation} \label{CCSDirichlet}

We could study the consistence, stability and convergence of
the finite difference scheme in Algorithm~\ref{fdm_sch2S} as we did
for the previous finite difference schemes for the heat equation
with forcing.  However, we will not do so.  There is a more elegant
approach to study the consistence, stability and convergence of
the finite difference schemes to numerically solve elliptic equations.

We first consider the Dirichlet problem (\ref{DirEqu}) with  
$f(x,y) = 0$ for all $(x,y) \in R$.  In that particular case, it is
called the Laplace equation.  Our first result will be a
maximum principle \footnote{It is a well know result that the solution
of the Laplace equation reach it maximum on the boundary of the
domain $R$.  The solution of $Q_\Delta(w) = 0$ has the same property.}
for the following finite difference scheme used to numerically solve
the Laplace equation.
\[
Q_\Delta(w_{i,j}) \equiv \frac{w_{i,j+1} - 2w_{i,j} + w_{i,j-1}}{(\dtx{x})^2}
+ \frac{w_{i+1,j} - 2w_{i,j} + w_{i-1,j}}{(\dtx{y})^2} = 0
\]
for $0 < i < N$ and $0 < j < M$, where
$w_{i,0} = g(x_i,0)$ and $w_{i,M} = g(x_i,b)$ for $0\leq i \leq N$, and
$w_{0,j} = g(0,y_j)$ and $w_{N,j} = g(a,y_j)$ for $0\leq j \leq M$.

As we did in Section~\ref{fdm_CCS}, we have
\[
  R = \left\{ (x,y) : 0\leq x \leq a \ \text{and} \ 0 \leq y \leq b \right\}
\]
and
\[
R_\Delta = \{ (x_i,y_j) : x_i = i \dtx{x} \ \text{for} \ 0 \leq i \leq N,
\ \text{and} \ y_j = j \dtx{y} \ \text{for} \ 1 \leq j \leq M \} \ ,
\]
where $\dtx{x} = a/N$ and $\dtx{y}=b/M$.  We also define
\begin{align*}
\partial R_\Delta = \{ (x_i,y) : x_i &= i \dtx{x} \ \text{for}
\ 0 \leq i \leq N, \ \text{and} \ y = y_0 = 0 \ \text{or}\ y = y_M = b \} \\
& \quad  \cup
\{ (x,y_j) : y_j = j \dtx{y} \ \text{for} \ 0 \leq i \leq M,
\ \text{and} \ x = x_0 = 0 \ \text{or}\ x = x_N = a \}
\end{align*}
and $R^o_\Delta = R_\Delta \setminus \partial R_\Delta$.

\begin{theorem} \label{LaplaceMaxPrinc}
Suppose that $v_{i,j} = v(x_i,y_j)$ for all $i$ and $j$, where
$v:R_\Delta \to \RR$.  If $Q_\Delta(v_{i,j}) \geq 0$ for all $(i,j)$ with
$(x_i,y_j)\in R^o_\Delta$, then
\[
  \max_{(x_i,y_j) \in R^o_\Delta} v_{i,j}
  \leq \max_{(x_i,y_j) \in \partial R_\Delta} v_{i,j} \ .
\]  
\end{theorem}

\begin{proof}
The proof is by contradiction.  Suppose that there exist
$(x_{i_0},y_{j_0}) \in R^o_\Delta$, (i.e.\ $0 < i_0 < M$
and $0 < j_0 < N$) such that $v_{i_0,j_0} \geq v_{i,j}$ for all
$(i,j)$ with $(x_i,y_j) \in R^o_\Delta$, and
$v_{i_0,j_0} > v_{i,j}$ for all $(i,j)$ with
$(x_i,y_j) \in \partial R_\Delta$.  We get from
\[
Q_\Delta\left(v_{i_0,j_0}\right)
\equiv \frac{v_{i_0,j_0+1} - 2v_{i_0,j_0} + v_{i_0,j_0-1}}{(\dtx{x})^2}
+ \frac{v_{i_0+1,j_0} - 2v_{i_0,j_0} + v_{i_0-1,j_0}}{(\dtx{y})^2}
\geq 0
\]
that
\[
2\left( \frac{1}{(\dtx{x})^2} + \frac{1}{(\dtx{y})^2} \right) v_{i_0,j_0} \\
\leq \frac{1}{(\dtx{x})^2} \left( v_{i_0,j_0+1} + v_{i_0,j_0-1} \right)
\frac{1}{(\dtx{x})^2} \left( v_{i_0+1,j_0} + v_{i_0-1,j_0} \right) \ .
\]
Since $v_{i,j} \leq v_{i_0,j_0}$ for all $(i,j)$, the only way to satisfy this
inequality is if
$v_{i_0,j_0+1}= v_{i_0,j_0-1} = v_{i_0+1,j_0} = v_{i_0-1,j_0} = v_{i_0,j_0}$.

We can then repeat the same procedure with $v_{i_0,j_0+1}$,
$v_{i_0,j_0-1}$, $v_{i_0+1,j_0}$ and $v_{i_0-1,j_0}$.  Moving that way
horizontally and vertically, we find that $v_{i,j} = v_{i_0,j_0}$ for all
$(i,j)$ \footnote{To be exact, we are missing the four corner points
of $R_\Delta$}, even those for $(x_i,y_j) \in \partial R_\Delta$.  This
contradicts our assumption that
$v_{i_0,j_0} > v_{i,j}$ for all $(i,j)$ with
$(x_i,y_j) \in \partial R_\Delta$.
\end{proof}

Using an argument like the one in the proof of the previous
theorem or simply applying the previous theorem to
$-v_{i,j}$ instead of $v_{i,j}$, we get the following result.

\begin{theorem} \label{LaplaceMinPrinc}
Suppose that $v_{i,j} = v(x_i,y_j)$ for all $i$ and $j$, where
$v:R_\Delta \to \RR$.  If $Q_\Delta(v_{i,j}) \leq 0$ for all $(i,j)$ with
$(x_i,y_j)\in R^o_\Delta$, then
\[
  \min_{(x_i,y_j) \in R^o_\Delta} v_{i,j}
  \geq \min_{(x_i,y_j) \in \partial R_\Delta} v_{i,j} \ .
\]  
\end{theorem}

It follows from the previous two theorems that the finite
difference scheme in Algorithm~\ref{fdm_sch2S} has a unique solution.
Suppose that
$\{ w_{i,j}^{[k]} : 0\leq i \leq N \ \text{and} \ 0 \leq j \leq M\}$
for $k=1$ and $2$ are two solutions of the finite
difference scheme in Algorithm~\ref{fdm_sch2S}.  We then have that
$\{ w_{i,j} : 0\leq i \leq N \ \text{and} \ 0 \leq j \leq M\}$
with $w_{i,j} = w_{i,j}^{[1]} - w_{i,j}^{[2]}$ for all $i$ and $j$ is
a solution of $\displaystyle \Delta_\Delta w_{i,j} = 0$ for all $(i,j)$
such that $(x_i,y_j) \in R^o_\Delta$, and $w_{i,j} = 0$ for all
$(i,j)$ such that $(x_i,y_j) \in \partial R_\Delta$.  It follows from
Theorems~\ref{LaplaceMaxPrinc} and \ref{LaplaceMinPrinc} that
\[
0 = \min_{(x_i,y_j) \in \partial R_\Delta} w_{i,j}
\leq \min_{(x_i,y_j) \in R^o_\Delta} w_{i,j}
\leq \max_{(x_i,y_j) \in R^o_\Delta} w_{i,j}
\leq \max_{(x_i,y_j) \in \partial R_\Delta} w_{i,j} = 0 \ .
\]
Thus $w_{i,j}^{[1]} - w_{i,j}^{[2]} = 0$ for $0\leq i \leq N$ and
$0\leq j \leq M$.

\subsection{Algorithm~\ref{fdm_sch2S}} 

\begin{prop}\label{fdm_sch2S_const}
The finite difference scheme in Algorithm~\ref{fdm_sch2S} is consistent.
\end{prop}

\begin{proof}
Using the notation introduced in Section~\ref{fdm_CCS}, we have that
\[
P\left(u(x,y), \pdydx{u}{x}(x,y), \pdydx{u}{y}(x,y),
  \pdydxn{u}{x}{2}(x,y), \ldots \right)
= \pdydxn{u}{x}{2} + \pdydxn{u}{y}{2}
\]
and
\[
P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right)
= \frac{w_{i+1,j} - 2w_{i,j} + w_{i-1,j}}{(\dtx{x})^2}
+ \frac{w_{i,j+1} - 2w_{i,j} + w_{i,j-1}}{(\dtx{y})^2}
\]
for the finite difference scheme in Algorithm~\ref{fdm_sch2S}.

To find the local truncation error of the finite difference scheme in
Algorithm~\ref{fdm_sch2S}, we need to use the formula in
(\ref{fdm_soDD}) twice, for the second other partial derivative of $u$
with respect to $x$ and the second other partial derivative of $u$
with respect to $y$.

Given any sufficiently differentiable function
$q:R \to \RR$, let $q_{i,j}= q(x_i,y_j)$ for all $(x_i,y_j) \in R_\Delta$. 
We have
\[
\pdydxn{q}{x}{2}(x_i,y_j) =
\frac{q_{i+1,j} - 2q_{i,j} + q_{i-1,j}}{(\dtx{x})^2}
- \frac{1}{4!} \left( \pdydxn{q}{x}{4}\left(\zeta_{i,j},y_j\right)
  + \pdydxn{q}{x}{4}\left(\eta_{i,j},y_j\right) \right) (\dtx{x})^2
\]
and
\[
\pdydxn{q}{y}{2}(x_i,y_j) =
\frac{q_{i,j+1} - 2q_{i,j} + q_{i,j-1}}{(\dtx{y})^2}
- \frac{1}{4!} \left( \pdydxn{q}{y}{4}\left(x_i,\mu_{i,j} \right)
+ \pdydxn{q}{y}{4}\left(x_i,\nu_{i,j} \right) \right) (\dtx{y})^2
\]
for $\zeta_{i,j}, \eta_{i,j} \in ]x_{i-1},x_{i+1}[$ and
$\mu_{i,j}, \nu_{i,j} \in ]y_{j-1},y_{j+1}[$.  Hence
\begin{align*}
&\tau_{i,j}(\dtx{x},\dtx{y},q)
= P\left(q(x_i,t_j), \pdydx{q}{x}(x_i,t_j), \pdydx{q}{y}(x_i,t_j),
  \pdydxn{q}{x}{2}(x_i,t_j), \ldots \right) \\
&\qquad\qquad\qquad\qquad
- P_\Delta\left(q(x_i,t_j), q(x_i,y_{j+1}), q(x_{i+1},t_j), \ldots\right) \\
&\qquad
= - \frac{1}{4!} \left( \pdydxn{q}{x}{4}\left(\zeta_{i,j},y_j\right)
+ \pdydxn{q}{x}{4}\left(\eta_{i,j},y_j\right) \right) (\dtx{x})^2
- \frac{1}{4!} \left( \pdydxn{q}{y}{4}\left(x_i,\mu_{i,j} \right)
+ \pdydxn{q}{y}{4}\left(x_i,\nu_{i,j} \right) \right) (\dtx{y})^2
\end{align*}
for $\zeta_{i,j}, \eta_{i,j} \in ]x_{i-1},x_{i+1}[$ and
$\mu_{i,j}, \nu_{i,j} \in ]y_{j-1},y_{j+1}[$.

If the partial derivatives of order up to four of $q$ are continuous on the
compact set $R$, then there exists $H$ such that
\[
\max_{(x,y)\in R}\, \left|\pdydxn{q}{x}{4}\left(x,t\right) \right| \leq
H \quad \text{and} \quad 
\max_{(x,y)\in R}\, \left|\pdydxn{q}{y}{4}\left(x,t\right) \right| \leq
H \ .
\]
Hence,
\begin{equation} \label{fdm_sch2STrunc}
\left| \tau_{i,j}(\dtx{x},\dtx{y},q) \right | \leq
\frac{H}{12} \left( (\dtx{x})^2 + (\dtx{y})^2\right)
\end{equation}
for $i$ and $j$.  We conclude that
\begin{equation} \label{sch2SCons}
\max \left\{ \left|\tau_{i,j}(\dtx{x},\dtx{y},q)\right| :
0<i<N \ \text{and} \ 0 < j < M
\right\} \rightarrow 0 \quad \text{as} \quad \min \{N, M\} \rightarrow \infty
\end{equation}
since $\dtx{x} = L/N$ and $\dtx{t} = T/M$ converge to $0$ as $N$ and
$M$ converge to infinity.

Since $\displaystyle B\left(u(x,y), \ldots \right) = u(x,y)$,
$\displaystyle B_\Delta(w_{i,j},\ldots) = w_{i,j}$ and $w_{i,j} = u(x_i,t_j)$
for $(i,j)$ such that $(x_i,t_j) \in \partial R_\Delta$, we get from 
(\ref{sch2SCons}) that Definition~\ref{fdm_consist_def} is satisfied.
\end{proof}

The stability of the finite difference scheme in
Algorithm~\ref{fdm_sch2S} will be a consequence of the next theorem.

\begin{theorem} \label{LaplaceVdeltaVInequ}
Suppose that $v_{i,j} = v(x_i,y_j)$ for all $i$ and $j$, where
$v:R_\Delta \to \RR$.  Then
\[
\max_{(x_i,y_j) \in R_\Delta} |v_{i,j}|
\leq \max_{(x_i,y_j) \in \partial R_\Delta} |v_{i,j}|
+ \frac{a^2}{2} \max_{(x_i,y_j) \in R^o_\Delta}
\left|Q_\Delta(v_{i,j})\right| \ .
\]
\end{theorem}

\begin{proof}
We need to define some real-valued functions on $R_\Delta$ to prove
this result.  First
\begin{align*}
  h : R_\Delta & \to \RR \\
  (x_i,y_j) & \mapsto x_i^2/2
\end{align*}
We obviously have that $0 \leq h(x_i,y_j) \leq a^2/2$ for all
$(x_i,y_j) \in R_\Delta$.  Moreover, if we let $h_{i,j} = h(x_i,y_j)$
for all $i$ and $j$, we have
\begin{align*}
  Q_\Delta(h_{i,j})
&= \frac{h_{i+1,j} - 2h_{i,j} + h_{i-1,j}}{(\dtx{x})^2}
+ \frac{h_{i,j+1} - 2h_{i,j} + h_{i,j-1}}{(\dtx{y})^2} \\
&= \frac{(x_i+\dtx{x})^2 - 2x_i^2 + (x_i-\dtx{x})^2}{2(\dtx{x})^2}
+ \frac{x_i^2 - 2x_i^2 + x_i^2}{2(\dtx{y})^2}
= \frac{(\dtx{x})^2 + (\dtx{x})^2}{2(\dtx{x})^2}
= 1
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.

let
$\displaystyle
K = \max_{(x_i,y_j) \in R^o_\Delta} \left| Q_\Delta(v_{i,j})\right|$.
We define two additional functions.
\begin{align*}
  g^{\pm} : R_\Delta & \to \RR \\
  (x_i,y_j) & \mapsto \pm v(x_i,y_j) + K h(x_i,y_j) = \pm v_{i,j} + K h_{i,j}
\end{align*}
Let $g_{i,j}^{\pm} = g^{\pm}(x_i,y_j) = \pm v_{i,j} + K h_{i,j}$ for
all $i$ and $j$.  By linearity of the operator $Q_\Delta$, we
have that
\[
Q_\Delta\left(g_{i,j}^{\pm}\right)
= \pm Q_\Delta(v_{i,j}) + K Q_\Delta(h_{i,j})
= \pm Q_\Delta(v_{i,j}) + K \geq 0
\]
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.
Therefore, we may apply Theorem~\ref{LaplaceMaxPrinc} to $g^{\pm}$.  For
$g^+$, we get
\begin{align*}
v_{i,j} & \leq \max_{(x_i,y_j) \in R^o_\Delta} v_{i,j}
\leq \max_{(x_i,y_j) \in R^o_\Delta} g^+_{i,j}
\leq \max_{(x_i,y_j) \in \partial R_\Delta} g^+_{i,j}
\leq \max_{(x_i,y_j) \in \partial R_\Delta} v_{i,j}
+ K \max_{(x_i,y_j) \in \partial R_\Delta} h_{i,j} \\
&\leq \max_{(x_i,y_j) \in \partial R_\Delta} |v_{i,j}| + \frac{K a^2}{2}
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.
For $g^-$, we also get
\begin{align*}
-v_{i,j} & \leq \max_{(x_i,y_j) \in R^o_\Delta} -v_{i,j}
\leq \max_{(x_i,y_j) \in R^o_\Delta} g^-_{i,j}
\leq \max_{(x_i,y_j) \in \partial R_\Delta} g^-_{i,j}
\leq \max_{(x_i,y_j) \in \partial R_\Delta} -v_{i,j}
+ K \max_{(x_i,y_j) \in \partial R_\Delta} h_{i,j} \\
&\leq \max_{(x_i,y_j) \in \partial R_\Delta} |v_{i,j}| + \frac{K a^2}{2}
\end{align*}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.
Thus
\[
|v_{i,j}| \leq \max_{(x_i,y_j) \in \partial R_\Delta} |v_{i,j}| + \frac{K a^2}{2}
\]
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.  This
gives the conclusion of the theorem.
\end{proof}

\begin{prop} \label{fdm_sch2S_Stab}
The finite difference scheme in Algorithm~\ref{fdm_sch2S} is stable
without constraints on $\dtx{x}$ and $\dtx{y}$.
\end{prop}

\begin{proof}
For the finite difference scheme in Algorithm~\ref{fdm_sch2S}, we
have that
$P_\Delta(w_{i,j}, \ldots) = \Delta_\Delta w_{i,j}$ for all $(i,j)$ such
that $(x_i,y_j) \in R^o_\Delta$,  and 
$B_\Delta(w_{i,j}, \ldots) = w_{i,j}$ for all $(i,j)$ such
that $(x_i,y_j) \in \partial R_\Delta$.  It then follows from the
previous theorem that the condition (\ref{stabCondFDM}) for the
definition of stability in Definition~\ref{fdmStableDefNo1} is satisfied
with $C = \max \{ 1, a^2/2\}$.
\end{proof}

The following result is a consequence of Proposition~\ref{fdm_sch2S_const},
Proposition~\ref{fdm_sch2S_Stab} and Theorem~\ref{StabConstConv}.

\begin{prop}
The finite difference scheme in Algorithm~\ref{fdm_sch2S} is
convergent without any constrains on $\dtx{x}$ and $\dtx{t}$.
\end{prop}

We can also prove this proposition using Theorem~\ref{LaplaceVdeltaVInequ}.

\begin{proof}
Suppose that
$\{ w_{i,j} : 0 \leq i \leq N \ \text{and} \ 0 \leq j \leq M \}$
is the solution of $Q_\Delta(w_{i,j}) = f(x_i,y_j)$ for all
$(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$, and
$w_{i,j} = g(x_i,y_j)$ for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.

Suppose that $u$ is the solution of the Dirichlet equation introduced
in Section~\ref{DirEquSection}; namely, $u$ is the solution of
$\Delta u(x,y)=f(x,y)$ for $(x,y) \in R \setminus \partial R$ and
$u(x,y) = g(x,y)$ for $(x,y) \in \partial R$.

We have from
\[
\tau_{i,j}(\dtx{x},\dtx{y},u)
= P\left(u(x_i,y_j) , \pdydx{u}{x}(x_i,y_j), \ldots\right)
- P_\Delta\left(u(x_i,y_j) , u(x_{i+1},y_j), \ldots\right)
\]
that
\begin{equation} \label{fdm_sch2S_equ1}
\begin{split}
&\frac{u(x_{i+1},y_j) - 2u(x_i,y_j) + u(x_{i-1},y_j)}{(\dtx{x})^2}
+ \frac{u(x_i,y_{j+1}) - 2u(x_i,y_j) + u(x_i,y_{j-1})}{(\dtx{y})^2} \\
&\qquad\qquad = f(x_i,y_j) - \tau_{i,j}(\dtx{x},\dtx{y},u)
\end{split}
\end{equation}
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$, where
$\tau_{i,j}(\dtx{x},\dtx{y},u)$ is defined in the proof of
Proposition~\ref{fdm_sch2S_const}.

Let $r_{i,j} = w_{i,j} - u(x_i,t_j)$ for $0\leq i \leq N$ and
$0 \leq j \leq M$.  If we subtract (\ref{fdm_sch2S_equ1}) from
$Q_\Delta(w_{i,j}) = f(x_i,y_j)$, we get
\[
Q_\Delta(r_{i,j}) =
\frac{r_{i+1,j} - 2r_{i,j} + r_{i-1,j}}{(\dtx{x})^2}
+ \frac{r_{i,j+1} - 2r_{i,j} + r_{i,j-1}}{(\dtx{y})^2} =
\tau_{i,j}(\dtx{x},\dtx{y},u) \ .
\]
for all $(i,j)$ such that $(x_i,y_j) \in R^o_\Delta$.

Because $w_{i,j} = u(x_i,y_j) = g(x_i,y_j)$ for $(i,j)$ such that
$(x_i,y_j) \in \partial R_\Delta$, we have that
$r_{i,j} = 0$ for $(i,j)$ such that
$(x_i,y_j) \in \partial R_\Delta$.  Hence, if we apply
Theorem~\ref{LaplaceVdeltaVInequ} to the function
\begin{align*}
  r : R_\Delta & \to \RR \\
  (x_i,y_j) & \mapsto r_{i,j}
\end{align*}
we get
\[
\max_{(x_i,y_j) \in R_\Delta} |r_{i,j}|
\leq \frac{a^2}{2} \max_{(x_i,y_j) \in R^o_\Delta}
\left|Q_\Delta(r_{i,j})\right|
= \frac{a^2}{2}
\max_{(x_i,y_j) \in R^o_\Delta} |\tau_{i,j}(\dtx{x},\dtx{y},u)| \ .
\]
It follows from (\ref{fdm_sch2STrunc}) that
\[
\max_{(x_i,y_j) \in R_\Delta} |w_{i,j}-u(x_i,y_j)|
\leq \frac{a^2H}{24} \left( (\dtx{x})^2 + (\dtx{y})^2\right)
\to 0
\quad \text{as} \quad \min\{N,M\} \to \infty \ .  \qedhere
\]
\end{proof}

\section{Wave Equation}

The finite difference scheme in Algorithm~\ref{fdm_sch3S} was
developed to numerically solve the wave equation
\begin{equation} \label{waveLT1}
  \pdydxn{u}{t}{2} = c^2 \pdydxn{u}{x}{2} \quad , \quad
  0 < x < L \ \text{and} \ 0<t<T
\end{equation}
with the boundary conditions
\begin{equation} \label{waveLT2}
u(0,t)=u(L,t)=0 \quad , \quad 0<t<T \ ,
\end{equation}
and the initial conditions
\begin{equation} \label{waveLT3}
u(x,0) = g(x) \ \text{and} \ \pdydx{u}{t}(x,0) = f(x) \quad , \quad
 0 \leq x \leq L \ .
\end{equation}

As we will show below for the special case of the wave equation
above, finite difference scheme are not ideal to numerically solve
hyperbolic differential equations.  Strict conditions on the step sizes
are required to get convergent finite difference schemes.  We will
address this issue in the next section before studying the stability,
consistency and convergence of the finite difference scheme in
Algorithm~\ref{fdm_sch3S}.

\subsection{The Role of the Domain of Dependence} \label{DomDepHyp}

This section uses some of the basic techniques to solve partial
differential equations.

Let us start with the wave equation on the real line.
\begin{equation}\label{waveRL1}
  \pdydxn{u}{t}{2} = c^2 \pdydxn{u}{x}{2} \quad , \quad
  -\infty < x < \infty \ \text{and} \ t > 0
\end{equation}
with the initial conditions
\begin{equation} \label{waveRL5}
u(x,0) = g(x) \ \text{and} \ \pdydx{u}{t}(x,0) = f(x) \quad , \quad
 -\infty < x < \infty \ .
\end{equation}
If we use the substitution $\xi = x+ ct$ and $\eta = x- ct$, we get
\[
  \pdydx{u}{t} = \pdydx{u}{\xi}\,\pdydx{\xi}{t} +
  \pdydx{u}{\eta}\, \pdydx{\eta}{t} = c \pdydx{u}{\xi} - c \pdydx{u}{\eta}
\]
and
\begin{align}
\pdydxn{u}{t}{2} &= \pdfdx{\left(\pdydx{u}{t}\right)}{t}
= \pdfdx{\left(c \pdydx{u}{\xi} - c \pdydx{u}{\eta}\right)}{\xi}
\,\pdydx{\xi}{t} +
\pdfdx{\left(c \pdydx{u}{\xi} - c \pdydx{u}{\eta}\right)}{\eta}
\,\pdydx{\eta}{t} \nonumber \\
&  = c \left(c \pdydxn{u}{\xi}{2} - c \pdydxnm{u}{\xi}{\eta}{2}{}{}\right)
- c \left(c \pdydxnm{u}{\eta}{\xi}{2}{}{} - c \pdydxn{u}{\eta}{2}\right)
= c^2 \left( \pdydxn{u}{\xi}{2} - 2 \pdydxnm{u}{\eta}{\xi}{2}{}{}
+ \pdydxn{u}{\eta}{2}\right) \ .  \label{waveRL2}
\end{align}
Similarly,
\[
  \pdydx{u}{x} = \pdydx{u}{\xi}\,\pdydx{\xi}{x} +
  \pdydx{u}{\eta}\, \pdydx{\eta}{x} = \pdydx{u}{\xi} + \pdydx{u}{\eta}
\]
and
\begin{equation} \label{waveRL3}
  \pdydxn{u}{x}{2} = \pdfdx{\left(\pdydx{u}{x}\right)}{x}
  = \pdydxn{u}{\xi}{2} + 2 \pdydxnm{u}{\eta}{\xi}{2}{}{}
+ \pdydxn{u}{\eta}{2} \ .
\end{equation}
If we substitute (\ref{waveRL2}) and (\ref{waveRL2}) in
(\ref{waveRL1}), and simplify the result, we get
\begin{equation} \label{waveRL4}
  \pdydxnm{u}{\eta}{\xi}{2}{}{} = 0 \ .
\end{equation}
Integrating this equation with respect to $\xi$ yields
$\displaystyle \pdydx{u}{\eta} = H(\eta)$ for
some function $H:\RR\rightarrow \RR$.  Integrating
$\displaystyle \pdydx{u}{\eta} = H(\eta)$ with
respect to $\eta$ yields
$\displaystyle u(\eta,\xi) = \int H(\eta) \dx{\eta} + G(\xi)$ for some
function $G:\RR\rightarrow \RR$.  If we define
$\displaystyle F(\eta) = \int H(\eta) \dx{\eta}$, we get the solution
\[
u(\eta,\xi) = F(\eta) + G(\xi)
\]
for (\ref{waveRL4}).  In terms of $x$ and $t$, we get the solution
\[
u(x,t) = F(x-ct) + G(x+ct)
\]
for (\ref{waveRL1}).  We now consider the initial conditions in
(\ref{waveRL5}).  We have that $f$ and $g$ satisfy the equations
$f(x) = F(x) + G(x)$ and $g(x) = -c F'(x) + c G'(x)$.
If we assume that $g$ is locally integrable, we may write
\[
\int_0^x g(s)\dx{s} = -c F(x) + c G(x) + c F(0) - c G(0) \ .
\]
We end up with two linearly independent equations for $F$ and $G$.
\begin{align}
F(x) + G(x) &= f(x) \label{waveRL6} \\
F(x) - G(x) &= F(0) - G(0) -\frac{1}{c} \int_0^x g(s)\dx{s} \ . \label{waveRL7}
\end{align}
Adding (\ref{waveRL6}) and (\ref{waveRL7}) and dividing by $2$ yield
\[
F(x) = \frac{1}{2} f(x) - \frac{1}{2c} \int_0^x g(s)\dx{s} +
\frac{1}{2} \left(F(0) - G(0)\right) \ .
\]
Subtracting (\ref{waveRL7}) from (\ref{waveRL6}) and dividing by $2$
yield
\[
G(x) = \frac{1}{2} f(x) + \frac{1}{2c} \int_0^x g(s)\dx{s} -
\frac{1}{2} \left(F(0) - G(0)\right) \ .
\]
We finally get the solution
\begin{equation} \label{waveRL8}
u(x,t) = F(x-ct) + G(x+ct)
= \frac{1}{2} \left(f(x-ct) + f(x+ct)\right)
+ \frac{1}{2c} \int_{x-ct}^{x+ct} g(s)\dx{s}
\end{equation}
for (\ref{waveRL1}) and (\ref{waveRL5}).  The
{\bfseries domain of dependence}\index{Wave Equation!Domain of Dependence}
for $u(\tilde{x},\tilde{t})$ is the set 
$\{ (x,t) :  0 \leq t \leq \tilde{t} \ \text{and}
\ ct + \tilde{x} - c\tilde{t} \leq x \leq -ct + \tilde{x} + c\tilde{t} \}$.
This is the set of all points $(x,t)$ where $f(x-ct)$, $f(x+ct)$ and
$g(s)$ in (\ref{waveRL8}) are evaluated to get the value of 
$u$ at $(\tilde{x},\tilde{t})$.
The domain of dependence for $u(\tilde{x},\tilde{t})$ 
is displayed in figure (\ref{waveFig2}).  The domain of dependence
will play an important role in our analysis of finite difference
schemes used to numerically solve the wave equation and hyperbolic
equations in general. 

\pdfF{finite_diff/wave_fig2}{Domain of dependence for the wave
equation}{Domain of dependence for $u(\tilde{x},\tilde{t})$, the
value of the solution $u$ of the wave equation at
$(\tilde{x},\tilde{t})$.}{waveFig2}

We now solve the wave equation (\ref{waveLT1}) with the boundary
conditions (\ref{waveLT2}) and the initial conditions (\ref{waveLT3}).
To do so, we will use the method of separation of variables.
The presentation will be a little more formal than usual.  However,
all the theoretical details could be filled in by the reader who has some
knowledge of $L^2$-spaces and Fourier series.

If we substitute $u(x,t) = F(x)G(t)$ in (\ref{waveLT1}), we get
\[
F(x)\,\dydxn{G}{t}{2}(t) = c^2 \dydxn{F}{x}{2}(x)\, G(t) \ .
\]
Thus, after dividing both sides by $c^2\,F(x)G(t)$, we get
\[
\frac{1}{c^2\,G(t)} \, \dydxn{G}{t}{2}(t) =
\frac{1}{F(x)}\, \dydxn{F}{x}{2}(x)
\quad , \quad t>0 \ \text{and} \ 0<x<L \ .
\]
Since the right hand side is independent of $t$ and the left hand side
is independent of $x$, we get
\[
\frac{1}{c^2\,G(t)} \, \dydxn{G}{t}{2}(t) = \frac{1}{F(x)}\,
\dydxn{F}{x}{2}(x) = k \quad , \quad t>0 \ \text{and} \ 0<x<L
\]
for some constant $k$.  We end up with two ordinary differential equations.
\begin{equation} \label{waveLT4}
\dydxn{F}{x}{2}(x) -k F(x) = 0 \quad \text{and}
\quad \dydxn{G}{t}{2}(t) - c^2\,k\,G(t)) = 0 \ .
\end{equation}

From $\displaystyle u(0,t) = 0$, we get
$F(0)G(t)=0$.  Since we assume that $G$ is not trivial, we get
$F(0)=0$.  Similarly, from $\displaystyle u(L,t) = 0$, we
get $F(L)G(t)=0$.  Again, since we assume that $G$ is not trivial, we
get $F(L)=0$.  The boundary conditions for 
the first ordinary differential equation in (\ref{waveLT4}) are
$F(0)=F(L)=0$.

We consider the boundary value problem
\begin{equation} \label{waveLT5}
\dydxn{F}{x}{2}(x) -k F(x) = 0 \quad \text{with} \quad F(0)=F(L)=0  \ .
\end{equation}
The form of the general solution of (\ref{waveLT5}) is determined by the
roots of the characteristic equation $\lambda^2-k=0$.

If $k>0$, the roots of the characteristic equation are $\pm \sqrt{k}$.
Since the roots are real, the general solution of the ordinary differential
equation is of the form
\[
F(x) = A e^{\sqrt{k}\,x} + B e^{-\sqrt{k}\,x} \ .
\]
However, $F(0)=0$ implies that $A + B = 0$ and $F(L)=0$ implies
$\displaystyle A e^{L\sqrt{k}} + B e^{-L\sqrt{k}} = 0$.
The only solution for these two equations is $A=B=0$ because
$\displaystyle e^{L\sqrt{k}} - e^{-L\sqrt{k}}
= e^{L\sqrt{k}}\left(1-e^{-2L\sqrt{k}}\right) \neq 0$ for $k\neq 0$.
Therefore, the trivial solution is the only solution
of the boundary value problem (\ref{waveLT5}) for $k>0$.

If $k=0$, the general solution of the ordinary differential equation
is $F(x)=B x+ A$.  However, $F(0)=0$ implies that $A=0$.  Hence $F(L)=0$
implies that $B L = 0$.  Thus $A = B = 0$.  Again, the trivial
solution is the only solution of the boundary value problem
(\ref{waveLT5}) for $k =0$.

If $k<0$, the roots of the characteristic equation are $\pm i \sqrt{-k}$.
Since the roots are complex, the general solution of the ordinary
differential equation is of the form
\[
F(x) = A \cos\left(\sqrt{-k}\,x\right) + B \sin\left(\sqrt{-k}\,x\right) \ .
\]
However, $F(0)=0$ implies that $A =0$.  Hence $F(L)=0$ implies that
$\displaystyle B \sin\left(L\sqrt{-k}\right) = 0$.  If we take $B=0$,
we get the trivial solution.  We must therefore have
$\displaystyle \sin\left(L\sqrt{-k}\right) = 0$ with $k\neq 0$.  This implies
that $\displaystyle k = k_n = -\left(n\pi/L\right)^2$ for $n$ a
positive integer.  The boundary value problem (\ref{waveLT5}) has non-trivial
solutions only for $\displaystyle k=k_n=-\left(n\pi/L\right)^2<0$ with
$n$ a positive integer, and the solutions associated to $k_n$ are of the form
\[
F(x)=F_n(x) = B_n \sin\left(\frac{n\pi x}{L}\right) \ .
\]

We only need to consider the second ordinary differential equation in
(\ref{waveLT4}) with $\displaystyle k=k_n=-\left(n\pi/L\right)^2$;
namely,
\[
\dydxn{G}{t}{2}(t) + \left(\frac{c n\pi}{L}\right)^2\,G(t) = 0 \ ,
\]
where $n$ is a positive integer.  For each $n$, this is a second order
ordinary differential equation with the characteristic equation
$\displaystyle \lambda^2 + (c n\pi/L)^2=0$.  The two
roots of this equation are the complex numbers
$\displaystyle \lambda_{\pm} = \pm (c n\pi/L)i$.
The general solution is therefore
\[
G(t) = G_n(t) = C_n \cos\left(\frac{cn\pi t}{L}\right)
+ D_n \sin\left(\frac{cn\pi t}{L}\right) \; .
\]

We have found that the functions
\[
u_n(x,t) = F_n(x)G_n(t) =
\left( a_n \cos\left(\frac{cn\pi t}{L}\right)
+ b_n \sin\left(\frac{cn\pi t}{L}\right) \right)
\sin\left(\frac{n\pi\,x}{L}\right)
\]
for $n>0$ satisfy the wave equation (\ref{waveLT1}) and the boundary
conditions (\ref{waveLT2}).   The constant $a_n$ is the product of the
constants $B_n$ and $C_n$, and $b_n$ is the product of $B_n$ and $D_n$.

To satisfy the initial conditions, we seek a solution of the form
\[
u(x,t) = \sum_{n=1}^\infty u_n(x,t)
= \sum_{n=1}^\infty
\left( a_n \cos\left(\frac{cn\pi t}{L}\right)
+ b_n \sin\left(\frac{cn\pi t}{L}\right) \right)
\sin\left(\frac{n\pi\,x}{L}\right) \ .
\]

From $u(x,0) = f(x)$, we get
\[
f(x) = \sum_{n=1}^\infty a_n\,\sin\left(\frac{n\pi x}{L}\right)
\quad , \quad 0<x<L \ .
\]
This is the Fourier sine series of $f$.  The coefficients of
this series are given by
\[
a_n = \frac{2}{L} \int_0^L f(x)
\sin\left(\frac{n\pi x}{L}\right) \dx{x} \ .
\]

From $\displaystyle \pdydx{u}{t}(x,0) = g(x)$, we get
\[
g(x) = \sum_{n=1}^\infty \frac{cn\pi b_n}{L}\,
\sin\left(\frac{n\pi x}{L}\right)
\quad , \quad 0<x<L \ .
\]
This is the Fourier sine series of $g$.  The formula to compute the
coefficients of this Fourier series yields
\[
b_n = \frac{2}{cn\pi} \int_0^L g(x)
\sin\left(\frac{n\pi x}{L}\right) \dx{x} \ .
\]

The solution of the wave equation (\ref{waveLT1}) with the boundary
conditions (\ref{waveLT2}) and the initial conditions (\ref{waveLT3})
is therefore
\begin{align*}
u(x,t) &= \sum_{n=1}^\infty a_n \cos\left(\frac{cn\pi t}{L}\right)
\sin\left(\frac{n\pi x}{L}\right)
+ \sum_{n=1}^\infty b_n \sin\left(\frac{cn\pi t}{L}\right)
\sin\left(\frac{n\pi x}{L}\right) \\
&= \sum_{n=1}^\infty \frac{a_n}{2}
\left( \sin\left(\frac{n\pi}{L}(x+ct)\right)
+ \sin\left(\frac{n\pi}{L}(x-ct)\right)\right) \\
&\qquad + \sum_{n=1}^\infty \frac{b_n}{2}
\left( \cos\left(\frac{n\pi}{L}(x-ct)\right)
- \cos\left(\frac{n\pi}{L}(x+ct)\right)\right) \ .
\end{align*}
The solution is of the form
\[
u(x,t) = \frac{1}{2} \left( f(x+ct) + f(x-ct) \right)
+ \frac{1}{2c} \int_{x-ct}^{x+ct} g(s) \dx{s}
\]
because
\[
\sum_{n=1}^\infty a_n \sin\left(\frac{n\pi s}{L}\right) = f(s)
\]
and
\[
\dfdx{\left( \sum_{n=1}^\infty b_n \cos\left(\frac{n\pi s}{L}\right)\right)}{s}
= -\sum_{n=1}^\infty \frac{n\pi b_n}{L} \sin\left(\frac{n\pi s}{L}\right)
= -\frac{1}{c}\, g(s) \ .
\]
In the discussion above, it is obviously assumed that $f$ and $g$,
initially defined on the interval $[0,L]$, are extended to
even and periodic functions of period $2L$ on the real line.

The domain of dependence of $u(\tilde{x},\tilde{t})$ in the region
$R = \{ (x,t) : 0\leq x \leq L \ \text{and}\  t \geq 0 \}$ is a little
more complex than for the wave equation on the real line but still
depends on the two
{\bfseries characteristic lines}\index{Wave Equation!Characteristic Lines}
$x = ct + \tilde{x} - c \tilde{t}$ and
$x = - ct + \tilde{x} + c \tilde{t}$ (if we consider all the possible
reflections of these two lines through the lines $x=0$ and $x= L$).
This two characteristic lines play a crucial role in the convergence
of finite difference schemes to numerically solve the wave equation.

If we consider the finite difference scheme in
Algorithm~\ref{fdm_sch3S}, we may define the
{\bfseries numerical domain of dependence}\index{Finite Difference
Methods!Numerical Domain of Dependence} of $w_{i,j}$ as the set of values
$\{ w_{r,s} : 0 \leq s \leq j \ \text{and} \ s + i - j \leq r \leq -s
+ i + j \}$.  This region is illustrated in Figure~\ref{waveFig3}.

\pdfF{finite_diff/wave_fig3}{Domain of dependence for a finite
difference scheme}{Domain of dependence for $w_{4,3}$ associated to
the finite difference scheme in Algorithm~\ref{fdm_sch3S}.}{waveFig3}

Suppose that $g$ and $f$ in the definition of the initial conditions
for the wave equation change drastically at a point $a \in ]0, L[$.
For instance, $g(x) = f(x) = 0$ for $x < a$ and increase exponentially
for $x>a$.  Suppose that $(\tilde{x},\tilde{t})$ is such that
$\tilde{x} < a < \tilde{x} + c \tilde{t}$.  Let us consider
grids such that $\rho = \dtx{x}/\dtx{t}$ is constant and satisfies
$\rho < c$, and such that $(\tilde{x},\tilde{t}) = (x_r,t_s)$ for some
$r$ and $s$.  Namely, $(\tilde{x},\tilde{t})$ is part of all the grids
that we consider.  We assume that $\rho$ is small enough (or
alternatively, that $a$ is large enough) to have
$x_{r+s} = (r+s)\dtx{x} < a$.   This situation is completely summarized in
Figure~\ref{waveFig4}.

\pdfF{finite_diff/wave_fig4}{Comparison of domains of
dependence}{Comparison between the domain of dependence for the wave
equation and the finite difference scheme in Algorithm~\ref{fdm_sch3S}
for $\dtx{x}/\dtx{t} < c$.}{waveFig4}

It is clear that $w_{r,s}$, the numerical approximation of
$u(\tilde{x},\tilde{t})$, will never take into consideration the
values of $f(x)$ and $g(x)$ for $x>a$ as $(\dtx{x},\dtx{t}) \to \VEC{0}$
with $\dtx{x}/\dtx{t} < c$ \footnote{$r$ and
$s$ will increase as $\dtx{x}$ and $\dtx{t}$ converge to $0$ but we
always assume that $x_r = \tilde{x}$ and $t_s = \tilde{t}$}.  But,
the domain of dependence of $u(\tilde{x},\tilde{t})$ tells us that the
value of $u(\tilde{x},\tilde{t})$ depends on the values of $g(x)$ and
$f(x)$ for $x>a$.  Thus, $w_{r,s}$ will generally not converge
to $u(\tilde{x},\tilde{t})$.

In conclusion, a necessary condition for our finite difference scheme
to converge for the wave equation is that $\dtx{x}/\dtx{t} \geq c$.
Thus, the numerical domain of dependence of the finite difference
scheme in Algorithm~\ref{fdm_sch3S} must include the domain of
dependence of the wave equation.  We will give a more rigorous
justification later.

The reader may wonder if the conclusion that we have just stated is
particular to the wave equation, and if our finite difference scheme
may behave more nicely with other hyperbolic differential equations.
To add more strength to our argument, we will consider hyperbolic
systems of linear first order partial differential equations.

\begin{defn}
A system of linear first order partial differential equations of the
form $\displaystyle   \VEC{v}_t + A \VEC{v}_x = 0$,
where $\VEC{v}: \RR^2 \rightarrow \RR^n$ and $A$ is a \nn real matrix,
is an {\bfseries hyperbolic system}\index{Partial Differential
Equations!Hyperbolic System} if $A$ is a \nn real symmetric
matrix\footnotemark.  The system of partial
differential equations is
{\bfseries strictly hyperbolic}\index{Partial Differential
Equations!Strictly Hyperbolic System} if all the real eigenvalues of
$A$ are distinct.
\end{defn}

\footnotetext{It is shown in linear algebra courses that real symmetric
matrices have only real eigenvalues.}

The wave equation
\[
\pdydxn{u}{t}{2} - c^2 \pdydxn{u}{x}{2} = 0
\]
can be reduced to a strictly hyperbolic system of first order partial
differential equations.  Let
\[
\VEC{v} =
\begin{pmatrix}
u_x \\ u_t/c
\end{pmatrix}
\ \text{and} \ 
A = \begin{pmatrix}
0 & c \\
c & 0
\end{pmatrix} \ .
\]
Then
\[
\VEC{v}_t - A \VEC{v}_x
=
\begin{pmatrix}
 u_{xt} \\ u_{tt}/c
\end{pmatrix}
-
\begin{pmatrix}
0 & c \\
c & 0
\end{pmatrix}
\begin{pmatrix}
u_{xx} \\ u_{xt}/c
\end{pmatrix}
=
\begin{pmatrix}
 u_{xt} - u_{xt} \\
u_{tt}/c - c u_{xx}
\end{pmatrix}
=
\begin{pmatrix}
 0 \\ 0
\end{pmatrix} \ ,
\]
where the eigenvalues of $A$ are $\pm c$.

One of the reasons to study hyperbolic systems of first order partial
differential equations is that one can use the method of
characteristic\footnote{The reader may consult an introductory book on
partial differential equations like \cite{Str} to learn more about the
method of characteristic to solve some partial differential
equations.} to understand the dangers of solving numerically
hyperbolic differential equations with finite difference schemes.

Consider the simple {\bfseries advection equation}\index{Partial
Differential Equations!Advection Equation}
\begin{equation}\label{folpde}
c\, \pdydx{u}{x} + \pdydx{u}{y} = 0  \quad , \quad -\infty < x <
\infty \ \text{and}\ y > 0
\end{equation}
with the initial condition $u(x,0) = g(x)$ for $x \in \RR$.
The {\bfseries characteristic equations}\index{Partial
Differential Equations!Characteristic Equation} for this partial differential
equation are
\[
x'(t) = c \ , \ y'(t) = 1 \ \text{and} \ u'(t)=0
\]
with the initial conditions
\[
x(0) = s \ , \ y(0) = 0 \ \text{and} \ u(0)=g(s) \ .
\]
The equation $x'(t) = c$ with $x(0)=s$ yields $x = ct + s$,
the equation $y'(t) = 1$ with $y(0)=0$ yields $y = t$ and
the equation $u'(t) = 0$ with $u(0)=g(s)$ yields $u = g(s)$.
We can solve $x = ct+s$ and $y=t$ in terms of $s$ and $t$ to get
$t=y$ and $s= x-cy$.  If we substitute this value of $s$ in the
expression for $u$, we get the solution
$\displaystyle u = u(x,y) = g\left(x-cy\right)$.  The function $u$ is
constant along the characteristic lines $x - c y = a$ for $a \in \RR$.
The value of $u$ at $(x,y)$ is the value of $u$ at the points below
and to the left of $x$ along the line $x - cy = a$.

Consider the following finite difference scheme to numerically solve
(\ref{folpde}) with $u(x,0) = g(x)$ for $x \in \RR$.   Let
$x_i = i\, \dtx{x}$ for $i \in \ZZ$ and $y_j = j\, \dtx{y}$ for $j\geq 0$.
An approximation $w_{i,j}$ of $u(x_i,y_j)$ is provided by the solution
of the finite difference equation
\begin{equation} \label{advSch1}
c\, \frac{w_{i+1,j}- w_{i,j}}{\dtx{x}} + \frac{w_{i,j+1} - w_{i,j}}{\dtx{y}}
= 0
\end{equation}
for $i \in \ZZ$ and $j \geq 0$, where $w_{i,0} = g(x_i)$ for
all $i$.  This scheme is illustrated in Figure~\ref{fdm_fig5}.

\pdfF{finite_diff/fdm_fig5}{A finite difference scheme for the advection
equation}{Schematic representation of the finite difference scheme 
given in (\ref{advSch1}) and the numerical domain of
dependence for $w_{2,3}$.}{fdm_fig5}

We get
\[
w_{i,j+1} = w_{i,j} - \frac{c \dtx{y}}{\dtx{x}} \, (w_{i+1,j} - w_{i,j}) \ .
\]
Thus $w_{i,j+1}$ depends on the values $w_{i,j}$ and $w_{i+1,j}$.  The
first value is at the point $(x_i,y_j)$ straight below
$(x_i,y_{j+1})$ and the other value is at the point $(x_{i+1},y_j)$
below and to the right of $(x_i,y_{j+1})$.  The numerical domain of
dependence for $w_{2,3}$ is illustrated in Figure~\ref{fdm_fig5}.
The numerical domain of dependence of $w_{i,j+1}$ does not contain the
characteristic line $x -cy = a$ through $(x_i,y_{j+1})$ that defines
$u(x_i,y_{j+1})$.  The finite difference scheme in (\ref{advSch1}) will
generally not converge to the solution of the advection equation
whatever the relation between $\dtx{y}$ and $\dtx{x}$.

Let us try another finite difference scheme to numerically solve
(\ref{folpde}) with $u(x,0) = g(x)$ for $x \in \RR$.   Let
$x_i = i\, \dtx{x}$ for $i \in \ZZ$ and $y_j = j\, \dtx{y}$ for $j\geq 0$
as before.  An approximation $w_{i,j}$ of $u(x_i,y_j)$ is provided by
the solution of the finite difference equation
\begin{equation} \label{advSch2}
c\, \frac{w_{i,j}- w_{i-1,j}}{\dtx{x}} + \frac{w_{i,j+1} - w_{i,j}}{\dtx{y}}
= 0
\end{equation}
for $i \in \ZZ$ and $j \geq 0$, where $w_{i,0} = g(x_i)$ for all $i$.
This scheme is illustrated in Figure~\ref{fdm_fig6}.

\pdfF{finite_diff/fdm_fig6}{Another finite difference scheme for the
advection equation}{Schematic representation of the finite difference
scheme given in (\ref{advSch2}) and the numerical domain of
dependence for $w_{2,3}$ for $\dtx{y}/\dtx{x} > 1/c$.}{fdm_fig6}

We get
\[
w_{i,j+1} = w_{i,j} - \frac{c \dtx{y}}{\dtx{x}} \, (w_{i,j} - w_{i-1,j}) \ .
\]
Now, $w_{i,j+1}$ depends on the values $w_{i-1,j}$ and $w_{i,j}$.  The
first value is at the point $(x_{i-1},y_j)$ below and to the right of 
$(x_i,y_{j+1})$ and the other value is at the point $(x_i,y_j)$
straight below the point $(x_i,y_{j+1})$.  The numerical domain of
dependence for $w_{2,3}$ is illustrated in Figure~\ref{fdm_fig6}.

If we assume that $\dtx{y}/\dtx{x} > 1/c$, then
the numerical domain of dependence of $w_{i,j+1}$ does not contain the
characteristic line $x - cy = a$ through $(x_i,y_{j+1})$ that defines
$u(x_i,y_{j+1})$ as can be seen in Figure~\ref{fdm_fig6}.  The finite
difference scheme in (\ref{advSch1}) will generally not converge to the
solution of the advection equation.  We must therefore assume that
$\dtx{y}/\dtx{x} \leq 1/c$ if we hope to get a converging finite
difference scheme.  Under this condition, the characteristic line
$x - cy = a$ through $(x_i,y_{j+1})$ is contained in the numerical
domain of dependence of $w_{i,j+1}$ for the finite difference scheme
given in (\ref{advSch2}) as shown in Figure~\ref{fdm_fig7}.

\pdfF{finite_diff/fdm_fig7}{Numerical domain of dependence of
$w_{2,3}$}{The numerical domain of dependence for $w_{2,3}$ associated
to the finite difference scheme given in (\ref{advSch2}) for
$\dtx{y}/\dtx{x} \leq 1/c$.}{fdm_fig7}

In conclusion, given a difference scheme associated to an hyperbolic
differential equation, we may say that a necessary condition for this
scheme to converge is that its numerical domain of dependence contains
the domain of dependence of the associated hyperbolic differential
equation.  This is known as the {\bfseries Courant-Friedrichs-Lewy (CFL)
condition}\index{Finite Difference Methods!Courant-Friedrichs-Lewy Condition}\index{Finite Difference Methods!CFL Condition}.
This may be a very restrictive condition as we have seen
in the few examples above.  For this reason, finite difference
schemes are not generally recommended to numerically solve hyperbolic
differential equation.  Instead, one may use the method of
characteristics or finite element methods to numerically solve
hyperbolic differential equations.

The issue associated to the domain of dependence when finite
difference schemes are used to numerically solve hyperbolic 
differential equations is not present for elliptic or parabolic
differential equations.  We were able to find stable, consistent and
convergent finite difference schemes for the heat equation with
forcing (a parabolic differential equation) and the Dirichlet equation
(an elliptic differential equation) that had no constraint on
the step sizes.

Nevertheless, we will study the stability, consistency and convergence
of the finite difference scheme given in Algorithm~\ref{fdm_sch3S}.
This will provide a rigorous justification for the restriction
$\dtx{t}/\dtx{x} \leq 1/c$ which is required to get a stable finite
difference scheme.

\subsection{Algorithm~\ref{fdm_sch3S}}

We study the $\ell^2$ convergence according to
Definition~\ref{ell2ConvDefN2} of Algorithm~\ref{fdm_sch3S} which is
used to approximate the solution of the wave equation.

As was mentioned in Remark~\ref{rmk_consist_defN2}, we will get
$\ell^2$ consistency according to Definition~\ref{fdm_consist_defN2}
if we prove consistency according to Definition~\ref{fdm_consist_def}.

\begin{prop}
The scheme in Algorithm~\ref{fdm_sch3S} is consistent according to
Definition~\ref{fdm_consist_def}.
\end{prop}

\begin{proof}
Using the notation introduced in Section~\ref{fdm_CCS}, we have that
\[
P\left(u(x,t), \pdydx{u}{x}(x,t), \pdydx{u}{y}(x,t),
  \pdydxn{u}{x}{2}(x,t), \ldots \right)
= \pdydxn{u}{t}{2}(x,t) - c^2 \pdydxn{u}{x}{2}(x,t)
\]
and
\[
P_\Delta\left(w_{i,j}, w_{i,j+1}, w_{i+1,j}, \ldots\right)
= \frac{w_{i,j+1} - 2w_{i,j} + w_{i,j-1}}{(\dtx{t})^2}
- c^2 \frac{w_{i+1,j} - 2w_{i,j} + w_{i-1,j}}{(\dtx{x})^2}
\]
for the finite difference scheme in Algorithm~\ref{fdm_sch3S}.

The procedure to deduce the local truncation error for the finite
difference equation in Algorithm~\ref{fdm_sch3S} is almost identical to
the procedure to deduce the local truncation error for the finite
difference equation in Algorithm~\ref{fdm_sch2S} given in the proof of
Proposition~\ref{fdm_sch2S_const}.  We find
\begin{align*}
&\tau_{i,j}(\dtx{x},\dtx{t},q) = 
P\left(q(x_i,t_j), \pdydx{q}{x}(x_i,t_j), \pdydx{q}{y}(x_i,t_j),
  \pdydxn{q}{x}{2}(x_i,t_j), \ldots \right) \\
&\qquad\qquad\qquad\qquad
- P_\Delta\left(q(x_i,t_j), q(x_i,y_{j+1}), q(x_{i+1},t_j), \ldots\right) \\
& \quad
= - \frac{1}{4!} \left( \pdydxn{q}{t}{4}\left(x_i,\zeta_{i,j} \right)
+ \pdydxn{q}{t}{4}\left(x_i,\eta_{i,j} \right) \right) (\dtx{t})^2
+ \frac{c^2}{4!} \left( \pdydxn{q}{x}{4}\left(\mu_{i,j},t_j\right)
+ \pdydxn{q}{x}{4}\left(\nu_{i,j},t_j\right) \right) (\dtx{x})^2
\end{align*}
for $\zeta_{i,j} , \eta_{i,j} \in ]t_{j-1},t_{j+1}[$ and
$\mu_{i,j} ,\nu_{i,j} \in ]x_{i-1},x_{i+1}[$.
If the partial derivatives of order up to four are continuous on the
compact set
$\displaystyle R = \left\{ (x,t) : 0 \leq x \leq L \ \text{and} \ 0 \leq
t \leq T \right\}$, then there exists $H$
such that
\[
\max_{(x,t)\in R}\, \left|\pdydxn{q}{x}{4}\left(x,t\right) \right| \leq
H \quad \text{and} \quad 
\max_{(x,t)\in R}\, \left|\pdydxn{q}{t}{4}\left(x,t\right) \right| \leq
H \ .
\]
Hence
\begin{equation} \label{sch3SCons}
\begin{split}
\max \left\{ \tau_{i,j} : 0<i<N \ \text{and} \ 0<j < M \right\}
& \leq \frac{H}{12}\,(\dtx{t})^2 + \frac{c^2 H}{12} (\dtx{x})^2 \rightarrow 0 \\
&\qquad\qquad \text{as} \quad \min \{N, M\} \rightarrow \infty
\end{split}
\end{equation}
since $\dtx{x} = L/N$ and $\dtx{t} = T/M$ converge to $0$ as $N$ and
$M$ converge to infinity.

Since $\displaystyle B\left(u(x,y), \ldots \right) = u(x,y)$,
$\displaystyle B_\Delta(w_{i,j},\ldots) = w_{i,j}$ and $w_{i,j} = u(x_i,t_j)$
for $(i,j)$ such that $(x_i,t_j) \in \partial R_\Delta$, we get from 
(\ref{sch3SCons}) that Definition~\ref{fdm_consist_def} is satisfied.
\end{proof}

\begin{prop}\label{CFLCondProp}
The finite difference scheme in Algorithm~\ref{fdm_sch3S} is
$\ell^2$-stable when
\[
\frac{\dtx{t}}{\dtx{x}} \leq \frac{1}{c} \ .
\]
\end{prop}

\begin{proof}
Since the definition of stability that we have presented were for
one-step finite difference schemes, we have to use a little trick to
prove $\ell^2$ stability for Algorithm~\ref{fdm_sch3S}.

To use Definition~\ref{fdmStableDefNo4}, we have to rewrite the finite
difference schemes in the
$\VEC{v}_{j+1} = \tilde{Q}_\alpha\VEC{v}_j + \tilde{\VEC{B}}_j$,
where 
\[
\VEC{v}_j = \begin{pmatrix} \VEC{w}_{j-1} \\ \VEC{w}_j \end{pmatrix} \ .
\]
Namely,
\[
\begin{pmatrix}
w_j \\ w_{j+1}
\end{pmatrix}
=
\begin{pmatrix}
0 & \Id \\ \Id & J  \\ 
\end{pmatrix}
\begin{pmatrix}
w_{j-1} \\ w_j
\end{pmatrix}
+
\begin{pmatrix}
0 \\ \VEC{B}_j
\end{pmatrix}
\]
for $0 \leq j < M$, where $J$ and $B_j$ are defined at the end of
Section~\ref{DerWaveSection}.

Since the matrix $\displaystyle \tilde{Q}_k \equiv
\begin{pmatrix} 0 & \Id \\ \Id & J \end{pmatrix}$ is symmetric, we may
use Proposition~\ref{MatrixStabProp}.
We need to find all the eigenvalues of $\tilde{Q}_\alpha$.
From Proposition~\ref{fdm_eigR}, we have that the eigenvalues of $J$ are
\[
\lambda_k = -2 +2\alpha -2\alpha \cos\left(\frac{k\pi}{N}\right)
= -2 + 4 \alpha \sin^2\left(\frac{k\pi}{2N}\right)
\quad , \quad 0 < k < N \ .
\]
Since the eigenvectors of $J$ associated to $\lambda_k$ are obviously
also eigenvectors of $\Id$ and $0$, we may use
Proposition~\ref{fdm_eig_RR} to claim that the eigenvalues of
$\displaystyle \begin{pmatrix} 0 & 1 \\ 1 & \lambda_i\end{pmatrix}$
for $0 < k < N$ are eigenvalues of $\tilde{Q}_\alpha$.  Namely, the
$2N-2$ distinct eigenvalues of $\tilde{Q}_\alpha$ are the roots of the
characteristic polynomials
\[
p_k(\lambda) =  \lambda^2 -\lambda_k \lambda - 1
= \lambda^2 -2\left(1 - 2 \alpha \sin^2\left(\frac{k\pi}{2N}\right)
\right) \lambda - 1
\]
for $0 < k < N$.  Let
\[
  \delta_k = 1 - 2\alpha \sin^2\left( \frac{k \pi}{2N} \right)
\]
for $0 < k < N$.  
We have that $p_k(\lambda) = \lambda^2 -2 \delta_k \lambda + 1 = 0$
for
\[
\lambda_{\pm} = \delta_k \pm \sqrt{\delta_k^2 - 1} \ .
\]
We have $\delta_k < 1$ because $\alpha>0$ and $0<k<N$.
When $\delta_k<-1$, we have
$\RE \lambda_{-} < -1$ and thus $|\lambda_{-}|>1$.  Hence, we only need to
consider $-1 \leq \delta_k < 1$.  When $-1 < \delta_k < 1$, we get
$\displaystyle
\lambda_\pm = \delta_k \pm i\sqrt{1 - \delta_k^2} \in \CC \setminus \RR$
and
$\displaystyle |\lambda_{-}|^2 = |\lambda_{+}|^2
= \delta_k^2 + ( 1 - \delta_k^2) = 1$.
When $\delta_k = -1$, we get $\lambda_{-} = \lambda_{+} = - 1$.  Thus
$|\lambda_{-}| = |\lambda_{+}| \leq 1$ only when $-1 \leq \delta_k < 1$.

We have shown that
\[
-1 \leq \delta_k = 1 -2 \alpha \sin^2\left( \frac{k \pi}{2N}\right) < 1
\]
implies $|\lambda_{\pm}|\leq 1$.  Since $\alpha >0$, the second
inequality is always true. From the first inequality, we get
\[
\alpha \sin^2\left( \frac{k \pi}{2N} \right) \leq 1 \ .
\]
This can be true for any $k$ and $N$ only if $\alpha \leq 1$;
namely,
\[
\alpha = \left( \frac{c \dtx{t}}{\dtx{x}}\right)^2 \leq 1 \ .
\]
\end{proof}

Since we have $\ell^2$ consistency and $\ell^2$ stability, we may
conclude that the finite difference scheme in
Algorithm~\ref{fdm_sch3S} is $\ell^2$-convergent if (and only if) 
$\dtx{t} / \dtx{x} \leq 1/c$.
This condition says that the numerical domain of dependence of the
finite difference scheme must include the domain of dependence of the
wave equation as we have seen in Section~\ref{DomDepHyp}.
This is called a {\bfseries Courant-Friedrichs-Lewy (CFL)
condition}\index{Finite Difference Methods!Courant-Friedrichs-Lewy Condition}
\index{Finite Difference Methods!CFL Condition}.

\begin{rmkList} \label{dfmLastRmks}
\begin{enumerate}
\item We may also determine the $L^2$ stability as defined in
Section~\ref{L2Stab} of the finite difference scheme in
Algorithm~(\ref{fdm_sch3S}).  To use the formulae in
Remark~\ref{fdmgfL2ext}, we first have to use the substitute
$z = 2\pi x /L$ to obtain a partial 
differential equation defined for $0 \leq z \leq 2\pi$ and
$0 \leq t \leq T$.  With this substitution, the wave equation becomes
\[
  \pdydxn{u}{t}{2} - \left(\frac{2\pi c}{L}\right)^2 \pdydxn{u}{z}{2} = 0
\]
The only difference with the original partial differential equation is
that $c$ is replaced by $2\pi c/L$.  \label{dfmLastRmksItem1}

We now have
$\displaystyle \alpha = \left(\frac{2\pi c \dtx{t}}{L \dtx{z}}\right)^2$
in the finite difference scheme in Algorithm~\ref{fdm_sch3S}.  We also have
$\dtx{z} = 2\pi/N$ and $\dtx{t} = T/M$.

We have (\ref{fdmgfL2Aext}) with $a_0 = 1$, $b_{-1} = b_1 = \alpha$,
$b_0 = 2(1-\alpha)$, $c_0 = -1$ and all the other $a_r$, $b_r$ and
$c_r$ null.  Hence
\[
\alpha_k = 1 \quad , \quad
\beta_k = \alpha e^{-k\dtx{z}\, i}
+ 2(1-\alpha) + \alpha e^{k \dtx{z} \, i} \quad \text{and}
\quad \gamma_k = -1 \ .
\]
We have to find the roots of the characteristics polynomials
\begin{align}
p_k(\lambda) &= \alpha_k \lambda^2 - \beta_k \lambda  - \gamma_k
= \lambda^2 - \left( \alpha e^{-k\dtx{z} \, i}
+ 2(1-\alpha) + \alpha e^{k \dtx{z} \, i} \right) \lambda + 1 \nonumber \\
&= \lambda^2 -\left(2 - 2\alpha\big(1+\cos( k \dtx{z})\big) \right) \lambda + 1
= \lambda^2 - \left( 2 -4 \alpha \sin^2\left( \frac{k \dtx{z}}{2}
  \right) \right)\lambda + 1 \label{fdmgfL2Cext}
\end{align}
for $0 \leq k < N$.  Let
\[
  \delta_k = 1 - 2\alpha \sin^2\left( \frac{k \dtx{z}}{2} \right) \ .
\]
We get $p_k(\lambda) = \lambda^2 - 2\delta_k \lambda  +1$ as in the
proof of Proposition~\ref{CFLCondProp}.  Proceeding as was done in
that proof, we get
\[
  \alpha = \left(\frac{2\pi c \dtx{t}}{L \dtx{z}}\right)^2
  = \left( \frac{c \dtx{t}}{\dtx{x}}\right)^2 < 1 \ .
\]
We have a strict inequality because $\alpha = 1$ is associated to a
root of absolute value $1$ but multiplicity $2$ for $p_k$.
\item  We did not really need to use the substitution $z = 2\pi x/L$
to reduce the problem to the interval $[0,2\pi]$ as we have done
above.   We could have done all the work in Section~\ref{L2Stab}
assuming periodic function of period $L$.  The space
$L^2([0,2\pi])$ is replaced by $L^2([0,L])$ with the norm
$\displaystyle \|f\|_2 =
\left( \frac{1}{L} \int_0^L |f(x)|^2 \dx{x} \right)^{1/2}$.  The Fourier
transform of $f$ is defined by
\[
\hat{f}(k) =
\frac{1}{L} \int_0^L f(x) e^{-(2\pi k x/L)i} \dx{x}
\]
for $k \in \ZZ$.  We seek a solution of (\ref{fdmgfL2Bext}) of the form
\[
  w_j = \sum_{k\in \ZZ} A_{k,j} e^{(2\pi k x/L) i}
\]
for some $A_{k,j} \in \RR$.   We get (\ref{{fdmgfL2Cext}}) with
\[
\alpha_k = \sum_{r=-m}^m a_r e^{(2\pi k r \dtx{x}/L) i} \quad , \quad
\beta_k = \sum_{r=-m}^m b_r  e^{(2\pi k r \dtx{x}/L) i} \quad \text{and} \quad
\gamma_k = \sum_{r=-m}^m c_r  e^{(2\pi k r \dtx{x}/L) i} \ .
\]
The values of $\alpha_k$, $\beta_k$ and $\gamma_k$ above are the
same values defined in (\ref{{fdmgfL2Dext}}) because
$\displaystyle \frac{2\pi k r \dtx{x}}{L} = 
\frac{2\pi k r}{N} = k r \dtx{z}$, where $z=2\pi x /L$ is the
substitution above.
\item We do not need to remember the formulae for $\alpha_k$, $\beta_k$
and $\gamma_k$ to find the characteristic polynomials
$\alpha_k \lambda^2 - \beta_k \lambda - \gamma_k$.  These
characteristic polynomials are given by the coefficients of
$\displaystyle e^{(2\pi k x/L) i}$ after we substitute the expression
of $w_j$ above in (\ref{fdmgfL2Bext}).  So, it suffices to substitute
$\displaystyle w_{n,j} = \lambda^j e^{(2\pi k x_n/L) i}$ in
(\ref{fdmgfL2Aext}) to get, after some simplifications, the
characteristic polynomial associated to $k$.

For instance, for the finite difference scheme in
Algorithm~\ref{fdm_sch3S} which is considered in the present section,
if we substitute $\displaystyle w_{s,j} = \lambda^j e^{(2\pi k x_s/L) i}$
in (\ref{fdm_sch3S}) \footnote{Where the index $i$ is replaced by $s$
because $i$ is presently used as the complex number such that
$i^2=-1$.}, we get
\begin{align*}
0 & = \lambda^{j+1} e^{(2\pi k x_s/L) i}
- 2 \lambda^j e^{(2\pi k x_s/L) i} + \lambda^{j-1} e^{(2\pi k x_s/L) i} \\
&\qquad - \alpha \left( \lambda^j e^{(2\pi k(x_s +\dtx{x})/L) i}
- 2 \lambda^j e^{(2\pi k x_s/L) i}
+ \lambda^j e^{(2\pi (x_s-\dtx{x})/L) i} \right) \ .
\end{align*}
If we divide by $\displaystyle \lambda^{j-1} e^{(2\pi k x_s/L) i}$, we get
\begin{align}
0 & = \lambda^2 - 2 \lambda + 1 - \alpha \left( e^{(2\pi k \dtx{x}/L) i} - 2
+ e^{-(2\pi k \dtx{x}/L) i} \right) \lambda \nonumber \\
& = \lambda^2 - 2 \left(1 - 2 \alpha \sin^2\left( \frac{\pi k \dtx{x}}{L}\right)
\right) \lambda + 1 \ . \label{sch3D_EigRoot}
\end{align}
This is $p_k(\lambda) = 0$ with $p_k$ defined in (\ref{fdmgfL2Cext})
because the substitution $z = 2\pi x /L$ yields
$\displaystyle \dtx{z} =  2 \pi \dtx{x}/L$.
\item Finally, we may study the $\ell^2$ stability as presented in
Section~\ref{ell2Theory}.  More precisely, we may use the content of
Remark~\ref{ell2TheoryMsteps}.  The finite difference scheme in
Algorithm~\ref{fdm_sch3S} is of the form (\ref{fdml2gfImplext}) with
$a_0 =1$, $b_{-1}= b_1 = -\alpha$, $b_0 = -2(1-\alpha)$, $c_0 = 1$ and
the other $a_r$, $b_r$ and $c_r$ are null.    \label{dfmLastRmksItem4}

Note that we assume that we are in the context of
Item~\ref{dfmLastRmksItem1} with 
$\displaystyle \alpha = \left(\frac{2\pi c \dtx{t}}{L \dtx{z}}\right)^2$
because the formulae in Remark~\ref{ell2TheoryMsteps} are for $2\pi$
periodic functions in their space variable.  The characteristic
polynomial in (\ref{fdml2gfImplBext}) is therefore
\[
\left(\lambda(z)\right)^2 - \big(\alpha e^{-zi} + 2(1-\alpha) +
\alpha e^{zi}\big) \lambda(z) + 1
= \left(\lambda(z)\right)^2 - 2\big(1 -2 \alpha \sin^2(z)\big)
\lambda(z) + 1 \ .
\]
Let $\displaystyle \gamma(z) = 1 -2 \alpha \sin^2(z)$.  We find
\[
  \lambda_{\pm}(z) = \gamma(z) \pm \sqrt{\gamma^2(z)-1}
\]
for $z \in [0,2p]$.  To have stability, we need to have
$|\lambda_{\pm}(z)| \leq 1$ for all $z$.  This is possible only if
$\alpha \leq 1$.   We again get
\[
  \alpha = \left(\frac{2\pi c \dtx{t}}{L \dtx{z}}\right)^2
  = \left( \frac{c \dtx{t}}{\dtx{x}}\right)^2 < 1 \ .
\]
\end{enumerate}
\end{rmkList}

\section{Exercises}

\begin{question}
Proof that the Crank-Nicolson scheme in Algorithm~\ref{fdm_sch11S} is
stable according to Definition~\ref{fdmStableDefNo1} if we assume that
$\displaystyle \frac{c^2\dtx{t}}{2 (\dtx{x})^2} \leq 1$.  This condition is
not required but the proof is easier with it.
\label{fdmQ1}
\end{question}

\begin{question}
Let $E$ be a Hilbert space with the norm
$\|\cdot\|$ defined by a scalar product $\ps{\cdot}{\cdot}$, and let
$P:E \to E$ be a normal operator.\\
\subQ{a} Prove that
$\displaystyle \left\|P^2 \right\| = \left\|P P^\ast\right\|$. \\
\subQ{b} Use (a) to prove that $\displaystyle \left\|P^2 \right\| = \|P\|^2$.
\label{fdmQ2}
\end{question}

\begin{question}
Prove that the finite difference scheme in Algorithm~\ref{fdm_sch1S}
is $\ell^2$-stable according to Definition~\ref{fdmStableDefNo4}.\\
Hint: Proposition~\ref{MatrixStabProp}.
\label{fdmQ3}
\end{question}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "notes"
%%% End: 

